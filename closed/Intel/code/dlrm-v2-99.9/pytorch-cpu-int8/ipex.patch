diff --git a/README.md b/README.md
index 2a6e25751..153b84ccb 100644
--- a/README.md
+++ b/README.md
@@ -6,12 +6,8 @@ Intel® Extension for PyTorch\* provides optimizations for both eager mode and g
 
 The extension can be loaded as a Python module for Python programs or linked as a C++ library for C++ programs. In Python scripts users can enable it dynamically by importing `intel_extension_for_pytorch`.
 
-* Check [CPU tutorial](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/) for detailed information of Intel® Extension for PyTorch\* for Intel® CPUs. Source code is available at the [main branch](https://github.com/intel/intel-extension-for-pytorch/tree/main).
-* Check [GPU tutorial](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/) for detailed information of Intel® Extension for PyTorch\* for Intel® GPUs. Source code is available at the [xpu-main branch](https://github.com/intel/intel-extension-for-pytorch/tree/xpu-main).
-
-## Large Language Models (LLMs) Optimization
-
-In the current technological landscape, Generative AI (GenAI) workloads and models have gained widespread attention and popularity. Large Language Models (LLMs) have emerged as the dominant models driving these GenAI applications. Starting from 2.1.0, specific optimizations for certain LLM models are introduced in the Intel® Extension for PyTorch\*. Check [LLM optimizations](./examples/cpu/inference/python/llm) for details.
+* Check [CPU tutorial](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/) for detailed information of Intel® Extension for PyTorch\* for Intel® CPUs. Source code is available at the [master branch](https://github.com/intel/intel-extension-for-pytorch/tree/master).
+* Check [GPU tutorial](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/) for detailed information of Intel® Extension for PyTorch\* for Intel® GPUs. Source code is available at the [xpu-master branch](https://github.com/intel/intel-extension-for-pytorch/tree/xpu-master).
 
 ## Installation
 
@@ -24,28 +20,28 @@ python -m pip install intel_extension_for_pytorch
 ```
 
 ```python
-python -m pip install intel_extension_for_pytorch --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/cpu/us/
+python -m pip install intel_extension_for_pytorch -f https://developer.intel.com/ipex-whl-stable-cpu
 ```
 
 **Note:** Intel® Extension for PyTorch\* has PyTorch version requirement. Please check more detailed information via the URL below.
 
 More installation methods can be found at [CPU Installation Guide](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html).
 
-Compilation instruction of the latest CPU code base `main` branch can be found at [Installation Guide](https://github.com/intel/intel-extension-for-pytorch/blob/main/docs/tutorials/installation.md#install-via-compiling-from-source).
+Compilation instruction of the latest CPU code base `master` branch can be found at [Installation Guide](https://github.com/intel/intel-extension-for-pytorch/blob/master/docs/tutorials/installation.md#install-via-compiling-from-source).
 
 ### GPU version
 
 You can install Intel® Extension for PyTorch\* for GPU via command below.
 
 ```python
-python -m pip install torch==2.0.1a0 torchvision==0.15.2a0 intel_extension_for_pytorch==2.0.110+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
+python -m pip install torch==2.0.1a0 torchvision==0.15.2a0 intel_extension_for_pytorch==2.0.110+xpu -f https://developer.intel.com/ipex-whl-stable-xpu
 ```
 
 **Note:** The patched PyTorch 2.0.1a0 is required to work with Intel® Extension for PyTorch\* on Intel® graphics card for now.
 
 More installation methods can be found at [GPU Installation Guide](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/tutorials/installation.html).
 
-Compilation instruction of the latest GPU code base `xpu-main` branch can be found at [Installation Guide](https://github.com/intel/intel-extension-for-pytorch/blob/xpu-main/docs/tutorials/installation.md#install-via-compiling-from-source).
+Compilation instruction of the latest GPU code base `xpu-master` branch can be found at [Installation Guide](https://github.com/intel/intel-extension-for-pytorch/blob/xpu-master/docs/tutorials/installation.md#install-via-compiling-from-source).
 
 ## Getting Started
 
@@ -53,8 +49,6 @@ Minor code changes are required for users to get start with Intel® Extension fo
 
 The following code snippet shows an inference code with FP32 data type. More examples on CPU, including training and C++ examples, are available at [CPU Example page](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/examples.html). More examples on GPU are available at [GPU Example page](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/tutorials/examples.html).
 
-**NOTE:** More detailed information about `torch.compile()` with `ipex` backend can be found at [Tutorial features page](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features.html#torch-compile-experimental-new-feature-from-2-0-0).
-
 ### Inference on CPU
 
 ```python
@@ -91,13 +85,13 @@ with torch.no_grad():
   model(data)
 ```
 
-## Intel® AI Reference Models
+## Model Zoo
 
-Use cases that had already been optimized by Intel engineers are available at [Intel® AI Reference Models](https://github.com/IntelAI/models/tree/pytorch-r2.1.100-models) (former Model Zoo). A bunch of PyTorch use cases for benchmarking are also available on the [Github page](https://github.com/IntelAI/models/tree/pytorch-r2.1.100-models/benchmarks#pytorch-use-cases). You can get performance benefits out-of-box by simply running scipts in the Model Zoo.
+Use cases that had already been optimized by Intel engineers are available at [Model Zoo for Intel® Architecture](https://github.com/IntelAI/models/tree/pytorch-r2.0-models). A bunch of PyTorch use cases for benchmarking are also available on the [Github page](https://github.com/IntelAI/models/tree/pytorch-r2.0-models/benchmarks#pytorch-use-cases). You can get performance benefits out-of-box by simply running scipts in the Model Zoo.
 
 ## License
 
-_Apache License_, Version _2.0_. As found in [LICENSE](https://github.com/intel/intel-extension-for-pytorch/blob/main/LICENSE) file.
+_Apache License_, Version _2.0_. As found in [LICENSE](https://github.com/intel/intel-extension-for-pytorch/blob/master/LICENSE) file.
 
 ## Security
 
diff --git a/csrc/cpu/CMakeLists.txt b/csrc/cpu/CMakeLists.txt
index 89b732cc5..5800fc8c4 100644
--- a/csrc/cpu/CMakeLists.txt
+++ b/csrc/cpu/CMakeLists.txt
@@ -157,13 +157,6 @@ else()
   message(FATAL_ERROR "ERROR: Cannot find oneMKL!")
 endif()
 
-find_package(OMP QUIET)
-if(${OpenMP_FOUND})
-  # It will use openmp lib of current compiler.
-  # message("!!!!!OpenMP_CXX_LIBRARIES: ${OpenMP_CXX_LIBRARIES}")
-  target_link_libraries(${PLUGIN_NAME_CPU} PUBLIC "${OpenMP_CXX_LIBRARIES}")
-endif()
-
 target_include_directories(${PLUGIN_NAME_CPU} PUBLIC ${TORCH_INCLUDE_DIRS})
 
 target_link_directories(${PLUGIN_NAME_CPU} PRIVATE ${CMAKE_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR})
diff --git a/csrc/cpu/aten/InstanceNorm.cpp b/csrc/cpu/aten/InstanceNorm.cpp
index 7b60f6a93..c50eea4df 100644
--- a/csrc/cpu/aten/InstanceNorm.cpp
+++ b/csrc/cpu/aten/InstanceNorm.cpp
@@ -444,11 +444,11 @@ TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
       torch_ipex::cpu::instance_norm_backward);
 }
 
-// IPEX_TORCH_LIBRARY_IMPL(aten, CPU, m) {
-//   m.impl(
-//       TORCH_SELECTIVE_NAME("aten::instance_norm"),
-//       TORCH_FN((&torch_ipex::cpu::instance_norm)));
-// }
+IPEX_TORCH_LIBRARY_IMPL(aten, CPU, m) {
+  m.impl(
+      TORCH_SELECTIVE_NAME("aten::instance_norm"),
+      TORCH_FN((&torch_ipex::cpu::instance_norm)));
+}
 
 } // namespace cpu
 } // namespace torch_ipex
\ No newline at end of file
diff --git a/csrc/cpu/aten/Linear.cpp b/csrc/cpu/aten/Linear.cpp
index 1a9642eb6..df9ea718e 100644
--- a/csrc/cpu/aten/Linear.cpp
+++ b/csrc/cpu/aten/Linear.cpp
@@ -356,38 +356,38 @@ at::Tensor ipex_linear_eltwise(
       input, weight, bias, eltwise, op_context, out_features);
 }
 
+DEFINE_DISPATCH(woq_linear_packB_stub);
 DEFINE_DISPATCH(woq_tpp_gemm_packB_stub);
 at::Tensor woq_linear_pack_weight(
     const at::Tensor& weight,
-    std::vector<int64_t>& weight_shape,
-    bool is_int4,
-    int64_t group_size,
+    const at::Tensor& scales,
+    const at::Tensor& zero_points,
     int64_t lowp_mode) {
   // TPP kernel does not support edge cases
   // It generates packed weight in 4d (Nc, Kc, block_k, block_n)
-  auto N = weight_shape[0], K = weight_shape[1];
+  auto N = weight.size(0), K = weight.size(1);
   // For TPP kernel, we only consider even K
   if (K % 2 == 0) {
+    bool is_int4 = weight.scalar_type() == c10::kQUInt4x2;
+    // int num_threads = at::get_num_threads();
     size_t block_n = 32;
-    size_t block_k = group_size > 0 ? std::min(group_size, (int64_t)64) : 64;
+    if (lowp_mode == 0) {
+      block_n = 16;
+    }
+    size_t block_k = 64;
     while (K % block_k != 0) {
       block_k /= 2;
     }
     assert(block_k > 0);
     if (is_int4) {
-      if (block_k % 4 && lowp_mode == 3) {
-        // This case is not supported by kernel
-        return weight;
-      }
       // Create a new non-quantized tensor in data type uint8 (Byte)
       // One uint8 holds two int4 values. Compressed along K.
       // N is padded to the nearest multiple of block_n.
-      // Note that weight is already compressed
       int64_t K_int4_compressed = K / 2;
       int64_t N_int4 = N % block_n ? N / block_n * block_n + block_n : N;
       at::Tensor weight_int4 = at::empty(
           {N_int4, K_int4_compressed}, device(c10::kCPU).dtype(c10::kByte));
-      int64_t weight_size_bytes = weight.numel();
+      int64_t weight_size_bytes = weight.numel() / 2;
       int64_t weight_int4_size_bytes = weight_int4.numel();
       int64_t pad_size_bytes = weight_int4_size_bytes - weight_size_bytes;
       std::memcpy(weight_int4.data_ptr(), weight.data_ptr(), weight_size_bytes);
@@ -395,25 +395,57 @@ at::Tensor woq_linear_pack_weight(
           (uint8_t*)weight_int4.data_ptr() + weight_size_bytes,
           0,
           pad_size_bytes);
-      return woq_tpp_gemm_packB_stub(
+      auto packed_b = woq_tpp_gemm_packB_stub(
           kCPU, weight_int4, is_int4, block_n, block_k, lowp_mode);
+      if (packed_b.defined()) {
+        return packed_b;
+      }
     }
-    if (N % block_n) {
-      return weight;
-    } else {
-      return woq_tpp_gemm_packB_stub(
+    if (!(N % block_n) && !(K % block_k)) {
+      auto packed_b = woq_tpp_gemm_packB_stub(
           kCPU, weight, is_int4, block_n, block_k, lowp_mode);
+      if (packed_b.defined()) {
+        return packed_b;
+      }
     }
   }
-  return weight;
+  return woq_linear_packB_stub(kCPU, weight, scales, zero_points);
 }
 
+DEFINE_DISPATCH(woq_linear_unpackB_stub);
 DEFINE_DISPATCH(woq_tpp_gemm_unpackB_stub);
 at::Tensor woq_linear_unpack_weight(
     const at::Tensor& weight,
     bool is_int4,
     int64_t lowp_mode) {
-  return woq_tpp_gemm_unpackB_stub(kCPU, weight, is_int4, lowp_mode);
+  if (weight.dim() > 2) {
+    auto unpacked_b =
+        woq_tpp_gemm_unpackB_stub(kCPU, weight, is_int4, lowp_mode);
+    if (unpacked_b.defined()) {
+      return unpacked_b;
+    }
+  }
+  return woq_linear_unpackB_stub(kCPU, weight);
+}
+
+DEFINE_DISPATCH(woq_gemm_kernel_stub);
+void woq_linear_kernel_output(
+    const at::Tensor& self,
+    const at::Tensor& weight,
+    const at::Tensor& scales_float,
+    const at::Tensor& zero_points_float,
+    const at::Tensor& bias,
+    int64_t lowp_mode,
+    at::Tensor& output) {
+  woq_gemm_kernel_stub(
+      kCPU,
+      self,
+      weight,
+      scales_float,
+      zero_points_float,
+      bias,
+      lowp_mode,
+      output);
 }
 
 DEFINE_DISPATCH(woq_tpp_gemm_kernel_stub);
@@ -424,26 +456,48 @@ at::Tensor woq_linear_kernel(
     const std::vector<at::Tensor>& zps_list,
     const std::vector<at::Tensor>& bias_list,
     bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t act_quant_mode) {
-  int64_t quant_w_mode = group_size > 0 ? 1 : 0;
-  return woq_tpp_gemm_kernel_stub(
-      kCPU,
+    int64_t num_concats) {
+  if (weight.dim() > 2) {
+    auto out = woq_tpp_gemm_kernel_stub(
+        kCPU,
+        self,
+        weight,
+        scales_list,
+        zps_list,
+        bias_list,
+        is_int4,
+        lowp_mode,
+        num_concats,
+        WOQ_FUSE_NONE, // no post op fusion
+        std::vector<at::Tensor>());
+    if (out.defined()) {
+      return out;
+    }
+  }
+  auto input_size = self.sizes();
+  std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
+  output_size.push_back(weight.size(0));
+  auto output = at::empty(output_size, self.options());
+  output.set_requires_grad(self.requires_grad());
+  woq_linear_kernel_output(
       self,
       weight,
-      scales_list,
-      zps_list,
-      bias_list,
-      is_int4,
+      scales_list[0],
+      zps_list[0],
+      bias_list[0],
       lowp_mode,
-      num_concats,
-      WOQ_FUSE_NONE, // no post op fusion
-      std::vector<at::Tensor>(),
-      act_quant_mode,
-      quant_w_mode,
-      group_size);
+      output);
+  if (num_concats > 1) {
+    // View as [..., num_concats, N/num_concats], transpose then make contiguous
+    // Finally view back as output shape
+    auto out_shape = output.sizes().vec();
+    out_shape.insert(out_shape.end() - 1, num_concats);
+    out_shape.back() /= num_concats;
+    return output.view(out_shape).transpose(0, -2).contiguous().view(
+        output.sizes().vec());
+  }
+  return output;
 }
 
 at::Tensor woq_linear_forward(
@@ -456,6 +510,32 @@ at::Tensor woq_linear_forward(
       ->run(input);
 }
 
+DEFINE_DISPATCH(woq_gemm_eltwise_kernel_stub);
+void woq_linear_eltwise_kernel_output(
+    const at::Tensor& self,
+    const at::Tensor& weight,
+    const at::Tensor& scales_float,
+    const at::Tensor& zero_points_float,
+    const at::Tensor& bias,
+    const c10::string_view& post_op,
+    const torch::List<c10::optional<at::Scalar>>& scalars,
+    const c10::optional<c10::string_view>& algorithm,
+    int64_t lowp_mode,
+    at::Tensor& output) {
+  woq_gemm_eltwise_kernel_stub(
+      kCPU,
+      self,
+      weight,
+      scales_float,
+      zero_points_float,
+      bias,
+      post_op,
+      scalars,
+      algorithm,
+      lowp_mode,
+      output);
+}
+
 at::Tensor woq_linear_eltwise_kernel(
     const at::Tensor& self,
     const at::Tensor& weight,
@@ -466,28 +546,44 @@ at::Tensor woq_linear_eltwise_kernel(
     const torch::List<c10::optional<at::Scalar>>& scalars,
     const c10::optional<c10::string_view>& algorithm,
     bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t act_quant_mode) {
+    int64_t num_concats) {
   int64_t post_op_fusion_type =
       post_op == "gelu" ? WOQ_FUSE_GELU : WOQ_FUSE_NONE;
-  int64_t quant_w_mode = group_size > 0 ? 1 : 0;
-  return woq_tpp_gemm_kernel_stub(
-      kCPU,
+  if (weight.dim() > 2) {
+    auto out = woq_tpp_gemm_kernel_stub(
+        kCPU,
+        self,
+        weight,
+        scales_list,
+        zps_list,
+        bias_list,
+        is_int4,
+        lowp_mode,
+        num_concats,
+        post_op_fusion_type,
+        std::vector<at::Tensor>());
+    if (out.defined()) {
+      return out;
+    }
+  }
+  auto input_size = self.sizes();
+  std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
+  output_size.push_back(weight.size(0));
+  auto output = at::empty(output_size, self.options());
+  output.set_requires_grad(self.requires_grad());
+  woq_linear_eltwise_kernel_output(
       self,
       weight,
-      scales_list,
-      zps_list,
-      bias_list,
-      is_int4,
+      scales_list[0],
+      zps_list[0],
+      bias_list[0],
+      post_op,
+      scalars,
+      algorithm,
       lowp_mode,
-      num_concats,
-      post_op_fusion_type,
-      std::vector<at::Tensor>(),
-      act_quant_mode,
-      quant_w_mode,
-      group_size);
+      output);
+  return output;
 }
 
 at::Tensor woq_linear_gelu_forward(
@@ -508,27 +604,87 @@ at::Tensor woq_linear_add_kernel(
     const std::vector<at::Tensor>& zps_list,
     const std::vector<at::Tensor>& bias_list,
     bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
     int64_t num_concats,
-    const std::vector<at::Tensor>& others,
-    int64_t act_quant_mode) {
-  int64_t quant_w_mode = group_size > 0 ? 1 : 0;
-  return woq_tpp_gemm_kernel_stub(
-      kCPU,
+    at::Tensor& accumu,
+    const c10::optional<at::Scalar>& alpha) {
+  c10::Scalar a = alpha.has_value() ? alpha.value() : 1.0f;
+  if (weight.dim() > 2) {
+    auto output = woq_tpp_gemm_kernel_stub(
+        kCPU,
+        self,
+        weight,
+        scales_list,
+        zps_list,
+        bias_list,
+        is_int4,
+        lowp_mode,
+        num_concats,
+        WOQ_FUSE_NONE, // no eltwise post op
+        std::vector<at::Tensor>());
+    if (output.defined()) {
+      at::add_out(accumu, output, accumu, a);
+      return accumu;
+    }
+  }
+  auto input_size = self.sizes();
+  std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
+  output_size.push_back(weight.size(0));
+  auto output = at::empty(output_size, self.options());
+  output.set_requires_grad(self.requires_grad());
+  woq_linear_kernel_output(
       self,
       weight,
-      scales_list,
-      zps_list,
-      bias_list,
-      is_int4,
+      scales_list[0],
+      zps_list[0],
+      bias_list[0],
       lowp_mode,
-      num_concats,
-      WOQ_FUSE_ADD, // post op add
-      others,
-      act_quant_mode,
-      quant_w_mode,
-      group_size);
+      output);
+  at::add_out(accumu, output, accumu, a);
+  return accumu;
+}
+
+at::Tensor woq_linear_add_kernel(
+    const at::Tensor& self,
+    const at::Tensor& weight,
+    const std::vector<at::Tensor>& scales_list,
+    const std::vector<at::Tensor>& zps_list,
+    const std::vector<at::Tensor>& bias_list,
+    bool is_int4,
+    int64_t lowp_mode,
+    int64_t num_concats,
+    const std::vector<at::Tensor>& others) {
+  if (weight.dim() > 2) {
+    auto out = woq_tpp_gemm_kernel_stub(
+        kCPU,
+        self,
+        weight,
+        scales_list,
+        zps_list,
+        bias_list,
+        is_int4,
+        lowp_mode,
+        num_concats,
+        WOQ_FUSE_ADD, // post op add
+        others);
+    if (out.defined()) {
+      return out;
+    }
+  }
+  auto input_size = self.sizes();
+  std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
+  output_size.push_back(weight.size(0));
+  auto output = at::empty(output_size, self.options());
+  output.set_requires_grad(self.requires_grad());
+  woq_linear_kernel_output(
+      self,
+      weight,
+      scales_list[0],
+      zps_list[0],
+      bias_list[0],
+      lowp_mode,
+      output);
+  return at::add(output, others[0]);
 }
 
 at::Tensor woq_linear_add_add_kernel(
@@ -538,27 +694,41 @@ at::Tensor woq_linear_add_add_kernel(
     const std::vector<at::Tensor>& zps_list,
     const std::vector<at::Tensor>& bias_list,
     bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
     int64_t num_concats,
-    const std::vector<at::Tensor>& others,
-    int64_t act_quant_mode) {
-  int64_t quant_w_mode = group_size > 0 ? 1 : 0;
-  return woq_tpp_gemm_kernel_stub(
-      kCPU,
+    const std::vector<at::Tensor>& others) {
+  if (weight.dim() > 2) {
+    auto out = woq_tpp_gemm_kernel_stub(
+        kCPU,
+        self,
+        weight,
+        scales_list,
+        zps_list,
+        bias_list,
+        is_int4,
+        lowp_mode,
+        num_concats,
+        WOQ_FUSE_ADD_ADD, // post op add-add
+        others);
+    if (out.defined()) {
+      return out;
+    }
+  }
+  auto input_size = self.sizes();
+  std::vector<int64_t> output_size(input_size.begin(), input_size.end() - 1);
+  output_size.push_back(weight.size(0));
+  auto output = at::empty(output_size, self.options());
+  output.set_requires_grad(self.requires_grad());
+  woq_linear_kernel_output(
       self,
       weight,
-      scales_list,
-      zps_list,
-      bias_list,
-      is_int4,
+      scales_list[0],
+      zps_list[0],
+      bias_list[0],
       lowp_mode,
-      num_concats,
-      WOQ_FUSE_ADD_ADD, // post op add-add
-      others,
-      act_quant_mode,
-      quant_w_mode,
-      group_size);
+      output);
+  auto y = at::add(output, others[0]);
+  return at::add(y, others[1]);
 }
 
 at::Tensor woq_linear_add_forward(
@@ -582,74 +752,6 @@ at::Tensor woq_linear_add_add_forward(
       ->run_add_add(input, others);
 }
 
-at::Tensor matmul_i8i8i32(const at::Tensor& input, const at::Tensor& weight) {
-  // x:s8 * w:s8 -> y:s32
-  // No bias
-  TORCH_CHECK(
-      input.scalar_type() == c10::kChar,
-      "matmul_i8i8i32: input dtype should be signed int8 but found ",
-      input.scalar_type());
-  TORCH_CHECK(
-      weight.scalar_type() == c10::kChar,
-      "matmul_i8i8i32: weight dtype should be signed int8 but found ",
-      weight.scalar_type());
-  TORCH_CHECK(
-      input.dim() == 2 && weight.dim() == 2,
-      "matmul_i8i8i32: Expect Input and weight are 2d but got ",
-      input.dim(),
-      " and ",
-      weight.dim());
-  TORCH_CHECK(
-      input.size(1) == weight.size(1),
-      "matmul_i8i8i32: Input shape and weight shape do not match, got ",
-      input.sizes(),
-      " and ",
-      weight.sizes());
-  auto output_shape = input.sizes().vec();
-  output_shape.back() = weight.size(0);
-  auto output = at::empty(output_shape, input.options().dtype(c10::kInt));
-  auto input_contig = input.contiguous();
-  auto weight_contig = weight.t().contiguous();
-  // Create ideep tensors for oneDNN computation
-  auto src = ideep::tensor(
-      {input_contig.sizes().vec(),
-       ideep::tensor::data_type::s8,
-       input_contig.strides().vec()},
-      input_contig.data_ptr());
-  auto wei = ideep::tensor(
-      {weight_contig.sizes().vec(),
-       ideep::tensor::data_type::s8,
-       weight_contig.strides().vec()},
-      weight_contig.data_ptr());
-  auto dst = ideep::tensor(
-      {output.sizes().vec(),
-       ideep::tensor::data_type::s32,
-       output.strides().vec()},
-      output.data_ptr());
-  // Create primitive desc
-  auto engine = ideep::engine::cpu_engine();
-  ideep::attr_t op_attr;
-  op_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
-  auto src_desc = src.get_desc();
-  auto wei_desc = wei.get_desc();
-  auto dst_desc = dst.get_desc();
-  auto prim_desc = dnnl::matmul::primitive_desc(
-      engine, src_desc, wei_desc, dst_desc, op_attr);
-  // Reorder weight
-  auto expected_weight = wei.reorder_if_differ_in(prim_desc.weights_desc());
-  // Prepare args for primitive
-  ideep::tensor scratchpad(prim_desc.scratchpad_desc());
-  ideep::exec_args args;
-  args.insert({DNNL_ARG_SRC, src});
-  args.insert({DNNL_ARG_WEIGHTS, expected_weight});
-  args.insert({DNNL_ARG_DST, dst});
-  args.insert({DNNL_ARG_SCRATCHPAD, scratchpad});
-  // Create primitve and execute
-  auto primitive = dnnl::matmul(prim_desc);
-  primitive.execute(ideep::stream::default_stream(), args);
-  return output;
-}
-
 } // namespace cpu
 } // namespace torch_ipex
 
@@ -761,15 +863,6 @@ at::Tensor woq_linear_add_add_forward(
       cpu_cached_cast(target_type, others));
 }
 
-at::Tensor matmul_i8i8i32(const at::Tensor& input, const at::Tensor& weight) {
-  c10::impl::ExcludeDispatchKeyGuard no_autocastCPU(DispatchKey::AutocastCPU);
-  static auto op = torch::Dispatcher::singleton()
-                       .findSchemaOrThrow("torch_ipex::matmul_i8i8i32", "")
-                       .typed<decltype(matmul_i8i8i32)>();
-  // input is int8. Don't cast to autocast dtype
-  return op.call(input, weight);
-}
-
 } // namespace autocast
 } // namespace torch_ipex
 
@@ -855,14 +948,6 @@ TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
       "linear_eltwise_backward",
       c10::DispatchKey::CPU,
       torch_ipex::cpu::linear_eltwise_backward);
-  // bnb
-  m.def("matmul_i8i8i32(Tensor input, Tensor weight) -> Tensor");
-  m.impl(
-      "matmul_i8i8i32", c10::DispatchKey::CPU, torch_ipex::cpu::matmul_i8i8i32);
-  m.impl(
-      "matmul_i8i8i32",
-      c10::DispatchKey::AutocastCPU,
-      torch_ipex::autocast::matmul_i8i8i32);
 }
 
 } // namespace
diff --git a/csrc/cpu/aten/Linear.h b/csrc/cpu/aten/Linear.h
index 3ef0febf8..db3900b64 100644
--- a/csrc/cpu/aten/Linear.h
+++ b/csrc/cpu/aten/Linear.h
@@ -81,9 +81,8 @@ at::Tensor ipex_linear_eltwise(
 // WOQ linear ops
 at::Tensor woq_linear_pack_weight(
     const at::Tensor& weight,
-    std::vector<int64_t>& weight_shape,
-    bool is_4bit,
-    int64_t group_size,
+    const at::Tensor& scale,
+    const at::Tensor& zero_points,
     int64_t lowp_mode);
 
 at::Tensor woq_linear_unpack_weight(
@@ -91,6 +90,15 @@ at::Tensor woq_linear_unpack_weight(
     bool is_int4,
     int64_t lowp_mode);
 
+void woq_linear_kernel_output(
+    const at::Tensor& self,
+    const at::Tensor& weight,
+    const at::Tensor& scales_float,
+    const at::Tensor& zero_points_float,
+    const at::Tensor& bias,
+    int64_t lowp_mode,
+    at::Tensor& output);
+
 at::Tensor woq_linear_kernel(
     const at::Tensor& self,
     const at::Tensor& weight,
@@ -98,10 +106,20 @@ at::Tensor woq_linear_kernel(
     const std::vector<at::Tensor>& zps_list,
     const std::vector<at::Tensor>& bias_list,
     bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t act_quant_mode);
+    int64_t num_concats);
+
+void woq_linear_eltwise_kernel_output(
+    const at::Tensor& self,
+    const at::Tensor& weight,
+    const at::Tensor& scales_float,
+    const at::Tensor& zero_points_float,
+    const at::Tensor& bias,
+    const c10::string_view& post_op,
+    const torch::List<c10::optional<at::Scalar>>& scalars,
+    const c10::optional<c10::string_view>& algorithm,
+    int64_t lowp_mode,
+    at::Tensor& output);
 
 at::Tensor woq_linear_eltwise_kernel(
     const at::Tensor& self,
@@ -113,10 +131,20 @@ at::Tensor woq_linear_eltwise_kernel(
     const torch::List<c10::optional<at::Scalar>>& scalars,
     const c10::optional<c10::string_view>& algorithm,
     bool is_int4,
-    int64_t group_size,
+    int64_t lowp_mode,
+    int64_t num_concats);
+
+at::Tensor woq_linear_add_kernel(
+    const at::Tensor& self,
+    const at::Tensor& weight,
+    const std::vector<at::Tensor>& scales_list,
+    const std::vector<at::Tensor>& zps_list,
+    const std::vector<at::Tensor>& bias_list,
+    bool is_int4,
     int64_t lowp_mode,
     int64_t num_concats,
-    int64_t act_quant_mode);
+    at::Tensor& accumu,
+    const c10::optional<at::Scalar>& alpha);
 
 at::Tensor woq_linear_add_kernel(
     const at::Tensor& self,
@@ -125,11 +153,9 @@ at::Tensor woq_linear_add_kernel(
     const std::vector<at::Tensor>& zps_list,
     const std::vector<at::Tensor>& bias_list,
     bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
     int64_t num_concats,
-    const std::vector<at::Tensor>& others,
-    int64_t act_quant_mode);
+    const std::vector<at::Tensor>& others);
 
 at::Tensor woq_linear_add_add_kernel(
     const at::Tensor& self,
@@ -138,11 +164,9 @@ at::Tensor woq_linear_add_add_kernel(
     const std::vector<at::Tensor>& zps_list,
     const std::vector<at::Tensor>& bias_list,
     bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
     int64_t num_concats,
-    const std::vector<at::Tensor>& others,
-    int64_t act_quant_mode);
+    const std::vector<at::Tensor>& others);
 
 namespace {
 void woq_gemm_kernel_impl(
@@ -216,10 +240,7 @@ using woq_tpp_gemm_kernel_fn = at::Tensor (*)(
     int64_t,
     int64_t,
     int64_t,
-    const std::vector<at::Tensor>&,
-    int64_t,
-    int64_t,
-    int64_t);
+    const std::vector<at::Tensor>&);
 
 using woq_tpp_gemm_packB_fn =
     at::Tensor (*)(const at::Tensor&, bool, size_t, size_t, int64_t);
diff --git a/csrc/cpu/aten/MergedEmbWithCat.cpp b/csrc/cpu/aten/MergedEmbWithCat.cpp
new file mode 100644
index 000000000..bb4d8bede
--- /dev/null
+++ b/csrc/cpu/aten/MergedEmbWithCat.cpp
@@ -0,0 +1,56 @@
+#include "MergedEmbWithCat.h"
+#include <ATen/AccumulateType.h>
+#include <ATen/Tensor.h>
+#include <torch/all.h>
+#include "autocast/autocast_mode.h"
+#include "cpu/kernels/Embeddingbag.h"
+
+namespace torch_ipex {
+namespace cpu {
+
+using namespace at;
+DEFINE_DISPATCH(mlperf_merged_emb_cat_kernel_stub);
+DEFINE_DISPATCH(qmlperf_merged_emb_cat_kernel_stub);
+
+Tensor mlperf_merged_emb_cat(
+    const TensorList& weights,
+    const TensorList& index,
+    const Tensor& dense,
+    const IntArrayRef multihot) {
+  return mlperf_merged_emb_cat_kernel_stub(kCPU, weights, index, dense, multihot);
+}
+
+Tensor dil_qmlperf_merged_emb_cat(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot,
+    double o_scale,
+    int64_t o_zp,
+    at::ScalarType odtype) {
+  std::vector<double> w_scale;
+  w_scale.reserve(weights.size());
+  for (int i = 0; i < weights.size(); i++){
+    w_scale[i] =at::native::q_scale_quant(weights[i]);
+  }
+  double dx_scale = at::native::q_scale_quant(dense);
+  int64_t dx_zp = at::native::q_zero_point_quant(dense);
+  return qmlperf_merged_emb_cat_kernel_stub(
+      kCPU, weights, index, dense, multihot, w_scale, dx_scale, dx_zp, o_scale, o_zp);
+}
+
+} // namespace cpu
+} // namespace torch_ipex
+
+namespace {
+
+TORCH_LIBRARY_FRAGMENT(torch_ipex, m) {
+  m.def(
+      "mlperf_merged_emb_cat(Tensor[] weights, Tensor[] index, Tensor dense, int[] multihot) -> Tensor");
+  m.impl(
+      "mlperf_merged_emb_cat",
+      c10::DispatchKey::CPU,
+      torch_ipex::cpu::mlperf_merged_emb_cat);
+}
+
+} // namespace
diff --git a/csrc/cpu/aten/MergedEmbWithCat.h b/csrc/cpu/aten/MergedEmbWithCat.h
new file mode 100644
index 000000000..4bc53393c
--- /dev/null
+++ b/csrc/cpu/aten/MergedEmbWithCat.h
@@ -0,0 +1,62 @@
+#include <ATen/AccumulateType.h>
+#include <ATen/Tensor.h>
+#include <dyndisp/DispatchStub.h>
+#include <torch/all.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+at::Tensor dil_qmlperf_merged_emb_cat(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot,
+    double o_scale,
+    int64_t o_zp,
+    at::ScalarType odtype);
+namespace {
+
+at::Tensor mlperf_merged_emb_cat_kernel_impl(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot);
+
+at::Tensor qmlperf_merged_emb_cat_kernel_impl(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot,
+    const c10::ArrayRef<double> w_scale,
+    const double dx_scale,
+    const int64_t dx_zp,
+    const double o_scale,
+    const int64_t o_zp);
+
+} // namespace
+
+using mlperf_merged_emb_cat_kernel_fn = at::Tensor (*)(
+    const at::TensorList&,
+    const at::TensorList&,
+    const at::Tensor&,
+    const at::IntArrayRef);
+DECLARE_DISPATCH(
+    mlperf_merged_emb_cat_kernel_fn,
+    mlperf_merged_emb_cat_kernel_stub);
+
+using qmlperf_merged_emb_cat_kernel_fn = at::Tensor (*)(
+    const at::TensorList&,
+    const at::TensorList&,
+    const at::Tensor&,
+    const at::IntArrayRef,
+    const c10::ArrayRef<double>,
+    const double,
+    const int64_t,
+    const double,
+    const int64_t);
+DECLARE_DISPATCH(
+    qmlperf_merged_emb_cat_kernel_fn,
+    qmlperf_merged_emb_cat_kernel_stub);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/MergedEmbCatKrnl.cpp b/csrc/cpu/aten/kernels/MergedEmbCatKrnl.cpp
index 1ddcce049..43f56340b 100644
--- a/csrc/cpu/aten/kernels/MergedEmbCatKrnl.cpp
+++ b/csrc/cpu/aten/kernels/MergedEmbCatKrnl.cpp
@@ -144,7 +144,7 @@ typename std::enable_if<std::is_same<data_t, float>::value, void>::
         data_t* result) {
 #if defined(CPU_CAPABILITY_AVX512)
   // FP32 avx512 fast path for emb_dim=128
-  // ~7% improvement while benchmarking on SPR 56C/S with 1 S.
+  // ~7% improvement while benchmarking on EMR 56C/S with 1 S.
   // benchmark config: num_emb=26, emb_dim=128, batch_size=32768
   // num_bags = [3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1] for
   // each table
@@ -217,7 +217,7 @@ typename std::enable_if<std::is_same<data_t, Half>::value, void>::
         data_t* result) {
 #if defined(CPU_CAPABILITY_AVX512_FP16)
   // FP16 avx512_fp16 fast path for emb_dim=128
-  // only ~1.5% improvement while benchmarking on SPR 56C/S with 1 S.
+  // only ~1.5% improvement while benchmarking on EMR 56C/S with 1 S.
   // benchmark config: num_emb=26, emb_dim=128, batch_size=32768
   // num_bags = [3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1] for
   // each table
@@ -340,7 +340,7 @@ typename std::enable_if<std::is_same<data_t, BFloat16>::value, void>::
         data_t* result) {
 #if defined(CPU_CAPABILITY_AVX512_BF16)
   // BF16 avx512_bf16 fast path for emb_dim=128
-  // ~30% improvement while benchmarking on SPR 56C/S with 1 S.
+  // ~30% improvement while benchmarking on EMR 56C/S with 1 S.
   // benchmark config: num_emb=26, emb_dim=128, batch_size=32768
   // num_bags = [3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1] for
   // each table
@@ -612,4 +612,4 @@ REGISTER_DISPATCH(
     &merged_embedding_cat_fw_impl);
 
 } // namespace cpu
-} // namespace torch_ipex
\ No newline at end of file
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/MergedEmbWithCatKrnl.cpp b/csrc/cpu/aten/kernels/MergedEmbWithCatKrnl.cpp
new file mode 100644
index 000000000..bfd3a71fc
--- /dev/null
+++ b/csrc/cpu/aten/kernels/MergedEmbWithCatKrnl.cpp
@@ -0,0 +1,677 @@
+#include <aten/MergedEmbWithCat.h>
+#include <torch/all.h>
+#include <torch/csrc/autograd/function.h>
+#include "vec/vec.h"
+#include <ATen/quantized/Quantizer.h>
+
+namespace torch_ipex {
+namespace cpu {
+
+namespace {
+
+inline void embeddingbag_kern_ps(
+    const int64_t batch,
+    const int64_t num_emb,
+    const int64_t emb_dim,
+    const int64_t num_hot,
+    const int64_t* index,
+    const float* weight,
+    float* result) {
+  // batch int64_t
+  // num_hot int64_t
+  // index, index of 1 embedding table [batch*num_hot], no offset required for
+  // mlperf case weight, embeddingtable weight, [len_emb, emb_dim] float output
+  // result float [batch, num_emb, emb_dim] scale float
+#if defined(CPU_CAPABILITY_AVX512)
+  for (int64_t b = 0; b < batch; ++b) {
+    __m512 x0, x1, x2, x3, x4, x5, x6, x7;
+    int64_t ofst_idx0 = b * num_hot;
+    int64_t idx = index[ofst_idx0] * emb_dim;
+    x0 = _mm512_load_ps(&weight[idx]);
+    x1 = _mm512_load_ps(&weight[idx + 16]);
+    x2 = _mm512_load_ps(&weight[idx + 32]);
+    x3 = _mm512_load_ps(&weight[idx + 48]);
+    x4 = _mm512_load_ps(&weight[idx + 64]);
+    x5 = _mm512_load_ps(&weight[idx + 80]);
+    x6 = _mm512_load_ps(&weight[idx + 96]);
+    x7 = _mm512_load_ps(&weight[idx + 112]);
+    for (int64_t j = 1; j < num_hot; ++j) {
+      idx = index[ofst_idx0 + j] * emb_dim;
+      x0 = _mm512_add_ps(x0, _mm512_load_ps(&weight[idx]));
+      x1 = _mm512_add_ps(x1, _mm512_load_ps(&weight[idx + 16]));
+      x2 = _mm512_add_ps(x2, _mm512_load_ps(&weight[idx + 32]));
+      x3 = _mm512_add_ps(x3, _mm512_load_ps(&weight[idx + 48]));
+      x4 = _mm512_add_ps(x4, _mm512_load_ps(&weight[idx + 64]));
+      x5 = _mm512_add_ps(x5, _mm512_load_ps(&weight[idx + 80]));
+      x6 = _mm512_add_ps(x6, _mm512_load_ps(&weight[idx + 96]));
+      x7 = _mm512_add_ps(x7, _mm512_load_ps(&weight[idx + 112]));
+    }
+    _mm512_store_ps(result, x0);
+    _mm512_store_ps(result + 16, x1);
+    _mm512_store_ps(result + 32, x2);
+    _mm512_store_ps(result + 48, x3);
+    _mm512_store_ps(result + 64, x4);
+    _mm512_store_ps(result + 80, x5);
+    _mm512_store_ps(result + 96, x6);
+    _mm512_store_ps(result + 112, x7);
+    result += (num_emb + 1) * emb_dim;
+  }
+#endif
+}
+
+inline void
+copy_kern_ps(const int64_t batch,
+             const int64_t num_emb,
+             int64_t emb_dim,
+             const float *dense,
+             float *result) {
+#if defined(CPU_CAPABILITY_AVX512)
+    for (int64_t b = 0; b < batch; ++b) {
+        __m512 x0 = _mm512_load_ps(dense);
+        __m512 x1 = _mm512_load_ps(dense + 16);
+        __m512 x2 = _mm512_load_ps(dense + 32);
+        __m512 x3 = _mm512_load_ps(dense + 48);
+        __m512 x4 = _mm512_load_ps(dense + 64);
+        __m512 x5 = _mm512_load_ps(dense + 80);
+        __m512 x6 = _mm512_load_ps(dense + 96);
+        __m512 x7 = _mm512_load_ps(dense + 112);
+        _mm512_store_ps(result, x0);
+        _mm512_store_ps(result + 16, x1);
+        _mm512_store_ps(result + 32, x2);
+        _mm512_store_ps(result + 48, x3);
+        _mm512_store_ps(result + 64, x4);
+        _mm512_store_ps(result + 80, x5);
+        _mm512_store_ps(result + 96, x6);
+        _mm512_store_ps(result + 112, x7);
+        result += (num_emb + 1) * emb_dim;
+        dense += emb_dim;
+    }
+#endif
+}
+
+void embeddingbagcat_f32(
+    int64_t batch,
+    int64_t num_emb,
+    int64_t emb_dim,
+    const int64_t* num_hot,
+    int64_t** index,
+    int64_t** offset,
+    float** weight,
+    float* dense,
+    float* result) {
+  constexpr int64_t mb = 128;
+  const int64_t bb = (batch - 1) / mb + 1;
+#pragma omp parallel for collapse(2)
+  for (int64_t b = 0; b < bb; ++b) {
+    for (int64_t n = 0; n < (num_emb + 1); ++n) {
+      const int64_t cur_batch = std::min(mb * (b + 1), batch) - b * mb;
+      float* r = &result[b *mb *27 *128 + n *128];
+      if (n == 0) {
+        copy_kern_ps(cur_batch,
+                     num_emb,
+                     emb_dim,
+                     &dense[b *mb *128],
+                     r);
+      } else {
+        const int64_t m = n - 1;
+        const int64_t idx_ofs = b * mb * num_hot[m];
+        embeddingbag_kern_ps(cur_batch,
+                            num_emb,
+                            emb_dim,
+                            num_hot[m],
+                            &index[m][idx_ofs],
+                            weight[m],
+                            r);
+      }
+    }
+  }
+}
+
+at::Tensor mlperf_merged_emb_cat_kernel_impl(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot) {
+  constexpr int64_t num_emb = 26;
+  constexpr int64_t emb_dim = 128;
+//   assert(weights.size() = num_emb);
+//   assert(index.size() = num_emb);
+//   assert(multihot.size() = num_emb);
+
+  int64_t batch_size = index[0].size(0) / multihot[0];
+  float* w_ptr[num_emb];
+  int64_t* index_ptr[num_emb];
+  for (int i = 0; i < num_emb; i++) {
+    w_ptr[i] = weights[i].data_ptr<float>();
+    index_ptr[i] = index[i].data_ptr<int64_t>();
+  }
+  const int64_t* multihot_ptr = multihot.data();
+  at::Tensor output = at::empty({batch_size, (num_emb + 1) * emb_dim}, weights[0].options());
+  float* o_ptr = output.data_ptr<float>();
+  float* d_ptr = dense.data_ptr<float>();
+  embeddingbagcat_f32(
+      batch_size,
+      num_emb,
+      emb_dim,
+      multihot_ptr,
+      index_ptr,
+      index_ptr,
+      w_ptr,
+      d_ptr,
+      o_ptr);
+  return output;
+}
+
+inline void qembeddingbag_kern_ps(
+    const int64_t batch,
+    const int64_t num_emb,
+    const int64_t num_hot,
+    const int64_t emb_dim,
+    const int64_t* index,
+    const int8_t* weight,
+    int8_t* result,
+    const float scale) {
+  // batch int64_t
+  // num_hot int64_t
+  // index, index of 1 embedding table [batch*num_hot], no offset required for
+  // mlperf case weight, embeddingtable weight, [len_emb, emb_dim] int8 output
+  // result int8 [batch, num_emb, emb_dim] scale float
+#if defined(CPU_CAPABILITY_AVX512)
+  __m512 scale_v = _mm512_set1_ps(scale);
+  for (int64_t b = 0; b < batch; ++b) {
+    __m512i x00, x64;
+    __m512i y0, y1, y2, y3, y4, y5, y6, y7;
+    __m512 f0, f1, f2, f3, f4, f5, f6, f7;
+    int64_t ofst_idx0 = b * num_hot;
+    int64_t idx = index[ofst_idx0] * emb_dim;
+    x00 = _mm512_load_si512(&weight[idx]);
+    x64 = _mm512_load_si512(&weight[idx + 64]);
+    y0 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 0));
+    y1 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 1));
+    y2 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 2));
+    y3 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 3));
+    y4 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 0));
+    y5 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 1));
+    y6 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 2));
+    y7 = _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 3));
+    for (int64_t j = 1; j < num_hot; ++j) {
+      idx = index[ofst_idx0 + j] * emb_dim;
+      x00 = _mm512_load_si512(&weight[idx]);
+      x64 = _mm512_load_si512(&weight[idx + 64]);
+      y0 = _mm512_add_epi32(
+          y0, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 0)));
+      y1 = _mm512_add_epi32(
+          y1, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 1)));
+      y2 = _mm512_add_epi32(
+          y2, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 2)));
+      y3 = _mm512_add_epi32(
+          y3, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x00, 3)));
+      y4 = _mm512_add_epi32(
+          y4, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 0)));
+      y5 = _mm512_add_epi32(
+          y5, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 1)));
+      y6 = _mm512_add_epi32(
+          y6, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 2)));
+      y7 = _mm512_add_epi32(
+          y7, _mm512_cvtepi8_epi32(_mm512_extracti32x4_epi32(x64, 3)));
+    }
+    f0 = _mm512_cvt_roundepi32_ps(
+        y0, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f1 = _mm512_cvt_roundepi32_ps(
+        y1, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f2 = _mm512_cvt_roundepi32_ps(
+        y2, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f3 = _mm512_cvt_roundepi32_ps(
+        y3, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f4 = _mm512_cvt_roundepi32_ps(
+        y4, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f5 = _mm512_cvt_roundepi32_ps(
+        y5, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f6 = _mm512_cvt_roundepi32_ps(
+        y6, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f7 = _mm512_cvt_roundepi32_ps(
+        y7, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    f0 = _mm512_mul_ps(f0, scale_v);
+    f1 = _mm512_mul_ps(f1, scale_v);
+    f2 = _mm512_mul_ps(f2, scale_v);
+    f3 = _mm512_mul_ps(f3, scale_v);
+    f4 = _mm512_mul_ps(f4, scale_v);
+    f5 = _mm512_mul_ps(f5, scale_v);
+    f6 = _mm512_mul_ps(f6, scale_v);
+    f7 = _mm512_mul_ps(f7, scale_v);
+    y0 = _mm512_cvt_roundps_epi32(
+        f0, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y1 = _mm512_cvt_roundps_epi32(
+        f1, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y2 = _mm512_cvt_roundps_epi32(
+        f2, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y3 = _mm512_cvt_roundps_epi32(
+        f3, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y4 = _mm512_cvt_roundps_epi32(
+        f4, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y5 = _mm512_cvt_roundps_epi32(
+        f5, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y6 = _mm512_cvt_roundps_epi32(
+        f6, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    y7 = _mm512_cvt_roundps_epi32(
+        f7, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
+    x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y0), 0);
+    x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y1), 1);
+    x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y2), 2);
+    x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y3), 3);
+    x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y4), 0);
+    x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y5), 1);
+    x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y6), 2);
+    x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y7), 3);
+    _mm512_store_si512(result, x00);
+    _mm512_store_si512(result + 64, x64);
+    result += (num_emb + 1) * emb_dim;
+  }
+#endif
+}
+
+inline void qembeddingbag_kern_ph(const int64_t batch,
+                                  const int64_t num_emb,
+                                  const int64_t num_hot,
+                                  const int64_t emb_dim,
+                                  const int64_t * index,
+                                  const int8_t *weight,
+                                  int8_t *result,
+                                  const float scale,
+                                  const int8_t ot_zp) {
+#if defined(CPU_CAPABILITY_AVX512_FP16)
+    // batch int64_t
+    // num_hot int64_t
+    // index, index of 1 embedding table [batch*num_hot], no offset required for mlperf case
+    // weight, embeddingtable weight, [len_emb, emb_dim] int8
+    // output result int8 [batch, num_emb, emb_dim]
+    // scale float
+    __m512h scale_v = (__m512h)_mm512_broadcast_f32x8(
+        (__m256)_mm512_cvtps_ph(_mm512_set1_ps(scale),
+                                _MM_FROUND_TO_NEAREST_INT));
+    __m512h zp_ot_v = _mm512_cvtepi16_ph(_mm512_set1_epi16(ot_zp));
+    for (int64_t b = 0; b < batch; ++b) {
+        __m512i x00, x64;
+        __m512i y00, y32, y64, y96;
+        __m512h h00, h32, h64, h96;
+        int64_t ofst_idx0 = b * num_hot;
+        int64_t idx = index[ofst_idx0] * emb_dim;
+        x00 = _mm512_load_si512(&weight[idx]);
+        x64 = _mm512_load_si512(&weight[idx + 64]);
+        y00 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x00, 0));
+        y32 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x00, 1));
+        y64 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x64, 0));
+        y96 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x64, 1));
+        for (int64_t j = 1; j < num_hot; ++j) {
+            idx = index[ofst_idx0 + j] * emb_dim;
+            x00 = _mm512_load_si512(&weight[idx]);
+            x64 = _mm512_load_si512(&weight[idx + 64]);
+            y00 = _mm512_add_epi16(y00,
+                                   _mm512_cvtepi8_epi16(
+                                       _mm512_extracti32x8_epi32(x00, 0)));
+            y32 = _mm512_add_epi16(y32,
+                                   _mm512_cvtepi8_epi16(
+                                       _mm512_extracti32x8_epi32(x00, 1)));
+            y64 = _mm512_add_epi16(y64,
+                                   _mm512_cvtepi8_epi16(
+                                       _mm512_extracti32x8_epi32(x64, 0)));
+            y96 = _mm512_add_epi16(y96,
+                                   _mm512_cvtepi8_epi16(
+                                       _mm512_extracti32x8_epi32(x64, 1)));
+        }
+        h00 = _mm512_cvt_roundepi16_ph(y00,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h32 = _mm512_cvt_roundepi16_ph(y32,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h64 = _mm512_cvt_roundepi16_ph(y64,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h96 = _mm512_cvt_roundepi16_ph(y96,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h00 = _mm512_mul_ph(h00, scale_v);
+        h32 = _mm512_mul_ph(h32, scale_v);
+        h64 = _mm512_mul_ph(h64, scale_v);
+        h96 = _mm512_mul_ph(h96, scale_v);
+
+        h00 = _mm512_add_ph(h00, zp_ot_v);
+        h32 = _mm512_add_ph(h32, zp_ot_v);
+        h64 = _mm512_add_ph(h64, zp_ot_v);
+        h96 = _mm512_add_ph(h96, zp_ot_v);
+
+        y00 = _mm512_cvt_roundph_epi16(h00,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        y32 = _mm512_cvt_roundph_epi16(h32,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        y64 = _mm512_cvt_roundph_epi16(h64,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        y96 = _mm512_cvt_roundph_epi16(h96,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+
+        x00 = _mm512_inserti64x4(x00, _mm512_cvtsepi16_epi8(y00), 0);
+        x00 = _mm512_inserti64x4(x00, _mm512_cvtsepi16_epi8(y32), 1);
+        x64 = _mm512_inserti64x4(x64, _mm512_cvtsepi16_epi8(y64), 0);
+        x64 = _mm512_inserti64x4(x64, _mm512_cvtsepi16_epi8(y96), 1);
+        _mm512_store_si512(result, x00);
+        _mm512_store_si512(result + 64, x64);
+        result += (num_emb + 1) * emb_dim;
+    }
+#endif
+}
+
+inline void
+scalecopy_kern_ps(const int64_t batch,
+                  const int64_t num_emb,
+                  int64_t emb_dim,
+                  const int8_t *dense,
+                  int8_t *result,
+                  const float scale) {
+#if defined(CPU_CAPABILITY_AVX512)
+    __m512 scale_v = _mm512_set1_ps(scale);
+    for (int64_t b = 0; b < batch; ++b) {
+        __m512i x00, x64;
+        __m512i y0, y1, y2, y3, y4, y5, y6, y7;
+        __m512 f0, f1, f2, f3, f4, f5, f6, f7;
+        x00 = _mm512_load_si512(dense);
+        x64 = _mm512_load_si512(dense + 64);
+        y0 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x00, 0));
+        y1 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x00, 1));
+        y2 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x00, 2));
+        y3 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x00, 3));
+        y4 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x64, 0));
+        y5 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x64, 1));
+        y6 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x64, 2));
+        y7 = _mm512_cvtepi8_epi32(
+            _mm512_extracti32x4_epi32(x64, 3));
+        f0 = _mm512_cvt_roundepi32_ps(y0,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f1 = _mm512_cvt_roundepi32_ps(y1,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f2 = _mm512_cvt_roundepi32_ps(y2,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f3 = _mm512_cvt_roundepi32_ps(y3,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f4 = _mm512_cvt_roundepi32_ps(y4,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f5 = _mm512_cvt_roundepi32_ps(y5,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f6 = _mm512_cvt_roundepi32_ps(y6,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f7 = _mm512_cvt_roundepi32_ps(y7,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        f0 = _mm512_mul_ps(f0, scale_v);
+        f1 = _mm512_mul_ps(f1, scale_v);
+        f2 = _mm512_mul_ps(f2, scale_v);
+        f3 = _mm512_mul_ps(f3, scale_v);
+        f4 = _mm512_mul_ps(f4, scale_v);
+        f5 = _mm512_mul_ps(f5, scale_v);
+        f6 = _mm512_mul_ps(f6, scale_v);
+        f7 = _mm512_mul_ps(f7, scale_v);
+        y0 = _mm512_cvt_roundps_epi32(f0,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y1 = _mm512_cvt_roundps_epi32(f1,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y2 = _mm512_cvt_roundps_epi32(f2,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y3 = _mm512_cvt_roundps_epi32(f3,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y4 = _mm512_cvt_roundps_epi32(f4,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y5 = _mm512_cvt_roundps_epi32(f5,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y6 = _mm512_cvt_roundps_epi32(f6,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        y7 = _mm512_cvt_roundps_epi32(f7,
+                                      (_MM_FROUND_TO_NEAREST_INT |
+                                       _MM_FROUND_NO_EXC));
+        x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y0), 0);
+        x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y1), 1);
+        x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y2), 2);
+        x00 = _mm512_inserti32x4(x00, _mm512_cvtsepi32_epi8(y3), 3);
+        x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y4), 0);
+        x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y5), 1);
+        x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y6), 2);
+        x64 = _mm512_inserti32x4(x64, _mm512_cvtsepi32_epi8(y7), 3);
+        _mm512_store_si512(result, x00);
+        _mm512_store_si512(result + 64, x64);
+        result += (num_emb + 1) * emb_dim;
+        dense += emb_dim;
+    }
+#endif
+}
+
+inline void
+scalecopy_kern_ph(const int64_t batch,
+                  const int64_t num_emb,
+                  int64_t emb_dim,
+                  const int8_t *dense,
+                  int8_t *result,
+                  const float scale,
+                  const int8_t dx_zp,
+                  const int8_t ot_zp) {
+#if defined(CPU_CAPABILITY_AVX512_FP16)
+    __m512h scale_v = (__m512h)_mm512_broadcast_f32x8(
+        (__m256)_mm512_cvtps_ph(_mm512_set1_ps(scale),
+                                _MM_FROUND_TO_NEAREST_INT));
+    __m512h zp_dx_v = _mm512_cvtepi16_ph(_mm512_set1_epi16(-dx_zp));
+    __m512h zp_ot_v = _mm512_cvtepi16_ph(_mm512_set1_epi16(ot_zp));
+    for (int64_t b = 0; b < batch; ++b) {
+        __m512i x00, x64;
+        __m512i y00, y32, y64, y96;
+        __m512h h00, h32, h64, h96;
+        __m512i y0, y1, y2, y3, y4, y5, y6, y7;
+        __m512 f0, f1, f2, f3, f4, f5, f6, f7;
+        x00 = _mm512_load_si512(dense);
+        x64 = _mm512_load_si512(dense + 64);
+        y00 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x00, 0));
+        y32 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x00, 1));
+        y64 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x64, 0));
+        y96 = _mm512_cvtepi8_epi16(
+            _mm512_extracti32x8_epi32(x64, 1));
+
+        h00 = _mm512_cvt_roundepi16_ph(y00,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h32 = _mm512_cvt_roundepi16_ph(y32,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h64 = _mm512_cvt_roundepi16_ph(y64,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        h96 = _mm512_cvt_roundepi16_ph(y96,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+
+        h00 = _mm512_add_ph(h00, zp_dx_v);
+        h32 = _mm512_add_ph(h32, zp_dx_v);
+        h64 = _mm512_add_ph(h64, zp_dx_v);
+        h96 = _mm512_add_ph(h96, zp_dx_v);
+
+        h00 = _mm512_mul_ph(h00, scale_v);
+        h32 = _mm512_mul_ph(h32, scale_v);
+        h64 = _mm512_mul_ph(h64, scale_v);
+        h96 = _mm512_mul_ph(h96, scale_v);
+
+        h00 = _mm512_add_ph(h00, zp_ot_v);
+        h32 = _mm512_add_ph(h32, zp_ot_v);
+        h64 = _mm512_add_ph(h64, zp_ot_v);
+        h96 = _mm512_add_ph(h96, zp_ot_v);
+
+        y00 = _mm512_cvt_roundph_epi16(h00,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        y32 = _mm512_cvt_roundph_epi16(h32,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        y64 = _mm512_cvt_roundph_epi16(h64,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+        y96 = _mm512_cvt_roundph_epi16(h96,
+                                       (_MM_FROUND_TO_NEAREST_INT |
+                                        _MM_FROUND_NO_EXC));
+
+        x00 = _mm512_inserti64x4(x00,
+                                 _mm512_cvtsepi16_epi8(y00), 0);
+        x00 = _mm512_inserti64x4(x00,
+                                 _mm512_cvtsepi16_epi8(y32), 1);
+        x64 = _mm512_inserti64x4(x64,
+                                 _mm512_cvtsepi16_epi8(y64), 0);
+        x64 = _mm512_inserti64x4(x64,
+                                 _mm512_cvtsepi16_epi8(y96), 1);
+        _mm512_store_si512(result, x00);
+        _mm512_store_si512(result + 64, x64);
+        result += (num_emb + 1) * emb_dim;
+        dense += emb_dim;
+    }
+#endif
+}
+
+void qembeddingbagcat(
+    int64_t batch,
+    int64_t num_emb,
+    int64_t emb_dim,
+    const int64_t* num_hot,
+    int64_t** index,
+    int64_t** offset,
+    int8_t** weight,
+    int8_t* dense,
+    int8_t* result,
+    const double* in_scale,
+    const double dx_scale,
+    const int64_t dx_zp,
+    const double ot_scale,
+    const int64_t ot_zp) {
+  constexpr int64_t mb = 512;
+  const int64_t bb = (batch - 1) / mb + 1;
+#pragma omp parallel for collapse(2) schedule(auto)
+  for (int64_t b = 0; b < bb; ++b) {
+    for (int64_t n = 0; n < (num_emb + 1); ++n) {
+      const int64_t cur_batch = std::min(mb * (b + 1), batch) - b * mb;
+      // shiyang: emb_dim need to be 128
+      int8_t *r = &result[b *mb *27 *128 + n *128];
+      if (n == 0) {
+        const float scale = dx_scale / ot_scale;
+        scalecopy_kern_ph(cur_batch,
+                          num_emb,
+                          emb_dim,
+                          &dense[b *mb *128],
+                          r,
+                          scale,
+                          dx_zp,
+                          ot_zp);
+      } else {
+        const int64_t m = n - 1;
+        const int64_t idx_ofs = b *mb *num_hot[m];
+        const float scale = in_scale[m] / ot_scale;
+        qembeddingbag_kern_ph(cur_batch,
+                              num_emb,
+                              num_hot[m],
+                              emb_dim,
+                              &index[m][idx_ofs],
+                              weight[m],
+                              r,
+                              scale,
+                              ot_zp);
+      }
+    }
+  }
+}
+
+at::Tensor qmlperf_merged_emb_cat_kernel_impl(
+    const at::TensorList& weights,
+    const at::TensorList& index,
+    const at::Tensor& dense,
+    const at::IntArrayRef multihot,
+    const c10::ArrayRef<double> w_scale,
+    const double dx_scale,
+    const int64_t dx_zp,
+    const double o_scale,
+    const int64_t o_zp) {
+  constexpr int64_t num_emb = 26;
+  constexpr int64_t emb_dim = 128;
+
+//   assert(weights.size() = num_emb);
+//   assert(index.size() = num_emb);
+//   assert(multihot.size() = num_emb);
+
+  int64_t batch_size = index[0].size(0) / multihot[0];
+  int8_t* w_ptr[num_emb];
+  int64_t* index_ptr[num_emb];
+  for (int i = 0; i < num_emb; i++) {
+    w_ptr[i] = weights[i].data_ptr<int8_t>();
+    index_ptr[i] = index[i].data_ptr<int64_t>();
+  }
+  const int64_t* multihot_ptr = multihot.data();
+  const double* w_scale_ptr = w_scale.data();
+  at::QuantizerPtr output_quantizer =
+      at::make_per_tensor_affine_quantizer(o_scale, o_zp, at::kQInt8);
+  at::Tensor output = at::new_qtensor(
+      /*sizes=*/{batch_size, (num_emb + 1) * emb_dim},
+      weights[0].options(),
+      output_quantizer);
+  int8_t* o_ptr = output.data_ptr<int8_t>();
+  int8_t* d_ptr = dense.data_ptr<int8_t>();
+  qembeddingbagcat(
+      batch_size,
+      num_emb,
+      emb_dim,
+      multihot_ptr,
+      index_ptr,
+      index_ptr,
+      w_ptr,
+      d_ptr,
+      o_ptr,
+      w_scale_ptr,
+      dx_scale,
+      dx_zp,
+      o_scale,
+      o_zp);
+   return output;
+}
+
+} // anonymous namespace
+
+REGISTER_DISPATCH(
+    mlperf_merged_emb_cat_kernel_stub,
+    &mlperf_merged_emb_cat_kernel_impl);
+REGISTER_DISPATCH(
+    qmlperf_merged_emb_cat_kernel_stub,
+    &qmlperf_merged_emb_cat_kernel_impl);
+
+} // namespace cpu
+} // namespace torch_ipex
diff --git a/csrc/cpu/aten/kernels/WoqLinearKrnl.cpp b/csrc/cpu/aten/kernels/WoqLinearKrnl.cpp
index de9b84050..a861a336b 100644
--- a/csrc/cpu/aten/kernels/WoqLinearKrnl.cpp
+++ b/csrc/cpu/aten/kernels/WoqLinearKrnl.cpp
@@ -2658,16 +2658,6 @@ void woq_gemm_kernel_impl(
               zero_points_float_ptr);
         }
       }
-    } else {
-      auto qw = woq_linear_unpackB_impl(weight);
-      auto w = qw.dequantize().to(self_.scalar_type()).to(c10::kFloat);
-      auto x = self.to(c10::ScalarType::Float);
-      auto out = at::linear(x, w);
-      if (bias.defined()) {
-        auto b = bias.to(self_.scalar_type()).to(c10::kFloat);
-        out = at::add(out, b);
-      }
-      output = out.to(self.scalar_type());
     }
   } else { // kPerChannelAffineFloatQParams
 
@@ -2815,7 +2805,7 @@ void woq_gemm_kernel_impl(
     } else {
       at::linear_out(output, self, w);
     }
-  } else if (self_.scalar_type() == at::kBFloat16) {
+  } else {
     auto w = weight.dequantize();
     auto x = self.to(c10::ScalarType::Float);
     // This is to align with the AVX512 kernel
@@ -2828,15 +2818,6 @@ void woq_gemm_kernel_impl(
       out = at::add(out, bias);
     }
     output = out.to(self.scalar_type());
-  } else {
-    auto w = weight.dequantize().to(self_.scalar_type()).to(c10::kFloat);
-    auto x = self.to(c10::ScalarType::Float);
-    auto out = at::linear(x, w);
-    if (bias.defined()) {
-      auto b = bias.to(self_.scalar_type()).to(c10::kFloat);
-      out = at::add(out, b);
-    }
-    output = out.to(self.scalar_type());
   }
 
 #endif
diff --git a/csrc/cpu/aten/kernels/WoqTppKrnl.cpp b/csrc/cpu/aten/kernels/WoqTppKrnl.cpp
index db36265a2..f8554c715 100644
--- a/csrc/cpu/aten/kernels/WoqTppKrnl.cpp
+++ b/csrc/cpu/aten/kernels/WoqTppKrnl.cpp
@@ -2,7 +2,6 @@
 // #include <torch/extension.h>
 #include <ATen/ATen.h>
 #include <ATen/Tensor.h>
-#include <ATen/cpu/vec/functional.h>
 #include <ATen/cpu/vec/vec.h>
 #include <aten/Linear.h>
 #include "csrc/cpu/tpp/woq/tla.h"
@@ -21,15 +20,6 @@ namespace {
 using namespace tpp;
 using TensorList = std::vector<at::Tensor>;
 
-#define FUSE_GELU 1
-#define FUSE_ADD 2
-#define FUSE_ADD_ADD 3
-
-#define LOWP_MODE_NONE 0
-#define LOWP_MODE_FP16 1
-#define LOWP_MODE_BF16 2
-#define LOWP_MODE_INT8 3
-
 // We only build optimized kernels if AVX512_FP16 is supported and gcc>=12.3
 // Otherwise we just return empty results
 // TODO(Weiwen) Merge WoqTppKrnl.cpp and WoqLinearKrnl.cpp and put the latter in
@@ -41,15 +31,6 @@ using TensorList = std::vector<at::Tensor>;
 constexpr long PREFETCH_K_DIST = 64; // TODO(jgong5): do not hard-code
 constexpr long LOOP_K_UNROLL = 4; // TODO(jgong5): do not hard-code
 
-#define UNQUANT_A -1
-#define QUANT_A_PER_TENSOR 0
-#define QUANT_A_PER_K_BLOCK 1
-#define QUANT_A_PER_M 2
-#define QUANT_A_PER_M_K_BLOCK 3
-
-#define QUANT_W_PER_CHANNEL 0
-#define QUANT_W_PER_K_BLOCK 1
-
 template <long N_GROUP_SIZE, typename VAT, typename LUT>
 inline VAT load_dequant_zp_only_int4(uint8_t* p, VAT vzps, LUT lut) {
   TLA_ASSERT(false, "not implemented");
@@ -408,7 +389,6 @@ template <
     long ldb,
     bool transA = false,
     bool ACC = false,
-    int quant_a_mode = -1,
     long PREFETCH_K_DIST = 0,
     typename Enabled = void>
 struct GemmMicroKernel {
@@ -433,7 +413,6 @@ template <
     long ldb,
     bool transA,
     bool ACC,
-    int quant_a_mode,
     long PREFETCH_K_DIST>
 struct GemmMicroKernel<
     T,
@@ -445,7 +424,6 @@ struct GemmMicroKernel<
     ldb,
     transA,
     ACC,
-    quant_a_mode,
     PREFETCH_K_DIST,
     typename std::enable_if_t<
         std::is_same<T, float>::value || std::is_same<T, half>::value>> {
@@ -585,14 +563,7 @@ struct GemmMicroKernel<
 };
 
 #ifdef __AVX512VNNI__
-template <
-    long M,
-    long N,
-    long ldb,
-    bool transA,
-    bool ACC,
-    int quant_a_mode,
-    long PREFETCH_K_DIST>
+template <long M, long N, long ldb, bool transA, bool ACC, long PREFETCH_K_DIST>
 struct GemmMicroKernel<
     /*Tin*/ uint8_t,
     /*Tout*/ float,
@@ -603,7 +574,6 @@ struct GemmMicroKernel<
     ldb,
     transA,
     ACC,
-    quant_a_mode,
     PREFETCH_K_DIST> {
   template <bool is_int4>
   static inline void call(
@@ -615,9 +585,8 @@ struct GemmMicroKernel<
       long ldc,
       float* scales,
       int8_t* zps,
-      float* scale_a,
-      int32_t* zp_a,
-      int32_t k_groups) {
+      float scale_a,
+      int32_t zp_a) {
     auto pqB = GetVLAPtr<uint8_t>(B, {ldb, 2}); // [K/4,N,4] packed in 4-bit
 
     static_assert(N % 16 == 0, "N must be a multiple of 16");
@@ -685,32 +654,10 @@ struct GemmMicroKernel<
       constexpr const int col = i % COLS;
       // compute (qC - compensate * zp_a) * scale_a * scale_b
       // where compensate = sum(qB)
-      __m512 vc_float;
-      if constexpr (
-          quant_a_mode == QUANT_A_PER_TENSOR ||
-          quant_a_mode == QUANT_A_PER_K_BLOCK) {
-        vc[i] = _mm512_sub_epi32(
-            vc[i],
-            _mm512_mullo_epi32(vcompensate[col], _mm512_set1_epi32(*zp_a)));
-        vc_float = _mm512_cvtepi32_ps(vc[i]);
-        vc_float = _mm512_mul_ps(vc_float, _mm512_set1_ps(*scale_a));
-      } else if constexpr (quant_a_mode == QUANT_A_PER_M) {
-        vc[i] = _mm512_sub_epi32(
-            vc[i],
-            _mm512_mullo_epi32(
-                vcompensate[col], _mm512_set1_epi32(*(zp_a + row))));
-        vc_float = _mm512_cvtepi32_ps(vc[i]);
-        vc_float = _mm512_mul_ps(vc_float, _mm512_set1_ps(*(scale_a + row)));
-      } else {
-        vc[i] = _mm512_sub_epi32(
-            vc[i],
-            _mm512_mullo_epi32(
-                vcompensate[col], _mm512_set1_epi32(*(zp_a + row * k_groups))));
-        vc_float = _mm512_cvtepi32_ps(vc[i]);
-        vc_float = _mm512_mul_ps(
-            vc_float, _mm512_set1_ps(*(scale_a + row * k_groups)));
-      }
-
+      vc[i] = _mm512_sub_epi32(
+          vc[i], _mm512_mullo_epi32(vcompensate[col], _mm512_set1_epi32(zp_a)));
+      __m512 vc_float = _mm512_cvtepi32_ps(vc[i]);
+      vc_float = _mm512_mul_ps(vc_float, _mm512_set1_ps(scale_a));
       vc_float = _mm512_mul_ps(vc_float, vscales[col]);
       if constexpr (ACC) {
         auto vc_old = _mm512_loadu_ps(C + row * ldc + col * 16);
@@ -1108,7 +1055,6 @@ template <
     bool transA,
     bool ACC,
     bool is_int4,
-    int quant_a_mode,
     long PREFETCH_K_DIST = 0>
 class DequantGemmTPP {
  public:
@@ -1123,9 +1069,8 @@ class DequantGemmTPP {
       TZero* zps,
       Tout* C,
       bool no_tile_cfg = true,
-      float* scale_a = nullptr,
-      int32_t* zp_a = nullptr,
-      int32_t k_groups = -1) {
+      float scale_a = 1.0,
+      int32_t zp_a = 0) {
     TLA_ASSERT(false, "not implemented");
   }
 
@@ -1147,7 +1092,6 @@ template <
     bool transA,
     bool ACC,
     bool is_int4,
-    int quant_a_mode,
     long PREFETCH_K_DIST>
 class DequantGemmTPP<
     Tin,
@@ -1160,7 +1104,6 @@ class DequantGemmTPP<
     transA,
     ACC,
     is_int4,
-    quant_a_mode,
     PREFETCH_K_DIST> {
  public:
   DequantGemmTPP(long M, long K, long lda, long ldc)
@@ -1190,9 +1133,8 @@ class DequantGemmTPP<
       Tin* zps,
       Tout* C,
       bool no_tile_cfg = true,
-      float* scale_a = nullptr,
-      int32_t* zp_a = nullptr,
-      int32_t k_groups = -1) {
+      float scale_a = 1.0,
+      int32_t zp_a = 0) {
     if (M < SMALL_BATCH_THRESHOLD &&
         ((std::is_same<Tin, half>() && std::is_same<Tout, half>()) ||
          (std::is_same<Tin, float>() && std::is_same<Tout, float>()))) {
@@ -1211,7 +1153,6 @@ class DequantGemmTPP<
                   ldb,
                   transA,
                   ACC,
-                  quant_a_mode,
                   PREFETCH_K_DIST>::
                   template call<is_int4>(
                       K,
@@ -1237,7 +1178,6 @@ class DequantGemmTPP<
                         ldb,
                         transA,
                         ACC,
-                        quant_a_mode,
                         PREFETCH_K_DIST>::
                         template call<is_int4>(
                             K,
@@ -1288,7 +1228,6 @@ template <
     long ldb,
     bool transA,
     bool ACC,
-    int quant_a_mode,
     long PREFETCH_K_DIST>
 class DequantGemmTPP<
     /*Tin*/ uint8_t,
@@ -1301,7 +1240,6 @@ class DequantGemmTPP<
     transA,
     ACC,
     /*is_int4*/ true,
-    quant_a_mode,
     PREFETCH_K_DIST> {
   using TBrgemmTPP = BrgemmTPP<int8_t, int32_t>;
 
@@ -1333,9 +1271,8 @@ class DequantGemmTPP<
       int8_t* zps,
       float* C,
       bool no_tile_cfg = true,
-      float* scale_a = nullptr,
-      int32_t* zp_a = nullptr,
-      int32_t k_groups = -1) {
+      float scale_a = 1.0,
+      int32_t zp_a = 0) {
     auto qA = GetVLAPtr<uint8_t>(A, {lda});
 #ifdef __AVX512VNNI__
     if (M < SMALL_BATCH_THRESHOLD) {
@@ -1343,17 +1280,6 @@ class DequantGemmTPP<
           BLOCK_M * N / 16 >= 16 ? BLOCK_M / 2 : BLOCK_M;
       for (long m = 0; m < M; m += PREFERRED_BLOCK_M) {
         long block_m = std::min(M - m, PREFERRED_BLOCK_M);
-        float* scale_a_m;
-        int32_t* zp_a_m;
-        if constexpr (
-            quant_a_mode == QUANT_A_PER_M ||
-            quant_a_mode == QUANT_A_PER_M_K_BLOCK) {
-          scale_a_m = scale_a + m * k_groups;
-          zp_a_m = zp_a + m * k_groups;
-        } else {
-          scale_a_m = scale_a;
-          zp_a_m = zp_a;
-        }
         enumerate_dispatcher<long, 4, PREFERRED_BLOCK_M>::call(
             block_m,
             [&](auto i) {
@@ -1367,7 +1293,6 @@ class DequantGemmTPP<
                   ldb,
                   /*transA*/ false,
                   ACC,
-                  quant_a_mode,
                   PREFETCH_K_DIST>::
                   template call<true>(
                       K,
@@ -1378,9 +1303,8 @@ class DequantGemmTPP<
                       ldc,
                       scales,
                       zps,
-                      scale_a_m,
-                      zp_a_m,
-                      k_groups);
+                      scale_a,
+                      zp_a);
             },
             [&](auto i) {
               range_dispatcher<long, 1, PREFERRED_BLOCK_M - 1>::call(
@@ -1396,7 +1320,6 @@ class DequantGemmTPP<
                         ldb,
                         /*transA*/ false,
                         ACC,
-                        quant_a_mode,
                         PREFETCH_K_DIST>::
                         template call<true>(
                             K,
@@ -1407,9 +1330,8 @@ class DequantGemmTPP<
                             ldc,
                             scales,
                             zps,
-                            scale_a_m,
-                            zp_a_m,
-                            k_groups);
+                            scale_a,
+                            zp_a);
                   },
                   [&](auto j) { failing_fallback(); });
             });
@@ -1429,19 +1351,7 @@ class DequantGemmTPP<
       for (long m = 0; m < M; ++m) {
 #pragma omp simd
         for (long n = 0; n < N; ++n) {
-          float* scale_a_m;
-          int32_t* zp_a_m;
-          if constexpr (
-              quant_a_mode == QUANT_A_PER_M ||
-              quant_a_mode == QUANT_A_PER_M_K_BLOCK) {
-            scale_a_m = scale_a + m * k_groups;
-            zp_a_m = zp_a + m * k_groups;
-          } else {
-            scale_a_m = scale_a;
-            zp_a_m = zp_a;
-          }
-          float c = (qC[m][n] - compensation[n] * (*zp_a_m)) * (*scale_a_m) *
-              scales[n];
+          float c = (qC[m][n] - compensation[n] * zp_a) * scale_a * scales[n];
           if constexpr (ACC) {
             C[m * ldc + n] += c;
           } else {
@@ -1472,6 +1382,10 @@ class DequantGemmTPP<
   long ldc;
 };
 
+#define FUSE_GELU 1
+#define FUSE_ADD 2
+#define FUSE_ADD_ADD 3
+
 // If T != TComp
 //   T -> TComp -> GEMM -> TComp -> bias/PostOp -> Tout
 // If T == TComp (we can save intermediate output buffer and schedule M/N/K
@@ -1483,24 +1397,21 @@ template <
     typename TGemmOut,
     typename Tout,
     typename TScale,
-    typename TZero,
-    int quant_a_mode = -1,
-    int quant_w_mode = 0>
+    typename TZero>
 void qlinear_woq_affine_impl(
     const at::Tensor& x,
     const at::Tensor& qw_packed,
     const at::Tensor& scales, // dtype is TComp
     const at::Tensor& zps, // dtype is TComp
     const at::Tensor& b, // dtype is TComp
-    at::Tensor& y,
+    at::Tensor y,
     bool is_int4,
     int k_splits,
     int num_concats,
     int fusion_type,
     const TensorList& others_list,
-    int64_t quant_block_k,
-    at::Tensor t_scale_a = at::empty({1}, at::kFloat),
-    at::Tensor t_zp_a = at::empty({1}, at::kInt)) {
+    float scale_a = 1.0f,
+    int32_t zp_a = 0) {
   auto x_sizes = x.sizes();
   auto w_sizes = qw_packed.sizes();
   auto M = x_sizes[0];
@@ -1510,10 +1421,6 @@ void qlinear_woq_affine_impl(
   auto Kb = w_sizes[2];
   auto N = Nc * Nb;
   auto K = Kc * Kb;
-  assert(quant_block_k % Kb == 0);
-  auto quant_block_multiple = quant_block_k == 0 ? 1 : quant_block_k / Kb;
-  auto quant_k_blocks =
-      quant_block_k == 0 ? 1 : (K + quant_block_k - 1) / quant_block_k;
 
   TLA_ASSERT(Nb % 16 == 0, "Nb must be a multiple of 16");
   TLA_ASSERT(
@@ -1551,18 +1458,14 @@ void qlinear_woq_affine_impl(
   auto ldy = num_concats <= 1 ? N : Nc / num_concats * Nb;
   auto ldc = (no_y_buf || k_splits > 1) ? ldy : Nb;
 
-  auto scales_a_ptr = t_scale_a.data_ptr<float>();
-  auto zps_a_ptr = t_zp_a.data_ptr<int32_t>();
   auto px = GetVLAPtr<T>(x, {Kc, Kb});
   auto pw = GetVLAPtr<uint8_t>(
       (uint8_t*)qw_packed.data_ptr(), {Kc, Kb * (is_int4 ? Nb / 2 : Nb)});
   auto py = GetVLAPtr<Tout>(y, {Nc, Nb}); /*[M, Nc, Nb]*/
   auto py_concat = GetVLAPtr<Tout>(
       y, {M, Nc / num_concats, Nb}); /*[num_concats, M, Nc/num_concats, Nb]*/
-  int scales_kc = quant_w_mode == QUANT_W_PER_CHANNEL ? QUANT_W_PER_K_BLOCK
-                                                      : quant_k_blocks;
-  auto pscales = GetVLAPtr<TScale>(scales, {scales_kc, Nb});
-  auto pzps = GetVLAPtr<TZero>(zps, {scales_kc, Nb});
+  auto pscales = GetVLAPtr<TScale>(scales, {Nb});
+  auto pzps = GetVLAPtr<TZero>(zps, {Nb});
   auto pb = GetVLAPtr<TGemmOut>(b, {Nb});
   auto tin0 = others_list.size() > 0 ? others_list[0] : at::Tensor{};
   auto pin0 = GetVLAPtr<Tout>(tin0, {Nc, Nb}); /*[M, Nc, Nb]*/
@@ -1656,7 +1559,6 @@ void qlinear_woq_affine_impl(
                 /*transA*/ false,
                 /*ACC*/ true,
                 is_int4,
-                quant_a_mode,
                 PREFETCH_K_DIST>(
                 /*M*/ BLOCK_M,
                 /*K*/ Kb,
@@ -1673,7 +1575,6 @@ void qlinear_woq_affine_impl(
                 /*transA*/ false,
                 /*ACC*/ true,
                 is_int4,
-                quant_a_mode,
                 0>(
                 /*M*/ BLOCK_M,
                 /*K*/ Kb,
@@ -1690,7 +1591,6 @@ void qlinear_woq_affine_impl(
                 /*transA*/ false,
                 /*ACC*/ true,
                 is_int4,
-                quant_a_mode,
                 PREFETCH_K_DIST>(
                 /*M*/ BLOCK_M_rem,
                 /*K*/ Kb,
@@ -1707,7 +1607,6 @@ void qlinear_woq_affine_impl(
                 /*transA*/ false,
                 /*ACC*/ true,
                 is_int4,
-                quant_a_mode,
                 0>(
                 /*M*/ BLOCK_M_rem,
                 /*K*/ Kb,
@@ -1749,38 +1648,6 @@ void qlinear_woq_affine_impl(
                     int m = idx[0];
                     int kc = idx[1];
                     int nc = idx[2];
-                    float* scale_a = nullptr;
-                    int32_t* zp_a = nullptr;
-                    int32_t k_groups = -1;
-                    int32_t quant_offset = kc / quant_block_multiple;
-                    if constexpr (std::is_same<TComp, uint8_t>()) {
-                      if constexpr (quant_a_mode == QUANT_A_PER_TENSOR) {
-                        scale_a = scales_a_ptr;
-                        zp_a = zps_a_ptr;
-                      } else if constexpr (
-                          quant_a_mode == QUANT_A_PER_K_BLOCK) {
-                        scale_a = scales_a_ptr + quant_offset;
-                        zp_a = zps_a_ptr + quant_offset;
-                      } else if constexpr (quant_a_mode == QUANT_A_PER_M) {
-                        scale_a = scales_a_ptr + m;
-                        zp_a = zps_a_ptr + m;
-                        k_groups = 1;
-                      } else {
-                        scale_a =
-                            scales_a_ptr + m * quant_k_blocks + quant_offset;
-                        zp_a = zps_a_ptr + m * quant_k_blocks + quant_offset;
-                        k_groups = quant_k_blocks;
-                      }
-                    }
-                    TScale* scale_w = nullptr;
-                    TZero* zp_w = nullptr;
-                    if constexpr (quant_w_mode == QUANT_W_PER_CHANNEL) {
-                      scale_w = pscales[nc][0];
-                      zp_w = pzps[nc][0];
-                    } else {
-                      scale_w = pscales[nc][quant_offset];
-                      zp_w = pzps[nc][quant_offset];
-                    }
                     bool is_rem = (m + BLOCK_M > M);
                     TGemmOut* y_ptr = num_concats <= 1
                         ? (TGemmOut*)py[m][nc]
@@ -1799,24 +1666,22 @@ void qlinear_woq_affine_impl(
                         dequant_gemm_tpp(
                             x_ptr,
                             pw[nc][kc],
-                            scale_w,
-                            zp_w,
+                            pscales[nc],
+                            pzps[nc],
                             y_ptr,
                             true,
                             scale_a,
-                            zp_a,
-                            k_groups);
+                            zp_a);
                       } else {
                         dequant_gemm_no_prefetch_tpp(
                             x_ptr,
                             pw[nc][kc],
-                            scale_w,
-                            zp_w,
+                            pscales[nc],
+                            pzps[nc],
                             y_ptr,
                             true,
                             scale_a,
-                            zp_a,
-                            k_groups);
+                            zp_a);
                         if (fusion_type > 0) {
                           post_ops_fn(m, nc);
                         }
@@ -1834,25 +1699,23 @@ void qlinear_woq_affine_impl(
                         dequant_gemm_rem_tpp(
                             x_ptr,
                             pw[nc][kc],
-                            scale_w,
-                            zp_w,
+                            pscales[nc],
+                            pzps[nc],
                             y_ptr,
                             false,
                             scale_a,
-                            zp_a,
-                            k_groups);
+                            zp_a);
                         dequant_gemm_tpp.config();
                       } else {
                         dequant_gemm_no_prefetch_rem_tpp(
                             x_ptr,
                             pw[nc][kc],
-                            scale_w,
-                            zp_w,
+                            pscales[nc],
+                            pzps[nc],
                             y_ptr,
                             false,
                             scale_a,
-                            zp_a,
-                            k_groups);
+                            zp_a);
                         dequant_gemm_no_prefetch_tpp.config();
                         if (fusion_type > 0) {
                           post_ops_rem_fn(m, nc);
@@ -1927,38 +1790,6 @@ void qlinear_woq_affine_impl(
                     }
                     for (int kc = kc_start; kc < kc_end; kc++) {
                       TComp* x_ptr = (TComp*)px[m][kc];
-                      float* scale_a = nullptr;
-                      int32_t* zp_a = nullptr;
-                      int32_t k_groups = -1;
-                      int32_t quant_offset = kc / quant_block_multiple;
-                      if constexpr (std::is_same<TComp, uint8_t>()) {
-                        if constexpr (quant_a_mode == QUANT_A_PER_TENSOR) {
-                          scale_a = scales_a_ptr;
-                          zp_a = zps_a_ptr;
-                        } else if constexpr (
-                            quant_a_mode == QUANT_A_PER_K_BLOCK) {
-                          scale_a = scales_a_ptr + quant_offset;
-                          zp_a = zps_a_ptr + quant_offset;
-                        } else if constexpr (quant_a_mode == QUANT_A_PER_M) {
-                          scale_a = scales_a_ptr + m;
-                          zp_a = zps_a_ptr + m;
-                          k_groups = 1;
-                        } else {
-                          scale_a =
-                              scales_a_ptr + m * quant_k_blocks + quant_offset;
-                          zp_a = zps_a_ptr + m * quant_k_blocks + quant_offset;
-                          k_groups = quant_k_blocks;
-                        }
-                      }
-                      TScale* scale_w = nullptr;
-                      TZero* zp_w = nullptr;
-                      if constexpr (quant_w_mode == QUANT_W_PER_CHANNEL) {
-                        scale_w = pscales[nc][0];
-                        zp_w = pzps[nc][0];
-                      } else {
-                        scale_w = pscales[nc][quant_offset];
-                        zp_w = pzps[nc][quant_offset];
-                      }
                       if (!is_rem) {
                         alignas(64) TComp x_buf[BLOCK_M][Kb];
                         if (!no_x_buf) {
@@ -1969,24 +1800,22 @@ void qlinear_woq_affine_impl(
                           dequant_gemm_tpp(
                               x_ptr,
                               pw[nc][kc],
-                              scale_w,
-                              zp_w,
+                              pscales[nc],
+                              pzps[nc],
                               y_ptr,
                               true,
                               scale_a,
-                              zp_a,
-                              k_groups);
+                              zp_a);
                         } else {
                           dequant_gemm_no_prefetch_tpp(
                               x_ptr,
                               pw[nc][kc],
-                              scale_w,
-                              zp_w,
+                              pscales[nc],
+                              pzps[nc],
                               y_ptr,
                               true,
                               scale_a,
-                              zp_a,
-                              k_groups);
+                              zp_a);
                         }
                       } else {
                         alignas(64) TComp x_buf[BLOCK_M][Kb];
@@ -1998,25 +1827,23 @@ void qlinear_woq_affine_impl(
                           dequant_gemm_rem_tpp(
                               x_ptr,
                               pw[nc][kc],
-                              scale_w,
-                              zp_w,
+                              pscales[nc],
+                              pzps[nc],
                               y_ptr,
                               false,
                               scale_a,
-                              zp_a,
-                              k_groups);
+                              zp_a);
                           dequant_gemm_tpp.config();
                         } else {
                           dequant_gemm_no_prefetch_rem_tpp(
                               x_ptr,
                               pw[nc][kc],
-                              scale_w,
-                              zp_w,
+                              pscales[nc],
+                              pzps[nc],
                               y_ptr,
                               false,
                               scale_a,
-                              zp_a,
-                              k_groups);
+                              zp_a);
                           dequant_gemm_no_prefetch_tpp.config();
                         }
                       }
@@ -2071,6 +1898,11 @@ void qlinear_woq_affine_impl(
           [](auto tuple) { failing_fallback(); });
 }
 
+#define LOWP_MODE_NONE 0
+#define LOWP_MODE_FP16 1
+#define LOWP_MODE_BF16 2
+#define LOWP_MODE_INT8 3
+
 /**
  * @brief pack the weight in quantized format.
  * @param qw quantized weight with shape [N, K]
@@ -2252,183 +2084,6 @@ void compute_int8_qparams_per_tensor(
   *zp = (int32_t)(-std::nearbyint(min / *scale));
 }
 
-template <typename scalar_t>
-inline scalar_t max_propagate_nan(scalar_t a, scalar_t b) {
-  if (at::_isnan(a)) {
-    return a;
-  }
-  return a > b ? a : b;
-}
-
-template <typename scalar_t>
-inline scalar_t min_propagate_nan(scalar_t a, scalar_t b) {
-  if (at::_isnan(a)) {
-    return a;
-  }
-  return a < b ? a : b;
-}
-
-template <typename T>
-std::pair<at::Tensor, at::Tensor> compute_int8_qparams_per_block(
-    const at::Tensor& t,
-    int quant_block_k,
-    int quant_a_mode) {
-  int M = t.size(0);
-  int K = t.size(1);
-  if (quant_a_mode == QUANT_A_PER_M) {
-    auto grouped_min = std::get<0>(t.min(-1));
-    auto grouped_max = std::get<0>(t.max(-1));
-    auto zeros = at::zeros_like(grouped_min);
-    auto min = at::minimum(grouped_min, zeros);
-    auto max = at::maximum(grouped_max, zeros);
-    auto scales = (max - min) / 255;
-    auto zps = -at::round(min / scales);
-    return std::make_pair<at::Tensor&&, at::Tensor&&>(
-        std::move(scales.to(c10::kFloat)), std::move(zps.to(c10::kInt)));
-  }
-  int k_rem = K % quant_block_k;
-  int block_k = quant_block_k;
-  auto grouped =
-      t.index({at::indexing::Slice(), at::indexing::Slice(0, K - k_rem)})
-          .view({M, K / quant_block_k, quant_block_k});
-  at::Tensor grouped_min, grouped_max;
-  if (quant_a_mode == QUANT_A_PER_K_BLOCK) {
-    grouped_min = std::get<0>(std::get<0>(grouped.min(-1)).min(0));
-    grouped_max = std::get<0>(std::get<0>(grouped.max(-1)).max(0));
-  } else {
-    grouped_min = std::get<0>(grouped.min(-1));
-    grouped_max = std::get<0>(grouped.max(-1));
-  }
-  auto zeros = at::zeros_like(grouped_min);
-  auto min = at::minimum(grouped_min, zeros);
-  auto max = at::maximum(grouped_max, zeros);
-  auto scales = (max - min) / 255.0f;
-  auto zps = -at::round(min / scales);
-  if (k_rem) {
-    auto grouped_rem =
-        t.index({at::indexing::Slice(), at::indexing::Slice(K - k_rem, K)})
-            .view({M, 1, k_rem});
-    at::Tensor grouped_rem_min, grouped_rem_max;
-    if (quant_a_mode == QUANT_A_PER_K_BLOCK) {
-      grouped_rem_min = std::get<0>(std::get<0>(grouped_rem.min(-1)).min(0));
-      grouped_rem_max = std::get<0>(std::get<0>(grouped_rem.max(-1)).max(0));
-    } else {
-      grouped_rem_min = std::get<0>(grouped_rem.min(-1));
-      grouped_rem_max = std::get<0>(grouped_rem.max(-1));
-    }
-    auto min_rem = at::minimum(grouped_rem_min, at::tensor({0}));
-    auto max_rem = at::maximum(grouped_rem_max, at::tensor({0}));
-    auto scales_rem = (max_rem - min_rem) / 255;
-    auto zps_rem = -at::round(min_rem / scales_rem);
-    scales = at::cat({scales, scales_rem}, 1).contiguous();
-    zps = at::cat({zps, zps_rem}, 1).contiguous();
-  }
-  return std::make_pair<at::Tensor&&, at::Tensor&&>(
-      std::move(scales.to(c10::kFloat)), std::move(zps.to(c10::kInt)));
-}
-
-template <>
-std::pair<at::Tensor, at::Tensor> compute_int8_qparams_per_block<bfloat16>(
-    const at::Tensor& t,
-    int quant_block_k,
-    int quant_a_mode) {
-  auto in_ptr = t.data_ptr<at::BFloat16>();
-  int M = t.size(0);
-  int K = t.size(1);
-  int Kc = (K + quant_block_k - 1) / quant_block_k;
-  auto vecsize = at::vec::Vectorized<float>::size();
-  at::Tensor scales, zps;
-  if (quant_a_mode == QUANT_A_PER_K_BLOCK) {
-    scales = at::empty({Kc}, t.options().dtype(at::kFloat));
-    zps = at::empty({Kc}, t.options().dtype(at::kInt));
-  } else if (quant_a_mode == QUANT_A_PER_M) {
-    scales = at::empty({M}, t.options().dtype(at::kFloat));
-    zps = at::empty({M}, t.options().dtype(at::kInt));
-  } else {
-    scales = at::empty({M, Kc}, t.options().dtype(at::kFloat));
-    zps = at::empty({M, Kc}, t.options().dtype(at::kInt));
-  }
-  auto scales_ptr = scales.data_ptr<float>();
-  auto zps_ptr = zps.data_ptr<int32_t>();
-  auto compute_minmax = [vecsize, scales_ptr, zps_ptr](
-                            at::BFloat16* ptr,
-                            int M,
-                            int K,
-                            int scale_offset,
-                            int zp_offset,
-                            int ld) {
-    float min_val = std::numeric_limits<float>::infinity();
-    float max_val = -std::numeric_limits<float>::infinity();
-    auto in_ptr_ = ptr;
-    auto min_vec = at::vec::Vectorized(min_val);
-    auto max_vec = at::vec::Vectorized(max_val);
-    for (int m = 0; m < M; m++) {
-      auto in_ptr0 = in_ptr_;
-      int k;
-      for (k = 0; k < K / vecsize * vecsize; k += vecsize) {
-        auto tmp0 = at::vec::Vectorized<at::BFloat16>::loadu(in_ptr0, vecsize);
-        at::vec::Vectorized<float> res_vec1(0);
-        at::vec::Vectorized<float> res_vec2(0);
-        std::tie(res_vec1, res_vec2) = at::vec::convert_bfloat16_float(tmp0);
-        auto tmp1 = res_vec1;
-        min_vec = at::vec::minimum(min_vec, tmp1);
-        max_vec = at::vec::maximum(tmp1, max_vec);
-        in_ptr0 += vecsize;
-      }
-      for (; k < K; k++) {
-        auto tmp0 = in_ptr0[k];
-        min_val = std::min(min_val, (float)tmp0);
-        max_val = std::max(max_val, (float)tmp0);
-      }
-      in_ptr_ += ld;
-    }
-    min_val = min_propagate_nan(
-        min_val,
-        at::vec::vec_reduce_all<float>(
-            [](at::vec::Vectorized<float>& x, at::vec::Vectorized<float>& y) {
-              return at::vec::minimum(x, y);
-            },
-            min_vec));
-    max_val = max_propagate_nan(
-        max_val,
-        at::vec::vec_reduce_all<float>(
-            [](at::vec::Vectorized<float>& x, at::vec::Vectorized<float>& y) {
-              return at::vec::maximum(x, y);
-            },
-            max_vec));
-    scales_ptr[scale_offset] = (max_val - min_val) / 255.0f;
-    zps_ptr[zp_offset] =
-        (int32_t)(-std::nearbyint(min_val / scales_ptr[scale_offset]));
-  };
-  if (quant_a_mode == QUANT_A_PER_K_BLOCK) {
-#pragma omp parallel for
-    for (int kc = 0; kc < Kc; kc++) {
-      int offset = kc * quant_block_k;
-      int block_k = std::min(quant_block_k, K - offset);
-      compute_minmax(in_ptr + offset, M, block_k, kc, kc, K);
-    }
-  } else if (quant_a_mode == QUANT_A_PER_M) {
-#pragma omp parallel for
-    for (int m = 0; m < M; m++) {
-      int offset = m * K;
-      compute_minmax(in_ptr + offset, 1, K, m, m, K);
-    }
-  } else {
-#pragma omp parallel for collapse(2)
-    for (int m = 0; m < M; m++) {
-      for (int kc = 0; kc < Kc; kc++) {
-        auto in_ptr0 = in_ptr + m * K + kc * quant_block_k;
-        auto scale_offset = m * Kc + kc;
-        auto zp_offset = m * Kc + kc;
-        int block_k = std::min(quant_block_k, K - kc * quant_block_k);
-        compute_minmax(in_ptr0, 1, block_k, scale_offset, zp_offset, K);
-      }
-    }
-  }
-  return std::make_pair<at::Tensor&&, at::Tensor&&>(
-      std::move(scales), std::move(zps));
-}
-
 template <typename T>
 at::Tensor quantize_per_tensor(const at::Tensor& t, float scale, int32_t zp) {
   // TODO(jgong5): optimize me
@@ -2514,9 +2169,9 @@ at::Tensor quantize_per_tensor<bfloat16>(
        i0 += static_cast<long>(1)) {
     auto tmp0 = in_ptr0[static_cast<long>(i0)];
     auto tmp1 = static_cast<float>(tmp0);
-    auto tmp2 = static_cast<float>(scale);
+    auto tmp2 = static_cast<float>(0.05);
     auto tmp3 = tmp1 / tmp2;
-    auto tmp4 = static_cast<float>(zp);
+    auto tmp4 = static_cast<float>(1.0);
     auto tmp5 = tmp3 + tmp4;
     auto tmp6 = std::nearbyint(tmp5);
     auto tmp7 = static_cast<float>(tmp6);
@@ -2544,144 +2199,6 @@ at::Tensor quantize_per_tensor<bfloat16>(
 #endif
 }
 
-template <typename T>
-at::Tensor quantize_per_block(
-    const at::Tensor& t,
-    const at::Tensor& scale,
-    const at::Tensor& zp,
-    int quant_block_k,
-    int quant_a_mode) {
-  int block_k = quant_block_k;
-  auto grouped = t.view({-1, t.size(-1) / block_k, block_k});
-  at::Tensor out;
-  if (quant_a_mode == QUANT_A_PER_K_BLOCK) {
-    out = at::clamp(
-        at::round(grouped / scale.unsqueeze(1)) + zp.unsqueeze(1), 0, 255);
-  } else if (quant_a_mode == QUANT_A_PER_M) {
-    out = at::clamp(
-        at::round(grouped / scale.unsqueeze(1).unsqueeze(2)) +
-            zp.unsqueeze(1).unsqueeze(2),
-        0,
-        255);
-  } else {
-    out = at::clamp(
-        at::round(grouped / scale.unsqueeze(-1)) + zp.unsqueeze(-1), 0, 255);
-  }
-  return out.to(at::kByte);
-}
-
-template <>
-at::Tensor quantize_per_block<bfloat16>(
-    const at::Tensor& t,
-    const at::Tensor& scale,
-    const at::Tensor& zp,
-    int quant_block_k,
-    int quant_a_mode) {
-  // t is shape of [M, K] and contiguous tensor
-  int64_t M = t.size(0);
-  int64_t K = t.size(1);
-  at::Tensor out = at::empty_like(t, at::kByte);
-  int Kc = (K + quant_block_k - 1) / quant_block_k;
-  auto scale_ptr = scale.data_ptr<float>();
-  auto zp_ptr = zp.data_ptr<int32_t>();
-  auto in_ptr = t.data_ptr<at::BFloat16>();
-  auto out_ptr = out.data_ptr<uint8_t>();
-  auto vecsize = at::vec::Vectorized<float>::size();
-  auto quantize_block = [vecsize](
-                            at::BFloat16* in_ptr,
-                            uint8_t* out_ptr,
-                            int block_k,
-                            float scale_,
-                            int zp_) {
-    int k;
-    for (k = 0; k < block_k / vecsize * vecsize; k += vecsize) {
-      auto in_ptr0 = in_ptr + k;
-      auto out_ptr0 = out_ptr + k;
-      auto tmp0 = at::vec::Vectorized<at::BFloat16>::loadu(in_ptr0, vecsize);
-      at::vec::Vectorized<float> res_vec1(0);
-      at::vec::Vectorized<float> res_vec2(0);
-      std::tie(res_vec1, res_vec2) = at::vec::convert_bfloat16_float(tmp0);
-      auto tmp1 = res_vec1;
-      auto tmp2 = at::vec::Vectorized<float>(static_cast<float>(scale_));
-      auto tmp3 = tmp1 / tmp2;
-      auto tmp4 = at::vec::Vectorized<float>(static_cast<float>(zp_));
-      auto tmp5 = tmp3 + tmp4;
-      auto tmp6 = tmp5.round();
-      auto tmp7 = (tmp6);
-      auto tmp8 = at::vec::Vectorized<float>(static_cast<float>(0.0));
-      auto tmp9 = at::vec::maximum(tmp7, tmp8);
-      auto tmp10 = at::vec::Vectorized<float>(static_cast<float>(255.0));
-      auto tmp11 = at::vec::minimum(tmp9, tmp10);
-      auto tmp12 = (tmp11);
-      auto tmp13 = at::vec::convert_float_to_uint8(tmp12);
-      tmp13.store(out_ptr0, vecsize);
-    }
-    for (; k < block_k; k++) {
-      auto tmp0 = in_ptr[k];
-      auto tmp1 = static_cast<float>(tmp0);
-      auto tmp2 = static_cast<float>(scale_);
-      auto tmp3 = tmp1 / tmp2;
-      auto tmp4 = static_cast<float>(zp_);
-      auto tmp5 = tmp3 + tmp4;
-      auto tmp6 = std::nearbyint(tmp5);
-      auto tmp7 = static_cast<float>(tmp6);
-      auto tmp8 = static_cast<float>(0.0);
-      auto tmp9 = 0;
-      if (at::_isnan(tmp7)) {
-        tmp9 = tmp7;
-      }
-      tmp9 = tmp7 > tmp8 ? tmp7 : tmp8;
-      auto tmp10 = static_cast<float>(255.0);
-      auto tmp11 = 0;
-      if (at::_isnan(tmp9)) {
-        tmp11 = tmp9;
-      }
-      tmp11 = tmp9 < tmp10 ? tmp9 : tmp10;
-      auto tmp12 = static_cast<float>(tmp11);
-      auto tmp13 = static_cast<unsigned char>(tmp12);
-      out_ptr[k] = tmp13;
-    }
-  };
-  if (quant_a_mode == QUANT_A_PER_K_BLOCK) {
-#pragma omp parallel for collapse(2)
-    for (int m = 0; m < M; m++) {
-      for (int kc = 0; kc < Kc; kc++) {
-        auto in_ptr0 = in_ptr + m * K + kc * quant_block_k;
-        auto out_ptr0 = out_ptr + m * K + kc * quant_block_k;
-        auto scale_ = scale_ptr[kc];
-        auto zp_ = zp_ptr[kc];
-        int block_k = std::min(quant_block_k, (int)K - kc * quant_block_k);
-        quantize_block(in_ptr0, out_ptr0, block_k, scale_, zp_);
-      }
-    }
-  } else if (quant_a_mode == QUANT_A_PER_M) {
-#pragma omp parallel for collapse(2)
-    for (int m = 0; m < M; m++) {
-      for (int kc = 0; kc < Kc; kc++) {
-        auto in_ptr0 = in_ptr + m * K + kc * quant_block_k;
-        auto out_ptr0 = out_ptr + m * K + kc * quant_block_k;
-        auto scale_ = scale_ptr[m];
-        auto zp_ = zp_ptr[m];
-        int block_k = std::min(quant_block_k, (int)K - kc * quant_block_k);
-        quantize_block(in_ptr0, out_ptr0, block_k, scale_, zp_);
-      }
-    }
-  } else {
-#pragma omp parallel for collapse(2)
-    for (int m = 0; m < M; m++) {
-      for (int kc = 0; kc < Kc; kc++) {
-        auto in_ptr0 = in_ptr + m * K + kc * quant_block_k;
-        auto out_ptr0 = out_ptr + m * K + kc * quant_block_k;
-        auto scale_ = scale_ptr[m * Kc + kc];
-        auto zp_ = zp_ptr[m * Kc + kc];
-        int block_k = std::min(quant_block_k, (int)K - kc * quant_block_k);
-        quantize_block(in_ptr0, out_ptr0, block_k, scale_, zp_);
-      }
-    }
-  }
-  return out;
-}
-
 /**
  * @brief quantized linear with weight in affine quantized format (scale +
  * zero-point) but activation in floating point format.
@@ -2710,10 +2227,7 @@ at::Tensor qlinear_woq_affine(
     int64_t lowp_mode,
     int64_t num_concats,
     int64_t fusion_type,
-    const TensorList& others_list,
-    int64_t quant_a_mode = -1,
-    int64_t quant_w_mode = 0,
-    int64_t quant_block_k = 0) {
+    const TensorList& others_list) {
   const int64_t k_splits = 0;
   // int8_idx is only valid with zp_list when lowp_mode == LOWP_MODE_INT8
   constexpr size_t fp32_idx = 0, fp16_idx = 1, bf16_idx = 2, int8_idx = 3;
@@ -2732,20 +2246,10 @@ at::Tensor qlinear_woq_affine(
     out_sizes.back() = N;
     auto y = at::empty(out_sizes, x.options());
     auto x_reshape = x.reshape({M, K});
-    product_dispatcher<
-        std::tuple<at::ScalarType, long>,
-        std::tuple<
-            enumerate_dispatcher<
-                at::ScalarType,
-                at::kFloat,
-                at::kBFloat16,
-                at::kHalf>,
-            range_dispatcher<long, 0, 1>>>::
+    enumerate_dispatcher<at::ScalarType, at::kFloat, at::kBFloat16, at::kHalf>::
         call(
-            std::make_tuple(x.scalar_type(), quant_w_mode),
-            [&](auto tuple) {
-              auto act_dtype = std::get<0>(tuple);
-              auto quant_w_mode_ = std::get<1>(tuple);
+            x.scalar_type(),
+            [&](auto act_dtype) {
               using act_type =
                   typename c10::impl::ScalarTypeToCPPType<act_dtype>::type;
               auto try_compute_in_half = [&]() {
@@ -2756,9 +2260,7 @@ at::Tensor qlinear_woq_affine(
                     /*TGemmOut*/ half,
                     act_type,
                     half,
-                    half,
-                    UNQUANT_A,
-                    quant_w_mode_>(
+                    half>(
                     x_reshape,
                     qw,
                     scales_list[fp16_idx],
@@ -2769,8 +2271,7 @@ at::Tensor qlinear_woq_affine(
                     k_splits,
                     num_concats,
                     fusion_type,
-                    others_list,
-                    quant_block_k);
+                    others_list);
 #else
                 qlinear_woq_affine_impl<
                     act_type,
@@ -2778,9 +2279,7 @@ at::Tensor qlinear_woq_affine(
                     /*TGemmOut*/ float,
                     act_type,
                     float,
-                    float,
-                    UNQUANT_A,
-                    quant_w_mode_>(
+                    float>(
                     x_reshape,
                     qw,
                     scales_list[fp32_idx],
@@ -2791,8 +2290,7 @@ at::Tensor qlinear_woq_affine(
                     k_splits,
                     num_concats,
                     fusion_type,
-                    others_list,
-                    quant_block_k);
+                    others_list);
 #endif
               };
               if (lowp_mode == LOWP_MODE_NONE) {
@@ -2805,9 +2303,7 @@ at::Tensor qlinear_woq_affine(
                       /*TGemmOut*/ float,
                       bfloat16,
                       bfloat16,
-                      bfloat16,
-                      UNQUANT_A,
-                      quant_w_mode_>(
+                      bfloat16>(
                       x_reshape,
                       qw,
                       scales_list[bf16_idx],
@@ -2818,8 +2314,7 @@ at::Tensor qlinear_woq_affine(
                       k_splits,
                       num_concats,
                       fusion_type,
-                      others_list,
-                      quant_block_k);
+                      others_list);
                 } else {
                   qlinear_woq_affine_impl<
                       float,
@@ -2827,9 +2322,7 @@ at::Tensor qlinear_woq_affine(
                       /*TGemmOut*/ float,
                       float,
                       float,
-                      float,
-                      UNQUANT_A,
-                      quant_w_mode_>(
+                      float>(
                       x_reshape,
                       qw,
                       scales_list[fp32_idx],
@@ -2840,8 +2333,7 @@ at::Tensor qlinear_woq_affine(
                       k_splits,
                       num_concats,
                       fusion_type,
-                      others_list,
-                      quant_block_k);
+                      others_list);
                 }
               } else if (lowp_mode == LOWP_MODE_FP16) {
                 try_compute_in_half();
@@ -2854,9 +2346,7 @@ at::Tensor qlinear_woq_affine(
                       /*TGemmOut*/ float,
                       act_type,
                       bfloat16,
-                      bfloat16,
-                      UNQUANT_A,
-                      quant_w_mode_>(
+                      bfloat16>(
                       x_reshape,
                       qw,
                       scales_list[bf16_idx],
@@ -2867,161 +2357,74 @@ at::Tensor qlinear_woq_affine(
                       k_splits,
                       num_concats,
                       fusion_type,
-                      others_list,
-                      quant_block_k);
+                      others_list);
                 } else {
                   try_compute_in_half();
                 }
               } else {
                 TLA_ASSERT(lowp_mode == LOWP_MODE_INT8, "invalid lowp_mode");
                 TLA_ASSERT(is_int4, "LOWP_MODE_INT8 only support is_int4=true");
-                if (quant_a_mode == QUANT_A_PER_TENSOR) {
-                  float scale_a;
-                  int32_t zp_a;
-                  auto x_reshape_contig = x_reshape.contiguous();
-                  compute_int8_qparams_per_tensor(
-                      x_reshape_contig, &scale_a, &zp_a);
-                  auto x_quantized = quantize_per_tensor<act_type>(
-                      x_reshape_contig, scale_a, zp_a);
-                  auto scale_a_t = at::full({1}, scale_a, at::kFloat);
-                  auto zp_a_t = at::full({1}, zp_a, at::kInt);
-                  qlinear_woq_affine_impl<
-                      uint8_t,
-                      uint8_t,
-                      /*TGemmOut*/ float,
-                      act_type,
-                      float,
-                      int8_t,
-                      QUANT_A_PER_TENSOR,
-                      quant_w_mode_>(
-                      x_quantized,
-                      qw,
-                      scales_list[fp32_idx],
-                      zp_list[int8_idx],
-                      biases[fp32_idx],
-                      y,
-                      is_int4,
-                      k_splits,
-                      num_concats,
-                      fusion_type,
-                      others_list,
-                      quant_block_k,
-                      scale_a_t,
-                      zp_a_t);
-                } else {
-                  if (quant_block_k <= 0) {
-                    quant_block_k = w_sizes[2]; // block_k for weight packing
-                  }
-                  auto x_reshape_contig = x_reshape.contiguous();
-                  auto [scale_a, zp_a] =
-                      compute_int8_qparams_per_block<act_type>(
-                          x_reshape_contig, quant_block_k, quant_a_mode);
-                  auto x_quantized = quantize_per_block<act_type>(
-                      x_reshape_contig,
-                      scale_a,
-                      zp_a,
-                      quant_block_k,
-                      quant_a_mode);
-                  range_dispatcher<
-                      long,
-                      QUANT_A_PER_K_BLOCK,
-                      QUANT_A_PER_M_K_BLOCK>::
-                      call(
-                          quant_a_mode,
-                          [&](auto quant_a_mode_) {
-                            qlinear_woq_affine_impl<
-                                uint8_t,
-                                uint8_t,
-                                /*TGemmOut*/ float,
-                                act_type,
-                                float,
-                                int8_t,
-                                quant_a_mode_,
-                                quant_w_mode_>(
-                                x_quantized,
-                                qw,
-                                scales_list[fp32_idx],
-                                zp_list[int8_idx],
-                                biases[fp32_idx],
-                                y,
-                                is_int4,
-                                k_splits,
-                                num_concats,
-                                fusion_type,
-                                others_list,
-                                quant_block_k,
-                                scale_a,
-                                zp_a);
-                          },
-                          [&](auto quant_a_mode_) { failing_fallback(); });
-                }
+                float scale_a;
+                int32_t zp_a;
+                auto x_reshape_contig = x_reshape.contiguous();
+                compute_int8_qparams_per_tensor(
+                    x_reshape_contig, &scale_a, &zp_a);
+                auto x_quantized = quantize_per_tensor<act_type>(
+                    x_reshape_contig, scale_a, zp_a);
+                qlinear_woq_affine_impl<
+                    uint8_t,
+                    uint8_t,
+                    /*TGemmOut*/ float,
+                    act_type,
+                    float,
+                    int8_t>(
+                    x_quantized,
+                    qw,
+                    scales_list[fp32_idx],
+                    zp_list[int8_idx],
+                    biases[fp32_idx],
+                    y,
+                    is_int4,
+                    k_splits,
+                    num_concats,
+                    fusion_type,
+                    others_list,
+                    scale_a,
+                    zp_a);
               }
             },
-            [](auto tuple) { failing_fallback(); });
+            failing_fallback<at::ScalarType>);
     return y;
   } else {
     TLA_ASSERT(
         qw.dim() == 2,
         "weight must be in 4D blocked format or 2D plain format");
-    auto K = x.size(-1);
-    auto M = x.numel() / K;
-    auto N = qw.size(0);
     auto compute_dtype = x.scalar_type();
     if (lowp_mode == LOWP_MODE_FP16) {
       compute_dtype = at::kHalf;
     } else if (lowp_mode == LOWP_MODE_BF16) {
-      compute_dtype = K >= SMALL_BATCH_THRESHOLD ? at::kBFloat16 : at::kHalf;
+      compute_dtype = at::kBFloat16;
     }
-    at::Tensor scale, zp;
-    scale = scales_list[fp32_idx].unsqueeze(-1);
-    zp = zp_list[fp32_idx].unsqueeze(-1);
     auto w =
         [&]() {
           if (is_int4) {
             using namespace at::indexing;
-            auto w_int8 =
-                at::empty({N, qw.size(1) * 2}, qw.options().dtype(at::kByte));
+            auto w_int8 = at::empty(
+                {qw.size(0), qw.size(1) * 2}, qw.options().dtype(at::kByte));
             w_int8.index({Slice(), Slice(None, None, 2)})
                 .copy_(qw.bitwise_and(0xf));
             w_int8.index({Slice(), Slice(1, None, 2)})
                 .copy_(qw.bitwise_right_shift(4));
-            at::Tensor dqw;
-            if (quant_w_mode == 0) {
-              dqw = (w_int8.to(at::kFloat) - zp) * scale;
-            } else {
-              int64_t num_blocks = scale.size(-2);
-              auto w_int8_view = w_int8.view({N, num_blocks, -1});
-              dqw = (w_int8_view.to(at::kFloat) - zp) * scale;
-              dqw = dqw.view({N, -1});
-            }
-            if (K != qw.size(1) * 2) {
-              TORCH_CHECK(
-                  K < qw.size(1) * 2,
-                  'WOQ Linear kernel: Unexpected weight shape');
-              auto dqw_narrowed = dqw.narrow(1, 0, K);
-              return dqw_narrowed;
-            }
-            return dqw;
+            return (w_int8.to(at::kFloat) - zp_list[fp32_idx]) *
+                scales_list[fp32_idx];
           } else {
-            at::Tensor dqw;
-            if (quant_w_mode == 0) {
-              dqw = (qw.to(at::kFloat) - zp) * scale;
-            } else {
-              int64_t num_blocks = scale.size(-2);
-              auto w_int8_view = qw.view({N, num_blocks, -1});
-              dqw = (w_int8_view.to(at::kFloat) - zp) * scale;
-              dqw = dqw.view({N, -1});
-            }
-            return dqw;
+            return (qw.to(at::kFloat) - zp_list[fp32_idx]) *
+                scales_list[fp32_idx];
           }
         }()
             .to(compute_dtype);
-    auto x_reshape = x.reshape({M, K});
-    auto x_fp = x_reshape.to(compute_dtype);
-    // PyTorch does not support computing in half yet
-    auto y = compute_dtype == at::kHalf
-        ? at::linear(x_fp.to(c10::kFloat), w.to(c10::kFloat))
-        : at::linear(x_fp, w);
+    auto x_fp = x.to(compute_dtype);
+    auto y = at::linear(x_fp, w);
     if (biases[0].defined()) {
       auto b_index = compute_dtype == at::kFloat ? fp32_idx
           : compute_dtype == at::kHalf           ? fp16_idx
@@ -3032,11 +2435,8 @@ at::Tensor qlinear_woq_affine(
       y = at::gelu(y);
     } else if (fusion_type == FUSE_ADD || fusion_type == FUSE_ADD_ADD) {
       for (auto& tin : others_list)
-        y = at::add(y, tin.view(y.sizes()));
+        y = at::add(y, tin);
     }
-    auto out_sizes = x.sizes().vec();
-    out_sizes.back() = N;
-    y = y.view(out_sizes);
     if (num_concats > 1) {
       y = y.view({-1, num_concats, y.size(-1) / num_concats})
               .transpose(0, 1)
@@ -3049,7 +2449,7 @@ at::Tensor qlinear_woq_affine(
 
 #else // defined(CPU_CAPABILITY_AVX512_FP16) && defined(COMPILER_PREREQ_MET)
 
-#define SMALL_BATCH_THRESHOLD 32
+static at::Tensor empty_tensor;
 
 at::Tensor qlinear_woq_affine(
     const at::Tensor& x,
@@ -3061,97 +2461,8 @@ at::Tensor qlinear_woq_affine(
     int64_t lowp_mode,
     int64_t num_concats,
     int64_t fusion_type,
-    const TensorList& others_list,
-    int64_t quant_a_mode = -1,
-    int64_t quant_w_mode = 0,
-    int64_t quant_block_k = 0) {
-  constexpr size_t fp32_idx = 0, fp16_idx = 1, bf16_idx = 2, int8_idx = 3;
-  auto biases = bias_list.empty()
-      ? TensorList({at::Tensor(), at::Tensor(), at::Tensor()})
-      : bias_list;
-  TLA_ASSERT(
-      qw.dim() == 2, "weight must be in 4D blocked format or 2D plain format");
-  auto K = x.size(-1);
-  auto M = x.numel() / K;
-  auto N = qw.size(0);
-  auto compute_dtype = x.scalar_type();
-  if (lowp_mode == LOWP_MODE_FP16) {
-    compute_dtype = at::kHalf;
-  } else if (lowp_mode == LOWP_MODE_BF16) {
-    compute_dtype = K >= SMALL_BATCH_THRESHOLD ? at::kBFloat16 : at::kHalf;
-  }
-  at::Tensor scale, zp;
-  scale = scales_list[fp32_idx].unsqueeze(-1);
-  zp = zp_list[fp32_idx].unsqueeze(-1);
-  auto w =
-      [&]() {
-        if (is_int4) {
-          using namespace at::indexing;
-          auto w_int8 =
-              at::empty({N, qw.size(1) * 2}, qw.options().dtype(at::kByte));
-          w_int8.index({Slice(), Slice(None, None, 2)})
-              .copy_(qw.bitwise_and(0xf));
-          w_int8.index({Slice(), Slice(1, None, 2)})
-              .copy_(qw.bitwise_right_shift(4));
-          at::Tensor dqw;
-          if (quant_w_mode == 0) {
-            dqw = (w_int8.to(at::kFloat) - zp) * scale;
-          } else {
-            int64_t num_blocks = scale.size(-2);
-            auto w_int8_view = w_int8.view({N, num_blocks, -1});
-            dqw = (w_int8_view.to(at::kFloat) - zp) * scale;
-            dqw = dqw.view({N, -1});
-          }
-          if (K != qw.size(1) * 2) {
-            TORCH_CHECK(
-                K < qw.size(1) * 2,
-                'WOQ Linear kernel: Unexpected weight shape');
-            auto dqw_narrowed = dqw.narrow(1, 0, K);
-            return dqw_narrowed;
-          }
-          return dqw;
-        } else {
-          at::Tensor dqw;
-          if (quant_w_mode == 0) {
-            dqw = (qw.to(at::kFloat) - zp) * scale;
-          } else {
-            int64_t num_blocks = scale.size(-2);
-            auto w_int8_view = qw.view({N, num_blocks, -1});
-            dqw = (w_int8_view.to(at::kFloat) - zp) * scale;
-            dqw = dqw.view({N, -1});
-          }
-          return dqw;
-        }
-      }()
-          .to(compute_dtype);
-  auto x_reshape = x.reshape({M, K});
-  auto x_fp = x_reshape.to(compute_dtype);
-  // PyTorch does not support computing in half yet
-  auto y = compute_dtype == at::kHalf
-      ? at::linear(x_fp.to(c10::kFloat), w.to(c10::kFloat))
-      : at::linear(x_fp, w);
-  if (biases[0].defined()) {
-    auto b_index = compute_dtype == at::kFloat ? fp32_idx
-        : compute_dtype == at::kHalf           ? fp16_idx
-                                               : bf16_idx;
-    y = at::add(y, biases[b_index]);
-  }
-  if (fusion_type == FUSE_GELU) {
-    y = at::gelu(y);
-  } else if (fusion_type == FUSE_ADD || fusion_type == FUSE_ADD_ADD) {
-    for (auto& tin : others_list)
-      y = at::add(y, tin.view(y.sizes()));
-  }
-  auto out_sizes = x.sizes().vec();
-  out_sizes.back() = N;
-  y = y.view(out_sizes);
-  if (num_concats > 1) {
-    y = y.view({-1, num_concats, y.size(-1) / num_concats})
-            .transpose(0, 1)
-            .contiguous()
-            .view({-1, y.size(-1)});
-  }
-  return y.to(x.scalar_type());
+    const TensorList& others_list) {
+  return empty_tensor;
 }
 
 at::Tensor qlinear_woq_pack(
@@ -3160,14 +2471,14 @@ at::Tensor qlinear_woq_pack(
     size_t block_n,
     size_t block_k,
     int64_t lowp_mode) {
-  return qw;
+  return empty_tensor;
 }
 
 at::Tensor qlinear_woq_unpack(
     const at::Tensor& qw_packed,
     bool is_int4,
     int64_t lowp_mode) {
-  return qw_packed;
+  return empty_tensor;
 }
 #endif // defined(CPU_CAPABILITY_AVX512_FP16) && defined(COMPILER_PREREQ_MET)
 
diff --git a/csrc/cpu/jit/codegen/onednn/quantization_patterns.h b/csrc/cpu/jit/codegen/onednn/quantization_patterns.h
index 748f990ee..0b3e9fa4b 100644
--- a/csrc/cpu/jit/codegen/onednn/quantization_patterns.h
+++ b/csrc/cpu/jit/codegen/onednn/quantization_patterns.h
@@ -131,7 +131,11 @@ void IpexQuantFusion(std::shared_ptr<torch::jit::Graph>& graph) {
       graph);
   graph_rewrite::replaceMergedEmbCatWithQmergedEmbCat(graph);
   GRAPH_DUMP(
-      "After replaceMergedEmbCatWithQmergedEmbCat. Before preprocessSizeForQLstm",
+      "After replaceMergedEmbCatWithQmergedEmbCat. Before replaceMLPerfMergedEmbCatWithQMLPerfMergedEmbCat",
+      graph);
+  graph_rewrite::replaceMLPerfMergedEmbCatWithQMLPerfMergedEmbCat(graph);
+  GRAPH_DUMP(
+      "After replaceMLPerfMergedEmbCatWithQMLPerfMergedEmbCat. Before preprocessSizeForQLstm",
       graph);
   graph_rewrite::preprocessSizeForQLstm(graph);
   GRAPH_DUMP(
diff --git a/csrc/cpu/jit/cpu/kernels/ContextLinearWoq.h b/csrc/cpu/jit/cpu/kernels/ContextLinearWoq.h
index 9fc9a5188..595a14127 100644
--- a/csrc/cpu/jit/cpu/kernels/ContextLinearWoq.h
+++ b/csrc/cpu/jit/cpu/kernels/ContextLinearWoq.h
@@ -7,7 +7,6 @@ namespace cpu {
 namespace detail {
 struct ContextLinearWoq final {
   at::Tensor at_weight_;
-  std::vector<int64_t> weight_shape_;
   c10::optional<at::Tensor> at_bias_;
   // The list contains three dtype versions of bias, scale and zp
   // i.e., fp32, fp16, bf16
@@ -16,75 +15,37 @@ struct ContextLinearWoq final {
   std::vector<at::Tensor> scales_list_;
   std::vector<at::Tensor> zero_points_list_;
   bool is_int4_;
-  int64_t group_size_;
   int64_t lowp_mode_;
   int64_t num_concats_;
-  int64_t act_quant_mode_;
+  // Original weight shape. Weight may be padded after packing
+  c10::optional<std::vector<int64_t>> orig_wei_shape_;
 
   ContextLinearWoq() = delete;
 
   ContextLinearWoq(
       at::Tensor&& at_weight,
-      std::vector<int64_t>&& weight_shape,
       at::Tensor&& scales_float,
       at::Tensor&& zero_point_float,
       c10::optional<at::Tensor>&& bias,
       bool is_int4 = false,
-      int64_t group_size = -1,
       int64_t lowp_mode = 0,
       int64_t num_concats = 1,
-      int64_t act_quant_mode = 0)
+      c10::optional<std::vector<int64_t>>&& orig_wei_shape = c10::nullopt)
       : at_weight_(std::move(at_weight)),
-        weight_shape_(std::move(weight_shape)),
         at_bias_(std::move(bias)),
         is_int4_(is_int4),
-        group_size_(group_size),
         lowp_mode_(lowp_mode),
         num_concats_(num_concats),
-        act_quant_mode_(act_quant_mode) {
+        orig_wei_shape_(std::move(orig_wei_shape)) {
     // Make three dtype versions of scale, zp and bias
     // There is one more dtype for zp
-    if (group_size > 0) {
-      // Reshape scales/zps for data locality in kernel
-      // [N, #block_k] -> [N / block_n, block_n, #block_k]
-      // -> [#block_n, #block_k, block_n]
-      at::Tensor scales_perm, zp_perm;
-      if (at_weight_.dim() == 4) {
-        // packed weight in 4d (Nc, Kc, block_k, block_n)
-        int64_t block_n = at_weight_.size(-1);
-        if (is_int4) {
-          block_n *= 2;
-        }
-        TORCH_CHECK(scales_float.size(0) % block_n == 0);
-        std::vector<int64_t> reshape_dim = {
-            scales_float.size(0) / block_n, block_n, scales_float.size(1)};
-        scales_perm = scales_float.view(reshape_dim)
-                          .permute({0, 2, 1})
-                          .contiguous()
-                          .to(c10::kFloat);
-        zp_perm =
-            zero_point_float.view(reshape_dim).permute({0, 2, 1}).contiguous();
-      } else {
-        scales_perm = scales_float.to(c10::kFloat);
-        zp_perm = zero_point_float;
-      }
-      auto scales_fp16 = scales_perm.to(c10::kHalf);
-      auto scales_bf16 = scales_perm.to(c10::kBFloat16);
-      scales_list_ = {scales_perm, scales_fp16, scales_bf16};
-      auto zp_fp16 = zp_perm.to(c10::kHalf);
-      auto zp_bf16 = zp_perm.to(c10::kBFloat16);
-      auto zp_int8 = zp_perm.to(c10::kChar);
-      zero_points_list_ = {zp_perm, zp_fp16, zp_bf16, zp_int8};
-    } else {
-      auto scales_fp32 = scales_float.to(c10::kFloat);
-      auto scales_fp16 = scales_float.to(c10::kHalf);
-      auto scales_bf16 = scales_float.to(c10::kBFloat16);
-      scales_list_ = {scales_fp32, scales_fp16, scales_bf16};
-      auto zp_fp16 = zero_point_float.to(c10::kHalf);
-      auto zp_bf16 = zero_point_float.to(c10::kBFloat16);
-      auto zp_int8 = zero_point_float.to(c10::kChar);
-      zero_points_list_ = {zero_point_float, zp_fp16, zp_bf16, zp_int8};
-    }
+    auto scales_fp16 = scales_float.to(c10::kHalf);
+    auto scales_bf16 = scales_float.to(c10::kBFloat16);
+    scales_list_ = {scales_float, scales_fp16, scales_bf16};
+    auto zp_fp16 = zero_point_float.to(c10::kHalf);
+    auto zp_bf16 = zero_point_float.to(c10::kBFloat16);
+    auto zp_int8 = zero_point_float.to(c10::kChar);
+    zero_points_list_ = {zero_point_float, zp_fp16, zp_bf16, zp_int8};
     if (at_bias_.has_value() && at_bias_.value().defined()) {
       auto& orig_bias = at_bias_.value();
       auto bias_fp32 = at_bias_.value().to(c10::kFloat);
diff --git a/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.cpp b/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.cpp
index 668cf29d0..37ed51d86 100644
--- a/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.cpp
+++ b/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.cpp
@@ -11,32 +11,16 @@ namespace woq_linear {
 
 c10::intrusive_ptr<WoqLinearOpContext> createWoqLinearPrePackOpContext(
     at::Tensor&& weight,
-    std::vector<int64_t>&& weight_shape,
-    at::Tensor&& scales,
-    at::Tensor&& zero_points,
     c10::optional<at::Tensor>&& bias,
     c10::optional<int64_t> batch_size,
-    bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t act_quant_mode) {
+    int64_t num_concats) {
   RECORD_FUNCTION(
       "ipex_prepack::createWoqLinearPrePackOpContext",
       c10::ArrayRef<c10::IValue>({}));
 
   return IpexWoqLinearOpContext::create_context(
-      std::move(weight),
-      std::move(weight_shape),
-      std::move(scales),
-      std::move(zero_points),
-      std::move(bias),
-      batch_size,
-      is_int4,
-      group_size,
-      lowp_mode,
-      num_concats,
-      act_quant_mode);
+      std::move(weight), std::move(bias), batch_size, lowp_mode, num_concats);
 }
 
 c10::intrusive_ptr<WoqLinearOpContext> createWoqLinearPrePackOpContextInt4(
@@ -45,126 +29,77 @@ c10::intrusive_ptr<WoqLinearOpContext> createWoqLinearPrePackOpContextInt4(
     at::Tensor&& zero_points,
     c10::optional<at::Tensor>&& bias,
     c10::optional<int64_t> batch_size,
-    int64_t group_size, // group_size along input channel
     int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t act_quant_mode) {
+    int64_t num_concats) {
   RECORD_FUNCTION(
       "ipex_prepack::createWoqLinearPrePackOpContextInt4",
       c10::ArrayRef<c10::IValue>({}));
-  // clang-format off
   // From
-  // Weight dtype = int32 (uint4 * 8) or uint8 (4bit * 2), scale dtype = fp16,
-  // zero points dtype = int32 (int4 * 8)
-  // To
-  // Weight dtype = quint4x2, scale dtype = fp32, zero points dtype = fp32
-  // There might be an extra output channel in weight and scales.
-  // clang-format on
+  // Weight dtype = int32 (uint4 * 8), scale dtype = fp16, zero points dtype =
+  // int32 (int4 * 8) To Weight dtype = quint4x2, scale dtype = fp32, zero
+  // points dtype = fp32 There might be an extra output channel in weight and
+  // scales bool extra_o_channel = false; // scales.numel() >
+  // zero_points.numel() * 8;
   auto scales_fp32 = scales.squeeze().to(c10::ScalarType::Float);
 
-  at::Tensor zp_fp32;
-
+  auto zp_fp32 = zero_points.scalar_type() == c10::kFloat
+      ? zero_points.squeeze()
+      : at::empty_like(scales_fp32);
+  // Convert compressed zero points to float
   if (zero_points.scalar_type() == c10::kInt) {
-    // Two cases: (1) each int32 contains 8 values of zero points
-    // (2) each int32 is a single value of zero point
-    if (zero_points.numel() != scales_fp32.numel()) {
-      // Assume group_size > 0 and zero point data are compressed
-      TORCH_CHECK(scales_fp32.dim() == 2 && zero_points.dim() == 2)
-      TORCH_CHECK(scales_fp32.size(0) == zero_points.size(0))
-      auto num_row = scales_fp32.size(0);
-      auto num_col = scales_fp32.size(1);
-      auto num_col_zp = zero_points.size(1);
-      // Convert compressed zero points to float
-      zp_fp32 = at::empty_like(scales_fp32);
+    if (zero_points.numel() == scales_fp32.numel() / 8 ||
+        zero_points.numel() == scales_fp32.numel() / 8 + 1) {
       float* zp_fp32_ptr = reinterpret_cast<float*>(zp_fp32.data_ptr());
       uint32_t* zp_int32_ptr =
           reinterpret_cast<uint32_t*>(zero_points.data_ptr());
-      for (size_t i = 0; i < num_row; ++i) {
-        for (size_t j = 0; j < num_col; ++j) {
-          zp_fp32_ptr[i * num_col + j] =
-              (float)((zp_int32_ptr[i * num_col_zp + j / 8] >> ((j % 8) * 4)) & 0xf);
+      for (size_t i = 0; i < zero_points.numel(); ++i) {
+        uint32_t zp_uint4x8 = zp_int32_ptr[i];
+        for (size_t j = 0; j < 8; ++j) {
+          zp_fp32_ptr[i * 8 + j] = (float)((zp_uint4x8 >> (j * 4)) & 0xf);
         }
       }
     } else if (zero_points.numel() == scales_fp32.numel()) {
-      // Not compressed
-      zp_fp32 = zero_points.squeeze().to(c10::kFloat);
+      zp_fp32 = zero_points.to(c10::kFloat).squeeze();
     } else {
       TORCH_CHECK(false, "IPEX WOQ INT4: unexpected zero points size");
     }
-  } else {
-    zp_fp32 = zero_points.squeeze().to(c10::kFloat);
   }
   // Support two cases here:
   // 1. fp32/bf16 weight after calibration
-  // 2. int4 weight after calibration, quantized and compressed, as int32/uint8
+  // 2. int4 weight after calibration, quantized and compressed, as int32
   at::Tensor weight_int4;
-  std::vector<int64_t> weight_shape(2);
-  if (weight.scalar_type() == c10::kInt || weight.scalar_type() == c10::kByte) {
+  if (weight.scalar_type() == c10::kInt) {
     // Create empty weight with desired options then copy data
     int64_t N = weight.size(0);
-    int64_t K_compressed = weight.size(1);
-    int64_t K_uint8 =
-        weight.scalar_type() == c10::kInt ? K_compressed * 8 / 2 : K_compressed;
-    weight_shape[0] = N;
-    weight_shape[1] = K_uint8 * 2;
-    std::vector<int64_t> weight_size = {N, K_uint8};
-    // Create an empty uint8 weight to hold int4 data
-    weight_int4 = at::empty(weight_size, device(c10::kCPU).dtype(c10::kByte));
-    auto sizeof_dtype = weight.scalar_type() == c10::kInt
-        ? sizeof(uint32_t)
-        : sizeof(unsigned char);
+    int64_t K_int32 = weight.size(1);
+    int64_t K = K_int32 * 8; // int32 = int4 * 8
+    std::vector<int64_t> weight_size = {N, K};
+    // Create an empty quint4x2 weight with scales and zero points
+    weight_int4 = at::_empty_per_channel_affine_quantized(
+        weight_size,
+        scales_fp32,
+        zp_fp32,
+        0,
+        device(c10::kCPU).dtype(c10::kQUInt4x2));
     std::memcpy(
         weight_int4.data_ptr(),
         weight.data_ptr(),
-        weight.numel() * sizeof_dtype);
-  } else if (
-      weight.scalar_type() == c10::kBFloat16 ||
-      weight.scalar_type() == c10::kFloat ||
-      weight.scalar_type() == c10::kHalf) {
-    weight_shape[0] = weight.size(0);
-    weight_shape[1] = weight.size(1);
+        weight.numel() * sizeof(uint32_t));
+  } else if (weight.scalar_type() == c10::kBFloat16) {
+    // Load bf16 weight and quantize
     auto weight_fp32 = weight.to(c10::kFloat);
-    at::Tensor weight_int4_as_uint8;
-    if (group_size > 0) {
-      auto weight_view =
-          weight_fp32.view({-1, weight.size(1) / group_size, group_size});
-      auto scale_view = scales_fp32.unsqueeze(2);
-      auto zp_view = zp_fp32.unsqueeze(2);
-      weight_int4_as_uint8 =
-          at::round(weight_view / scale_view + zp_view).to(c10::kByte);
-    } else {
-      auto scale_view = scales_fp32.unsqueeze(1);
-      auto zp_view = zp_fp32.unsqueeze(1);
-      weight_int4_as_uint8 =
-          at::round(weight / scale_view + zp_view).to(c10::kByte);
-    }
-    weight_int4_as_uint8 = weight_int4_as_uint8.view(weight_shape);
-    using at::indexing::None;
-    using at::indexing::Slice;
-    at::Tensor even_columns =
-        weight_int4_as_uint8.index({Slice(), Slice(1, None, 2)});
-    even_columns = even_columns.bitwise_left_shift(4);
-    at::Tensor odd_columns =
-        weight_int4_as_uint8.index({Slice(), Slice(None, None, 2)});
-    weight_int4 = even_columns.bitwise_or(odd_columns);
-  } else {
-    TORCH_CHECK(
-        false,
-        "IPEX WOQ INT4: unexpected weight data type: ",
-        weight.scalar_type());
+    weight_int4 = at::quantize_per_channel(
+        weight_fp32, scales_fp32, zp_fp32, 0, c10::kQUInt4x2);
+  } else if (weight.scalar_type() == c10::kFloat) {
+    weight_int4 = at::quantize_per_channel(
+        weight, scales_fp32, zp_fp32, 0, c10::kQUInt4x2);
   }
   return IpexWoqLinearOpContext::create_context(
       std::move(weight_int4),
-      std::move(weight_shape),
-      std::move(scales_fp32),
-      std::move(zp_fp32),
       std::move(bias),
       batch_size,
-      /*is_int4*/ true,
-      group_size,
       lowp_mode,
-      num_concats,
-      act_quant_mode);
+      num_concats);
 }
 
 at::Tensor woq_linear_run(
@@ -178,83 +113,73 @@ at::Tensor woq_linear_run(
 
 ContextLinearWoq create(
     at::Tensor& weight,
-    std::vector<int64_t>& weight_shape,
     at::Tensor& scales,
     at::Tensor& zero_points,
     const c10::optional<at::Tensor>& bias,
     const c10::optional<int64_t> batch_size,
-    bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t act_quant_mode) {
-  auto packed_weight = woq_linear_pack_weight(
-      weight, weight_shape, is_int4, group_size, lowp_mode);
+    int64_t num_concats) {
+  auto packed_weight =
+      woq_linear_pack_weight(weight, scales, zero_points, lowp_mode);
+  bool is_int4 = weight.scalar_type() == c10::kQUInt4x2;
   auto packed_shape = packed_weight.sizes();
   int64_t N = weight.size(0);
   int64_t K = weight.size(1);
-  // If OC is not a multiple of BLOCK_N, it may be padded.
-  bool oc_is_padded = (packed_shape.size() == 4 && is_int4 &&
-                       packed_shape[0] * packed_shape[3] * 2 != N) ||
+  bool weight_is_padded = (packed_shape.size() == 4 && is_int4 &&
+                           packed_shape[0] * packed_shape[3] * 2 != N) ||
       (packed_shape.size() == 4 && !is_int4 &&
        packed_shape[0] * packed_shape[3] != N) ||
       (packed_shape.size() == 2 && packed_shape[0] != N);
   auto zero_points_float = zero_points.to(c10::kFloat);
-  if (oc_is_padded) {
+  if (weight_is_padded) {
     int64_t padded_N = packed_shape.size() == 4
         ? (is_int4 ? packed_shape[0] * packed_shape[3] * 2
                    : packed_shape[0] * packed_shape[3])
         : packed_shape[0];
-    std::vector<int64_t> pad_vec = scales.dim() == 1
-        ? std::vector<int64_t>({0, padded_N - N})
-        : std::vector<int64_t>({0, 0, 0, padded_N - N});
-    auto scales_padded = at::pad(scales, pad_vec, "constant", 1.f);
+    auto scales_padded = at::pad(scales, {0, padded_N - N}, "constant", 1.f);
     auto zero_points_padded =
-        at::pad(zero_points_float, pad_vec, "constant", 0.f);
+        at::pad(zero_points_float, {0, padded_N - N}, "constant", 0.f);
     if (bias.has_value()) {
       auto bias_padded =
           at::pad(bias.value(), {0, padded_N - N}, "constant", 0.f);
       return ContextLinearWoq(
           std::move(packed_weight),
-          std::move(weight_shape),
           std::move(scales_padded),
           std::move(zero_points_padded),
           c10::make_optional(bias_padded),
           is_int4,
-          group_size,
           lowp_mode,
           num_concats,
-          act_quant_mode);
+          c10::make_optional(weight.sizes().vec()));
     } else {
       return ContextLinearWoq(
           std::move(packed_weight),
-          std::move(weight_shape),
           std::move(scales_padded),
           std::move(zero_points_padded),
           c10::nullopt,
           is_int4,
-          group_size,
           lowp_mode,
           num_concats,
-          act_quant_mode);
+          c10::make_optional(weight.sizes().vec()));
     }
   }
   return ContextLinearWoq(
       std::move(packed_weight),
-      std::move(weight_shape),
       std::move(scales),
       std::move(zero_points_float),
       bias.has_value() ? c10::make_optional(*bias) : c10::nullopt,
       is_int4,
-      group_size,
       lowp_mode,
       num_concats,
-      act_quant_mode);
+      weight_is_padded ? c10::make_optional(weight.sizes().vec())
+                       : c10::nullopt);
 }
 
 at::Tensor run(ContextLinearWoq& context, const at::Tensor& input) {
   // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
-  auto w_k = context.weight_shape_[1];
+  auto w_k = context.at_weight_.dim() == 2
+      ? context.at_weight_.size(1)
+      : context.at_weight_.size(1) * context.at_weight_.size(2);
   TORCH_CHECK(
       input.size(input.dim() - 1) == w_k,
       "WOQ linear: input and weight shapes do not match, got k = ",
@@ -263,22 +188,30 @@ at::Tensor run(ContextLinearWoq& context, const at::Tensor& input) {
       w_k,
       " respectively.");
   auto input_ = input.contiguous();
-  auto res = woq_linear_kernel(
+  // if weight is not padded, context.orig_wei_shape_ has no value
+  if (context.orig_wei_shape_.has_value()) {
+    auto res = woq_linear_kernel(
+        input_,
+        context.at_weight_,
+        context.scales_list_,
+        context.zero_points_list_,
+        context.bias_list_,
+        context.is_int4_,
+        context.lowp_mode_,
+        context.num_concats_);
+    // weight shape is [N by K], output shape is [M by N] or [batch by M by N]
+    int64_t N = context.orig_wei_shape_.value()[0];
+    return at::slice(res, /*dim*/ -1, /*start*/ 0, /*end*/ N, /*step*/ 1);
+  }
+  return woq_linear_kernel(
       input_,
       context.at_weight_,
       context.scales_list_,
       context.zero_points_list_,
       context.bias_list_,
       context.is_int4_,
-      context.group_size_,
       context.lowp_mode_,
-      context.num_concats_,
-      context.act_quant_mode_);
-  if (res.size(-1) != context.weight_shape_[0]) {
-    int64_t N = context.weight_shape_[0];
-    return at::narrow(res, /*dim*/ -1, /*start*/ 0, /*end*/ N);
-  }
-  return res;
+      context.num_concats_);
 }
 
 // Called by IpexWoqLinearOpContext::run_eltwise
@@ -289,7 +222,9 @@ at::Tensor run_eltwise(
     const torch::List<c10::optional<at::Scalar>>& scalars,
     const c10::optional<c10::string_view>& algorithm) {
   // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
-  auto w_k = context.weight_shape_[1];
+  auto w_k = context.at_weight_.dim() == 2
+      ? context.at_weight_.size(1)
+      : context.at_weight_.size(1) * context.at_weight_.size(2);
   TORCH_CHECK(
       input.size(input.dim() - 1) == w_k,
       "WOQ linear: input and weight shapes do not match, got k = ",
@@ -308,10 +243,89 @@ at::Tensor run_eltwise(
       scalars,
       algorithm,
       context.is_int4_,
-      context.group_size_,
+      context.lowp_mode_,
+      context.num_concats_);
+}
+
+// Registered as JIT op
+at::Tensor woq_linear_eltwise_run(
+    const at::Tensor& input,
+    const at::Tensor& op_context,
+    const c10::string_view& post_op,
+    const torch::List<c10::optional<at::Scalar>>& scalars,
+    const c10::optional<c10::string_view>& algorithm) {
+  static std::map<c10::string_view, std::string> postop_to_record_name_map = {
+      {"relu", "torch_ipex::woq_linear_relu_run"},
+      {"gelu", "torch_ipex::woq_linear_gelu_run"},
+  };
+  RECORD_FUNCTION(
+      postop_to_record_name_map[post_op], c10::ArrayRef<c10::IValue>({}));
+  return reinterpret_cast<IpexWoqLinearOpContext*>(
+             op_context.data_ptr<int64_t>()[0])
+      ->run_eltwise(input, post_op, scalars, algorithm);
+}
+
+// Called by IpexWoqLinearOpContext::run_add
+at::Tensor run_add(
+    ContextLinearWoq& context,
+    const at::Tensor& input,
+    at::Tensor& accumu,
+    const c10::optional<at::Scalar>& alpha) {
+  // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
+  auto w_k = context.at_weight_.dim() == 2
+      ? context.at_weight_.size(1)
+      : context.at_weight_.size(1) * context.at_weight_.size(2);
+  TORCH_CHECK(
+      input.size(input.dim() - 1) == w_k,
+      "WOQ linear: input and weight shapes do not match, got k = ",
+      input.size(input.dim() - 1),
+      " and ",
+      w_k,
+      " respectively.");
+  auto input_ = input.contiguous();
+  return woq_linear_add_kernel(
+      input_,
+      context.at_weight_,
+      context.scales_list_,
+      context.zero_points_list_,
+      context.bias_list_,
+      context.is_int4_,
       context.lowp_mode_,
       context.num_concats_,
-      context.act_quant_mode_);
+      accumu,
+      alpha);
+}
+
+// Called by IpexWoqLinearOpContext::run_add_relu
+at::Tensor run_add_relu(
+    ContextLinearWoq& context,
+    const at::Tensor& input,
+    at::Tensor& accumu,
+    const c10::optional<at::Scalar>& alpha) {
+  // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
+  auto w_k = context.at_weight_.dim() == 2
+      ? context.at_weight_.size(1)
+      : context.at_weight_.size(1) * context.at_weight_.size(2);
+  TORCH_CHECK(
+      input.size(input.dim() - 1) == w_k,
+      "WOQ linear: input and weight shapes do not match, got k = ",
+      input.size(input.dim() - 1),
+      " and ",
+      w_k,
+      " respectively.");
+  auto input_ = input.contiguous();
+  auto output = woq_linear_kernel(
+      input_,
+      context.at_weight_,
+      context.scales_list_,
+      context.zero_points_list_,
+      context.bias_list_,
+      context.is_int4_,
+      context.lowp_mode_,
+      context.num_concats_);
+  at::add_out(accumu, output, accumu, alpha.value());
+  at::relu_(accumu);
+  return accumu;
 }
 
 // Called by IpexWoqLinearOpContext::run_add
@@ -320,7 +334,9 @@ at::Tensor run_add(
     const at::Tensor& input,
     const std::vector<at::Tensor>& others) {
   // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
-  auto w_k = context.weight_shape_[1];
+  auto w_k = context.at_weight_.dim() == 2
+      ? context.at_weight_.size(1)
+      : context.at_weight_.size(1) * context.at_weight_.size(2);
   TORCH_CHECK(
       input.size(input.dim() - 1) == w_k,
       "WOQ linear: input and weight shapes do not match, got k = ",
@@ -336,11 +352,9 @@ at::Tensor run_add(
       context.zero_points_list_,
       context.bias_list_,
       context.is_int4_,
-      context.group_size_,
       context.lowp_mode_,
       context.num_concats_,
-      others,
-      context.act_quant_mode_);
+      others);
 }
 
 // Called by IpexWoqLinearOpContext::run_add_add
@@ -349,7 +363,9 @@ at::Tensor run_add_add(
     const at::Tensor& input,
     const std::vector<at::Tensor>& others) {
   // TPP kernel packs weight to 4d (Nc, Kc, block_k, block_n)
-  auto w_k = context.weight_shape_[1];
+  auto w_k = context.at_weight_.dim() == 2
+      ? context.at_weight_.size(1)
+      : context.at_weight_.size(1) * context.at_weight_.size(2);
   TORCH_CHECK(
       input.size(input.dim() - 1) == w_k,
       "WOQ linear: input and weight shapes do not match, got k = ",
@@ -365,11 +381,35 @@ at::Tensor run_add_add(
       context.zero_points_list_,
       context.bias_list_,
       context.is_int4_,
-      context.group_size_,
       context.lowp_mode_,
       context.num_concats_,
-      others,
-      context.act_quant_mode_);
+      others);
+}
+
+// Registered as JIT op
+at::Tensor woq_linear_add_run(
+    const at::Tensor& input,
+    at::Tensor& accumu,
+    const c10::optional<at::Scalar>& alpha,
+    const at::Tensor& op_context) {
+  RECORD_FUNCTION(
+      "torch_ipex::woq_linear_add_run", c10::ArrayRef<c10::IValue>({}));
+  return reinterpret_cast<IpexWoqLinearOpContext*>(
+             op_context.data_ptr<int64_t>()[0])
+      ->run_add(input, accumu, alpha);
+}
+
+// Registered as JIT op
+at::Tensor woq_linear_add_relu_run(
+    const at::Tensor& input,
+    at::Tensor& accumu,
+    const c10::optional<at::Scalar>& alpha,
+    const at::Tensor& op_context) {
+  RECORD_FUNCTION(
+      "torch_ipex::woq_linear_add_relu_run", c10::ArrayRef<c10::IValue>({}));
+  return reinterpret_cast<IpexWoqLinearOpContext*>(
+             op_context.data_ptr<int64_t>()[0])
+      ->run_add_relu(input, accumu, alpha);
 }
 
 at::Tensor pack(ContextLinearWoq& context, const at::Tensor& tensor) {
@@ -388,16 +428,22 @@ at::Tensor unpack(ContextLinearWoq& context, const at::Tensor& tensor) {
     auto zero_points = context.zero_points_list_[0];
     if (context.is_int4_) {
       auto unpacked_shape = unpacked_weight.sizes().vec(); // = N * K/2
-      auto shape = context.weight_shape_;
-      shape.back() /= 2;
-      at::Tensor qweight =
-          at::empty(shape, device(c10::kCPU).dtype(c10::kByte));
+      auto shape = context.orig_wei_shape_.has_value()
+          ? context.orig_wei_shape_.value()
+          : std::vector<int64_t>({unpacked_shape[0], unpacked_shape[1] * 2});
+      at::Tensor qweight = at::_empty_per_channel_affine_quantized(
+          shape,
+          scales,
+          zero_points,
+          0,
+          device(c10::kCPU).dtype(c10::kQUInt4x2));
       assert(qweight.numel() % 2 == 0);
       std::memcpy(
-          qweight.data_ptr(), unpacked_weight.data_ptr(), qweight.numel());
+          qweight.data_ptr(), unpacked_weight.data_ptr(), qweight.numel() / 2);
       return qweight;
     } else { // int8
-      return unpacked_weight;
+      return at::_make_per_channel_quantized_tensor(
+          unpacked_weight.int_repr(), scales, zero_points.to(c10::kInt), 0);
     }
   }
   return unpacked_weight;
diff --git a/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.h b/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.h
index 6f573ec9f..a54442aef 100644
--- a/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.h
+++ b/csrc/cpu/jit/cpu/kernels/LinearWoqPacked.h
@@ -12,16 +12,10 @@ namespace woq_linear {
 // WOQ = weight-only quantization
 c10::intrusive_ptr<WoqLinearOpContext> createWoqLinearPrePackOpContext(
     at::Tensor&& weight,
-    std::vector<int64_t>&& weight_shape,
-    at::Tensor&& scales,
-    at::Tensor&& zero_points,
     c10::optional<at::Tensor>&& bias,
     c10::optional<int64_t> batch_size,
-    bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t act_quant_mode);
+    int64_t num_concats);
 
 c10::intrusive_ptr<WoqLinearOpContext> createWoqLinearPrePackOpContextInt4(
     at::Tensor&& weight,
@@ -29,10 +23,8 @@ c10::intrusive_ptr<WoqLinearOpContext> createWoqLinearPrePackOpContextInt4(
     at::Tensor&& zero_points,
     c10::optional<at::Tensor>&& bias,
     c10::optional<int64_t> batch_size,
-    int64_t group_size,
     int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t act_quant_mode);
+    int64_t num_concats);
 
 at::Tensor woq_linear_run(
     const at::Tensor& input,
@@ -40,16 +32,12 @@ at::Tensor woq_linear_run(
 
 ContextLinearWoq create(
     at::Tensor& weight,
-    std::vector<int64_t>& weight_shape,
     at::Tensor& scales,
     at::Tensor& zero_points,
     const c10::optional<at::Tensor>& bias,
     const c10::optional<int64_t> batch_size,
-    bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t act_quant_mode);
+    int64_t num_concats);
 
 at::Tensor run(ContextLinearWoq& context, const at::Tensor& input);
 
@@ -60,6 +48,25 @@ at::Tensor run_eltwise(
     const torch::List<c10::optional<at::Scalar>>& scalars,
     const c10::optional<c10::string_view>& algorithm);
 
+at::Tensor woq_linear_eltwise_run(
+    const at::Tensor& input,
+    const at::Tensor& op_context,
+    const c10::string_view& post_op,
+    const torch::List<c10::optional<at::Scalar>>& scalars,
+    const c10::optional<c10::string_view>& algorithm);
+
+at::Tensor run_add(
+    ContextLinearWoq& context,
+    const at::Tensor& input,
+    at::Tensor& accumu,
+    const c10::optional<at::Scalar>& alpha);
+
+at::Tensor run_add_relu(
+    ContextLinearWoq& context,
+    const at::Tensor& input,
+    at::Tensor& accumu,
+    const c10::optional<at::Scalar>& alpha);
+
 at::Tensor run_add(
     ContextLinearWoq& context,
     const at::Tensor& input,
diff --git a/csrc/cpu/jit/cpu/kernels/OpContext.cpp b/csrc/cpu/jit/cpu/kernels/OpContext.cpp
index 3014bd9b9..22bcf3392 100644
--- a/csrc/cpu/jit/cpu/kernels/OpContext.cpp
+++ b/csrc/cpu/jit/cpu/kernels/OpContext.cpp
@@ -362,30 +362,125 @@ void IpexConvTransposeOpContext::load_from_ctx(
 // For weight-only quantization
 c10::intrusive_ptr<WoqLinearOpContext> IpexWoqLinearOpContext::create_context(
     at::Tensor&& weight,
-    std::vector<int64_t>&& weight_shape,
-    at::Tensor&& scales_fp32,
-    at::Tensor&& zp_fp32,
     c10::optional<at::Tensor>&& bias,
     c10::optional<int64_t> batch_size,
-    bool is_int4,
-    int64_t group_size,
     int64_t lowp_mode,
-    int64_t num_concats,
-    int64_t act_quant_mode) {
-  auto op_context = torch_ipex::cpu::detail::woq_linear::create(
-      weight,
-      weight_shape,
-      scales_fp32,
-      zp_fp32,
-      bias,
-      batch_size,
-      is_int4,
-      group_size,
-      lowp_mode,
-      num_concats,
-      act_quant_mode);
-  return c10::make_intrusive<IpexWoqLinearOpContext>(
-      batch_size, std::move(op_context));
+    int64_t num_concats) {
+  auto N = weight.size(0);
+  const auto qtype = weight.qscheme();
+  if (weight.scalar_type() == c10::ScalarType::QInt8) {
+    // extract scales from weight
+    std::vector<float> weight_scales_float(1, 0.0);
+    if (qtype == c10::kPerTensorAffine) {
+      weight_scales_float[0] = weight.q_scale();
+    } else if (qtype == c10::kPerChannelAffine) {
+      weight_scales_float.resize(N, 0.0);
+      for (const auto i : c10::irange(N)) {
+        weight_scales_float[i] = weight.q_per_channel_scales()[i].item<float>();
+      }
+    }
+
+    at::Tensor scales = at::empty(
+        {static_cast<long>(weight_scales_float.size())},
+        at::device(c10::kCPU).dtype(c10::kFloat));
+    std::copy(
+        weight_scales_float.begin(),
+        weight_scales_float.end(),
+        scales.data_ptr<float>());
+
+    // extract zero_points from weight
+    std::vector<int32_t> weight_zero_points_int32(1, 0);
+    if (qtype == c10::kPerTensorAffine) {
+      weight_zero_points_int32[0] = weight.q_zero_point();
+    } else if (qtype == c10::kPerChannelAffine) {
+      weight_zero_points_int32.resize(N, 0);
+      for (const auto i : c10::irange(N)) {
+        weight_zero_points_int32[i] =
+            weight.q_per_channel_zero_points()[i].item<int32_t>();
+      }
+    }
+    at::Tensor zero_points_int32 = at::empty(
+        {static_cast<long>(weight_zero_points_int32.size())},
+        at::device(c10::kCPU).dtype(c10::kInt));
+    std::copy(
+        weight_zero_points_int32.begin(),
+        weight_zero_points_int32.end(),
+        zero_points_int32.data_ptr<int32_t>());
+
+    // convert zero_points from int32_t to float
+    std::vector<float> weight_zero_points_float(1, 0);
+    if (qtype == c10::kPerTensorAffine) {
+      weight_zero_points_float[0] = (float)weight.q_zero_point();
+    } else if (qtype == c10::kPerChannelAffine) {
+      weight_zero_points_float.resize(N, 0);
+      for (const auto i : c10::irange(N)) {
+        weight_zero_points_float[i] =
+            (float)weight.q_per_channel_zero_points()[i].item<int32_t>();
+      }
+    }
+    at::Tensor zero_points_float = at::empty(
+        {static_cast<long>(weight_zero_points_float.size())},
+        at::device(c10::kCPU).dtype(c10::kFloat));
+    std::copy(
+        weight_zero_points_float.begin(),
+        weight_zero_points_float.end(),
+        zero_points_float.data_ptr<float>());
+
+    auto op_context = torch_ipex::cpu::detail::woq_linear::create(
+        weight,
+        scales,
+        zero_points_int32,
+        bias,
+        batch_size,
+        lowp_mode,
+        num_concats);
+    return c10::make_intrusive<IpexWoqLinearOpContext>(
+        batch_size, std::move(op_context));
+  } else {
+    // extract scales from weight
+    std::vector<float> weight_scales_float(1, 0.0);
+    if (qtype == c10::kPerChannelAffineFloatQParams) {
+      weight_scales_float.resize(N, 0.0);
+      for (const auto i : c10::irange(N)) {
+        weight_scales_float[i] = weight.q_per_channel_scales()[i].item<float>();
+      }
+    }
+
+    at::Tensor scales = at::empty(
+        {static_cast<long>(weight_scales_float.size())},
+        at::device(c10::kCPU).dtype(c10::kFloat));
+    std::copy(
+        weight_scales_float.begin(),
+        weight_scales_float.end(),
+        scales.data_ptr<float>());
+
+    // extract zero_points from weight
+    std::vector<float> weight_zero_points_float(1, 0);
+    if (qtype == c10::kPerChannelAffineFloatQParams) {
+      weight_zero_points_float.resize(N, 0);
+      for (const auto i : c10::irange(N)) {
+        weight_zero_points_float[i] =
+            weight.q_per_channel_zero_points()[i].item<float>();
+      }
+    }
+    at::Tensor zero_points_float = at::empty(
+        {static_cast<long>(weight_zero_points_float.size())},
+        at::device(c10::kCPU).dtype(c10::kFloat));
+    std::copy(
+        weight_zero_points_float.begin(),
+        weight_zero_points_float.end(),
+        zero_points_float.data_ptr<float>());
+    auto op_context = torch_ipex::cpu::detail::woq_linear::create(
+        weight,
+        scales,
+        zero_points_float,
+        bias,
+        batch_size,
+        lowp_mode,
+        num_concats);
+    return c10::make_intrusive<IpexWoqLinearOpContext>(
+        batch_size, std::move(op_context));
+  }
 }
 
 at::Tensor IpexWoqLinearOpContext::get_data_handle() {
@@ -407,6 +502,22 @@ at::Tensor IpexWoqLinearOpContext::run_eltwise(
       op_context_, input, post_op, scalars, algorithm);
 }
 
+at::Tensor IpexWoqLinearOpContext::run_add(
+    const at::Tensor& input,
+    at::Tensor& accumu,
+    const c10::optional<at::Scalar>& alpha) {
+  return torch_ipex::cpu::detail::woq_linear::run_add(
+      op_context_, input, accumu, alpha);
+}
+
+at::Tensor IpexWoqLinearOpContext::run_add_relu(
+    const at::Tensor& input,
+    at::Tensor& accumu,
+    const c10::optional<at::Scalar>& alpha) {
+  return torch_ipex::cpu::detail::woq_linear::run_add_relu(
+      op_context_, input, accumu, alpha);
+}
+
 at::Tensor IpexWoqLinearOpContext::run_add(
     const at::Tensor& input,
     const std::vector<at::Tensor>& others) {
@@ -433,46 +544,6 @@ c10::optional<at::Tensor> IpexWoqLinearOpContext::get_at_bias() {
   return op_context_.at_bias_;
 }
 
-at::Tensor IpexWoqLinearOpContext::get_scales() {
-  if (op_context_.group_size_ > 0 && op_context_.at_weight_.dim() == 4) {
-    // [#block_n, #block_k, n_block_size] -> [#block_n, n_block_size, #block_k]
-    // -> [N, #block_k]
-    auto scales = op_context_.scales_list_[0].permute({0, 2, 1}).contiguous();
-    scales = scales.view({-1, scales.size(-1)});
-    if (scales.size(0) > op_context_.weight_shape_[0]) {
-      return scales.narrow(0, 0, op_context_.weight_shape_[0]);
-    }
-    return scales;
-  }
-  if (op_context_.scales_list_[0].size(0) > op_context_.weight_shape_[0]) {
-    return op_context_.scales_list_[0].narrow(
-        0, 0, op_context_.weight_shape_[0]);
-  }
-  return op_context_.scales_list_[0];
-}
-
-at::Tensor IpexWoqLinearOpContext::get_zero_points() {
-  if (op_context_.group_size_ > 0 && op_context_.at_weight_.dim() == 4) {
-    // [#block_n, #block_k, n_block_size] -> [#block_n, n_block_size, #block_k]
-    // -> [N, #block_k]
-    auto zp = op_context_.zero_points_list_[0].permute({0, 2, 1}).contiguous();
-    zp = zp.view({-1, zp.size(-1)});
-    if (zp.size(0) > op_context_.weight_shape_[0]) {
-      return zp.narrow(0, 0, op_context_.weight_shape_[0]);
-    }
-    return zp;
-  }
-  if (op_context_.zero_points_list_[0].size(0) > op_context_.weight_shape_[0]) {
-    return op_context_.zero_points_list_[0].narrow(
-        0, 0, op_context_.weight_shape_[0]);
-  }
-  return op_context_.zero_points_list_[0];
-}
-
-std::vector<int64_t> IpexWoqLinearOpContext::get_weight_shape() {
-  return op_context_.weight_shape_;
-}
-
 detail::ContextLinearWoq& IpexWoqLinearOpContext::get_context() {
   return op_context_;
 }
diff --git a/csrc/cpu/jit/cpu/kernels/OpContext.h b/csrc/cpu/jit/cpu/kernels/OpContext.h
index 6c15601ad..e43eb5cf5 100644
--- a/csrc/cpu/jit/cpu/kernels/OpContext.h
+++ b/csrc/cpu/jit/cpu/kernels/OpContext.h
@@ -361,17 +361,11 @@ class IpexLinearMKLOpContext final : public MKLOpContext {
 
 // Weight-only quantization
 using SerializationTypeWoqLinearPrePack = std::tuple<
-    at::Tensor, // weight
-    std::vector<int64_t>, // weight shape
-    at::Tensor, // scales
-    at::Tensor, // zero points
-    c10::optional<at::Tensor>, // bias
-    c10::optional<int64_t>, // batch size
-    bool, // is_int4
-    int64_t, // group size
-    int64_t, // lowp_mode
-    int64_t, // num_concats
-    int64_t>; // act_quant_mode
+    at::Tensor,
+    c10::optional<at::Tensor>,
+    c10::optional<int64_t>,
+    int64_t,
+    int64_t>;
 
 class WoqLinearOpContext : public torch::jit::CustomClassHolder {
  protected:
@@ -380,22 +374,13 @@ class WoqLinearOpContext : public torch::jit::CustomClassHolder {
  public:
   SerializationTypeWoqLinearPrePack unpack() {
     auto orig_weight_ = this->to_public(this->get_at_packed_weight());
-    auto weight_shape_ = this->get_weight_shape();
     auto orig_bias_ = this->get_context().at_bias_;
-    auto scales = this->get_scales();
-    auto zero_points = this->get_zero_points();
     return std::make_tuple(
         orig_weight_,
-        weight_shape_,
-        scales,
-        zero_points,
         orig_bias_,
         batch_size_,
-        this->get_context().is_int4_,
-        this->get_context().group_size_,
         this->get_context().lowp_mode_,
-        this->get_context().num_concats_,
-        this->get_context().act_quant_mode_);
+        this->get_context().num_concats_);
   }
 
   virtual at::Tensor get_data_handle() = 0;
@@ -408,6 +393,16 @@ class WoqLinearOpContext : public torch::jit::CustomClassHolder {
       const torch::List<c10::optional<at::Scalar>>& scalars,
       const c10::optional<c10::string_view>& algorithm) = 0;
 
+  virtual at::Tensor run_add(
+      const at::Tensor& input,
+      at::Tensor& accumu,
+      const c10::optional<at::Scalar>& alpha) = 0;
+
+  virtual at::Tensor run_add_relu(
+      const at::Tensor& input,
+      at::Tensor& accumu,
+      const c10::optional<at::Scalar>& alpha) = 0;
+
   virtual at::Tensor run_add(
       const at::Tensor& input,
       const std::vector<at::Tensor>& others) = 0;
@@ -422,12 +417,6 @@ class WoqLinearOpContext : public torch::jit::CustomClassHolder {
 
   virtual c10::optional<at::Tensor> get_at_bias() = 0;
 
-  virtual at::Tensor get_scales() = 0;
-
-  virtual at::Tensor get_zero_points() = 0;
-
-  virtual std::vector<int64_t> get_weight_shape() = 0;
-
   virtual at::Tensor pack(const at::Tensor& tensor) = 0;
 
   virtual detail::ContextLinearWoq& get_context() = 0;
@@ -464,6 +453,16 @@ class IpexWoqLinearOpContext final : public WoqLinearOpContext {
       const torch::List<c10::optional<at::Scalar>>& scalars,
       const c10::optional<c10::string_view>& algorithm) override;
 
+  virtual at::Tensor run_add(
+      const at::Tensor& input,
+      at::Tensor& accumu,
+      const c10::optional<at::Scalar>& alpha) override;
+
+  virtual at::Tensor run_add_relu(
+      const at::Tensor& input,
+      at::Tensor& accumu,
+      const c10::optional<at::Scalar>& alpha) override;
+
   virtual at::Tensor run_add(
       const at::Tensor& input,
       const std::vector<at::Tensor>& others) override;
@@ -478,28 +477,16 @@ class IpexWoqLinearOpContext final : public WoqLinearOpContext {
 
   virtual c10::optional<at::Tensor> get_at_bias() override;
 
-  virtual at::Tensor get_scales() override;
-
-  virtual at::Tensor get_zero_points() override;
-
-  virtual std::vector<int64_t> get_weight_shape() override;
-
   virtual at::Tensor pack(const at::Tensor& tensor) override;
 
   virtual detail::ContextLinearWoq& get_context() override;
 
   static c10::intrusive_ptr<WoqLinearOpContext> create_context(
       at::Tensor&& weight,
-      std::vector<int64_t>&& weight_shape,
-      at::Tensor&& scales_fp32,
-      at::Tensor&& zp_fp32,
       c10::optional<at::Tensor>&& bias,
       c10::optional<int64_t> batch_size,
-      bool is_int4,
-      int64_t group_size,
       int64_t lowp_mode,
-      int64_t num_concats,
-      int64_t act_quant_mode);
+      int64_t num_concats);
 
   virtual void load_from_ctx(
       c10::intrusive_ptr<WoqLinearOpContext> other) override;
diff --git a/csrc/cpu/jit/cpu/kernels/RegisterOpContextClass.cpp b/csrc/cpu/jit/cpu/kernels/RegisterOpContextClass.cpp
index 20300577e..34229af3a 100644
--- a/csrc/cpu/jit/cpu/kernels/RegisterOpContextClass.cpp
+++ b/csrc/cpu/jit/cpu/kernels/RegisterOpContextClass.cpp
@@ -128,29 +128,16 @@ TORCH_LIBRARY(ipex_prepack, m) {
           [](SerializationTypeWoqLinearPrePack state)
               -> c10::intrusive_ptr<WoqLinearOpContext> { // __setstate__
             return createWoqLinearPrePackOpContext(
-                std::move(std::get<0>(state)), // weight
-                std::move(std::get<1>(state)), // weight shape
-                std::move(std::get<2>(state)), // scales
-                std::move(std::get<3>(state)), // zero points
-                std::move(std::get<4>(state)), // bias
-                std::move(std::get<5>(state)), // batch size
-                std::move(std::get<6>(state)), // is_int4
-                std::move(std::get<7>(state)), // group size
-                std::move(std::get<8>(state)), // lowp_mode
-                std::move(std::get<9>(state)), // num_concats
-                std::move(std::get<10>(state))); // act_quant_mode
+                std::move(std::get<0>(state)),
+                std::move(std::get<1>(state)),
+                std::move(std::get<2>(state)),
+                std::move(std::get<3>(state)),
+                std::move(std::get<4>(state)));
           })
       .def(
           "get_weight",
           &torch_ipex::cpu::WoqLinearOpContext::get_at_packed_weight)
       .def("get_bias", &torch_ipex::cpu::WoqLinearOpContext::get_at_bias)
-      .def("get_scales", &torch_ipex::cpu::WoqLinearOpContext::get_scales)
-      .def(
-          "get_zero_points",
-          &torch_ipex::cpu::WoqLinearOpContext::get_zero_points)
-      .def(
-          "get_weight_shape",
-          &torch_ipex::cpu::WoqLinearOpContext::get_weight_shape)
       .def("pack", &torch_ipex::cpu::WoqLinearOpContext::pack)
       .def("to_public", &torch_ipex::cpu::WoqLinearOpContext::to_public)
       .def(
@@ -175,10 +162,10 @@ TORCH_LIBRARY(ipex_prepack, m) {
       "bool input_is_channels_last, int[] input_sizes) "
       "-> __torch__.torch.classes.ipex_prepack.ConvTransposeOpContext");
   m.def(
-      "weight_only_qlinear_prepack(Tensor W, int[] W_shape, Tensor scales, Tensor zero_points, Tensor? B, int? batch_size, bool is_int4, int group_size, int lowp_mode, int num_concats, int act_quant_mode) "
+      "weight_only_qlinear_prepack(Tensor W, Tensor? B, int? batch_size, int lowp_mode, int num_concats) "
       "-> __torch__.torch.classes.ipex_prepack.WoqLinearOpContext");
   m.def(
-      "weight_only_qlinear_prepack_int4(Tensor W, Tensor scales, Tensor zero_points, Tensor? B, int? batch_size, int group_size, int lowp_mode, int num_concats, int act_quant_mode) "
+      "weight_only_qlinear_prepack_int4(Tensor W, Tensor scales, Tensor zero_points, Tensor? B, int? batch_size, int lowp_mode, int num_concats) "
       "-> __torch__.torch.classes.ipex_prepack.WoqLinearOpContext");
 }
 
@@ -189,7 +176,7 @@ TORCH_LIBRARY_IMPL(ipex_prepack, CPU, m) {
   m.impl(
       "conv_transpose_prepack", TORCH_FN(createConvTransposePrePackOpContext));
 }
-TORCH_LIBRARY_IMPL(ipex_prepack, CPU, m) {
+TORCH_LIBRARY_IMPL(ipex_prepack, QuantizedCPU, m) {
   m.impl(
       "weight_only_qlinear_prepack", TORCH_FN(createWoqLinearPrePackOpContext));
 }
diff --git a/csrc/cpu/jit/passes/graph_rewrite.cpp b/csrc/cpu/jit/passes/graph_rewrite.cpp
index 78b835abd..adc43a2c9 100644
--- a/csrc/cpu/jit/passes/graph_rewrite.cpp
+++ b/csrc/cpu/jit/passes/graph_rewrite.cpp
@@ -819,6 +819,77 @@ void replaceMergedEmbCatWithQmergedEmbCat(std::shared_ptr<Graph>& graph) {
     rewriter.runOnGraph(graph);
   }
 }
+void replaceMLPerfMergedEmbCatWithQMLPerfMergedEmbCat(std::shared_ptr<Graph>& graph) {
+  // graph->print(std::cout);
+  std::vector<std::string> patterns;
+  std::vector<std::string> replacements;
+  std::string graph_common_head = R"(graph()";
+  std::string graph_common_tail = R"(, %index, %qdense, %num_hot, %o_scale, %o_zp, %o_dtype):
+  )";
+  std::string list_construct_common_head =
+      R"(%weights : Tensor[] = prim::ListConstruct()";
+  std::string list_construct_common_tail = R"() )";
+  std::string replacement_common_tail =
+      R"(%out = ipex::qmlperf_merged_emb_cat(%weights, %index, %qdense, %num_hot, %o_scale, %o_zp, %o_dtype) return (%out) )";
+  std::string pattern_common_tail =
+      R"(%dense=aten::dequantize(%qdense)  %out = torch_ipex::mlperf_merged_emb_cat(%weights, %index, %dense, %num_hot)  %qout = aten::quantize_per_tensor(%out, %o_scale, %o_zp, %o_dtype) return (%qout) )";
+
+  for (auto* n : graph->block()->nodes()) {
+    if (n->kind() ==
+        Symbol::fromQualString("torch_ipex::mlperf_merged_emb_cat")) {
+      size_t id = 0;
+      auto weightslist = n->input(0)->node();
+      auto indexlist = n->input(1)->node();
+
+      bool is_quantized = std::any_of(
+          weightslist->inputs().begin(),
+          weightslist->inputs().end(),
+          [](auto& v) {
+            return v->node()->kind() == Symbol::aten("dequantize");
+          });
+
+      if (!is_quantized)
+        return;
+
+      std::string pattern = R"()";
+      std::string replacement = R"()";
+      std::string dequantizes = R"()";
+      std::vector<std::string> qinputs;
+      std::vector<std::string> dqinputs;
+      for (auto input : weightslist->inputs()) {
+        if (input->node()->kind() == Symbol::aten("dequantize")) {
+          qinputs.push_back("%q" + std::to_string(id));
+          dqinputs.push_back("%dq" + std::to_string(id));
+          std::string dequantize = "%dq" + std::to_string(id) +
+              " : Tensor = aten::dequantize(" + "%q" + std::to_string(id) + ")";
+          dequantizes.append(dequantize);
+          ++id;
+        }
+      }
+
+      std::string header =
+          graph_common_head + c10::Join(", ", qinputs) + graph_common_tail;
+      pattern += header;
+      pattern += dequantizes;
+      pattern += list_construct_common_head + c10::Join(", ", dqinputs) +
+          list_construct_common_tail;
+      pattern += pattern_common_tail;
+      patterns.push_back(pattern);
+
+      replacement = header;
+      replacement += list_construct_common_head + c10::Join(", ", qinputs) +
+          list_construct_common_tail;
+      replacement += replacement_common_tail;
+      replacements.push_back(replacement);
+    }
+  }
+
+  SubgraphRewriter rewriter;
+  for (size_t i = 0; i < patterns.size(); i++) {
+    rewriter.RegisterRewritePattern(patterns[i], replacements[i]);
+    rewriter.runOnGraph(graph);
+  }
+}
 
 // When converting LSTM to int8 LSTM, IPEX will pre-hook the LSTM forward
 // function to insert quant and dequant node. After converting the model, when
@@ -1313,46 +1384,46 @@ void replaceAtenMaxPool2dWithIpexMaxPool2d(std::shared_ptr<Graph>& graph) {
 
 void simplifyAllReduce(std::shared_ptr<Graph>& graph) {
   std::string all_reduce_v1 = R"(
-    graph(%a, %weight, %out_features1, %out_features2, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha, %idx, %no, %dtype, %zero):
+    graph(%a, %weight, %out_features1, %out_features2, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha, %idx, %no, %dtype, %zero):
       %r1 = torch_ipex::tpp_linear(%a, %weight, %out_features1)
-      %r2 = deepspeed_comm::all_reduce(%r1)
+      %r2 = deepspeed_comm::all_reduce(%r1, %reduceop, %tag, %ranks, %group_size)
       %r3 = torch_ipex::tpp_linear_gelu(%b, %fc_in_weight, %fc_in_bias, %out_features2)
       %r4 = aten::to(%r3, %idx, %no, %no, %dtype)
       %r5 = aten::contiguous(%r4, %zero)
       %r6 = torch_ipex::tpp_linear(%r5, %fc_out_weight, %out_features1)
-      %r7 = deepspeed_comm::all_reduce(%r6)
+      %r7 = deepspeed_comm::all_reduce(%r6, %reduceop, %tag, %ranks, %group_size)
       %r8 = aten::add_(%r7, %fc_out_bias, %alpha)
       %r = aten::add(%r2, %r8, %alpha)
       return (%r) )";
   std::string all_reduce_repl_v1 = R"(
-    graph(%a, %weight, %out_features1, %out_features2, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha, %idx, %no, %dtype, %zero):
+    graph(%a, %weight, %out_features1, %out_features2, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha, %idx, %no, %dtype, %zero):
       %r1 = torch_ipex::tpp_linear(%a, %weight, %out_features1)
       %r2 = torch_ipex::tpp_linear_gelu(%b, %fc_in_weight, %fc_in_bias, %out_features2)
       %r3 = aten::to(%r2, %idx, %no, %no, %dtype)
       %r4 = aten::contiguous(%r3, %zero)
       %r5 = torch_ipex::tpp_linear(%r4, %fc_out_weight, %out_features1)
       %r6 = aten::add(%r1, %r5, %alpha)
-      %r7 = deepspeed_comm::all_reduce(%r6)
+      %r7 = deepspeed_comm::all_reduce(%r6, %reduceop, %tag, %ranks, %group_size)
       %r = aten::add_(%r7, %fc_out_bias, %alpha)
       return (%r) )";
 
   std::string all_reduce_v2 = R"(
-    graph(%a, %weight, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha):
+    graph(%a, %weight, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha):
       %r1 = ipex_prepack::linear_run(%a, %weight)
-      %r2 = deepspeed_comm::all_reduce(%r1)
+      %r2 = deepspeed_comm::all_reduce(%r1, %reduceop, %tag, %ranks, %group_size)
       %r3 = ipex_prepack::linear_gelu_run(%b, %fc_in_weight, %fc_in_bias)
       %r4 = ipex_prepack::linear_run(%r3, %fc_out_weight)
-      %r5 = deepspeed_comm::all_reduce(%r4)
+      %r5 = deepspeed_comm::all_reduce(%r4, %reduceop, %tag, %ranks, %group_size)
       %r6 = aten::add_(%r5, %fc_out_bias, %alpha)
       %r = aten::add(%r2, %r6, %alpha)
       return (%r) )";
   std::string all_reduce_repl_v2 = R"(
-    graph(%a, %weight, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha):
+    graph(%a, %weight, %reduceop, %tag, %ranks, %group_size, %b, %fc_in_weight, %fc_in_bias, %fc_out_weight, %fc_out_bias, %alpha):
       %r1 = ipex_prepack::linear_run(%a, %weight)
       %r2 = ipex_prepack::linear_gelu_run(%b, %fc_in_weight, %fc_in_bias)
       %r3 = ipex_prepack::linear_run(%r2, %fc_out_weight)
       %r4 = aten::add(%r1, %r3, %alpha)
-      %r5 = deepspeed_comm::all_reduce(%r4)
+      %r5 = deepspeed_comm::all_reduce(%r4, %reduceop, %tag, %ranks, %group_size)
       %r = aten::add_(%r5, %fc_out_bias, %alpha)
       return (%r) )";
 
diff --git a/csrc/cpu/jit/passes/graph_rewrite.h b/csrc/cpu/jit/passes/graph_rewrite.h
index 6ee12fd10..c43049490 100644
--- a/csrc/cpu/jit/passes/graph_rewrite.h
+++ b/csrc/cpu/jit/passes/graph_rewrite.h
@@ -37,6 +37,8 @@ void replaceInteractionWithQInteraction(
     std::shared_ptr<torch::jit::Graph>& graph);
 void replaceMergedEmbCatWithQmergedEmbCat(
     std::shared_ptr<torch::jit::Graph>& graph);
+void replaceMLPerfMergedEmbCatWithQMLPerfMergedEmbCat(
+    std::shared_ptr<torch::jit::Graph>& graph);
 void preprocessSizeForQLstm(std::shared_ptr<torch::jit::Graph>& graph);
 void replaceLstmWithQLstm(std::shared_ptr<torch::jit::Graph>& graph);
 void replaceAddWithQAdd(std::shared_ptr<torch::jit::Graph>& graph);
diff --git a/csrc/cpu/jit/passes/register_dnnl_jit_ops.cpp b/csrc/cpu/jit/passes/register_dnnl_jit_ops.cpp
index df7ec3e59..8a1a217bf 100644
--- a/csrc/cpu/jit/passes/register_dnnl_jit_ops.cpp
+++ b/csrc/cpu/jit/passes/register_dnnl_jit_ops.cpp
@@ -7,6 +7,7 @@
 #include "aten/ConcatBnRelu.h"
 #include "aten/MergedEmbCat.h"
 #include "aten/RMSNorm.h"
+#include "aten/MergedEmbWithCat.h"
 #include "cpu/kernels/ConvPacked.h"
 #include "cpu/kernels/ConvTransposePacked.h"
 #include "cpu/kernels/Einsum.h"
@@ -1399,6 +1400,26 @@ torch::jit::RegisterOperators op({
         },
         aliasAnalysisFromSchema()),
 
+    Operator(
+        "ipex::qmlperf_merged_emb_cat(Tensor[] weights,  Tensor[] index, Tensor dense, int[] num_hots, float o_scale, int o_zp, "
+        "ScalarType o_dtype) -> Tensor",
+        [](const Node* node) -> Operation {
+          return [](Stack* stack) {
+            auto result = dil_qmlperf_merged_emb_cat(
+                (std::move(peek(stack, 0, 7))).toTensorVector(),
+                (std::move(peek(stack, 1, 7))).toTensorVector(),
+                (std::move(peek(stack, 2, 7))).toTensor(),
+                (std::move(peek(stack, 3, 7))).toIntVector(),
+                (std::move(peek(stack, 4, 7))).toDouble(),
+                (std::move(peek(stack, 5, 7))).toInt(),
+                (std::move(peek(stack, 6, 7))).toScalarType());
+            drop(stack, 7);
+            torch::jit::pack(stack, std::move(result));
+            return 0;
+          };
+        },
+        aliasAnalysisFromSchema()),
+
     Operator(
         "ipex::quantized_lstm(Tensor quantized_input, Tensor[] hx, Tensor [] quantized_weights, bool has_biases, int num_layers, float dropout_p, bool train, bool bidirectional, bool batch_first, float scale, int zp, int dtype) -> (Tensor, Tensor, Tensor)",
         [](const Node* node) -> Operation {
diff --git a/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h b/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h
index f14871538..a8031dddf 100644
--- a/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h
+++ b/csrc/cpu/tpp/kernels/TPPGEMMKrnl.h
@@ -92,14 +92,6 @@ inline void tpp_linear_bias(
   auto in_sizes = t_in.sizes();
   auto wt_sizes = t_wt.sizes();
   auto BS = in_sizes[0] * in_sizes[1];
-  if (BS > FT_OPT_SIZE) { // first token compute
-    if (wt_sizes[3] != 100) {
-      t_wt = wt_tensor_for_first_token<T>(t_wt);
-      wt_sizes = t_wt.sizes();
-    }
-    large_cache_opt = true;
-  }
-
   auto C = in_sizes[2];
 
   auto Nc = wt_sizes[1];
@@ -177,14 +169,10 @@ inline void tpp_linear_no_bias(
     at::Tensor& t_out) {
   auto in_sizes = t_in.sizes();
   auto BS = in_sizes[0] * in_sizes[1];
-  auto wt_sizes = t_wt.sizes();
   if (BS > FT_OPT_SIZE) { // first token compute
-    if (wt_sizes[3] != 100) {
-      t_wt = wt_tensor_for_first_token<T>(t_wt);
-      wt_sizes = t_wt.sizes();
-    }
-    large_cache_opt = true;
+    t_wt = wt_tensor_for_first_token<T>(t_wt);
   }
+  auto wt_sizes = t_wt.sizes();
   auto C = in_sizes[2];
 
   auto Nc = wt_sizes[1];
@@ -250,7 +238,6 @@ inline void tpp_linear_mul(
   auto BS = in_sizes[0] * in_sizes[1];
   if (BS > FT_OPT_SIZE) { // first token compute
     t_wt = wt_tensor_for_first_token<T>(t_wt);
-    large_cache_opt = true;
   }
   auto wt_sizes = t_wt.sizes();
   auto C = in_sizes[2];
@@ -343,9 +330,7 @@ inline void tpp_linear_add_add(
   auto BS = in_sizes[0] * in_sizes[1];
   if (BS > FT_OPT_SIZE) { // first token compute
     t_wt = wt_tensor_for_first_token<T>(t_wt);
-    large_cache_opt = true;
   }
-
   auto wt_sizes = t_wt.sizes();
   auto C = in_sizes[2];
 
@@ -438,7 +423,6 @@ inline void tpp_linear_gelu(
   auto BS = in_sizes[0] * in_sizes[1];
   if (BS > FT_OPT_SIZE) { // first token compute
     t_wt = wt_tensor_for_first_token<T>(t_wt);
-    large_cache_opt = true;
   }
   auto wt_sizes = t_wt.sizes();
   auto C = in_sizes[2];
@@ -528,7 +512,6 @@ inline void tpp_linear_add(
   auto BS = in_sizes[0] * in_sizes[1];
   if (BS > FT_OPT_SIZE) { // first token compute
     t_wt = wt_tensor_for_first_token<T>(t_wt);
-    large_cache_opt = true;
   }
   auto wt_sizes = t_wt.sizes();
   auto C = in_sizes[2];
@@ -618,7 +601,6 @@ inline void tpp_linear_silu(
   auto BS = in_sizes[0] * in_sizes[1];
   if (BS > FT_OPT_SIZE) { // first token compute
     t_wt = wt_tensor_for_first_token<T>(t_wt);
-    large_cache_opt = true;
   }
   auto wt_sizes = t_wt.sizes();
   auto C = in_sizes[2];
@@ -707,7 +689,6 @@ inline void tpp_linear_relu(
   auto BS = in_sizes[0] * in_sizes[1];
   if (BS > FT_OPT_SIZE) { // first token compute
     t_wt = wt_tensor_for_first_token<T>(t_wt);
-    large_cache_opt = true;
   }
   auto wt_sizes = t_wt.sizes();
   auto C = in_sizes[2];
diff --git a/docker/Dockerfile.compile b/docker/Dockerfile.compile
index 8989c56c1..279c5461d 100644
--- a/docker/Dockerfile.compile
+++ b/docker/Dockerfile.compile
@@ -6,12 +6,8 @@
 #           https://docs.docker.com/develop/develop-images/build_enhancements/
 
 ARG BASE_IMAGE=ubuntu:22.04
-FROM ${BASE_IMAGE} AS base
-RUN if [ -f /etc/apt/apt.conf.d/proxy.conf ]; then rm /etc/apt/apt.conf.d/proxy.conf; fi && \
-    if [ ! -z ${HTTP_PROXY} ]; then echo "Acquire::http::Proxy \"${HTTP_PROXY}\";" >> /etc/apt/apt.conf.d/proxy.conf; fi && \
-    if [ ! -z ${HTTPS_PROXY} ]; then echo "Acquire::https::Proxy \"${HTTPS_PROXY}\";" >> /etc/apt/apt.conf.d/proxy.conf; fi
+FROM ${BASE_IMAGE}
 RUN apt update && \
-    apt full-upgrade -y && \
     DEBIAN_FRONTEND=noninteractive apt install --no-install-recommends -y \
     sudo \
     ca-certificates \
@@ -19,56 +15,31 @@ RUN apt update && \
     curl \
     wget \
     vim \
+    ccache \
     numactl \
-    gcc-12 \
-    g++-12 \
-    make
-RUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 100 && \
-    update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-12 100 && \
-    update-alternatives --install /usr/bin/cc cc /usr/bin/gcc 100 && \
-    update-alternatives --install /usr/bin/c++ c++ /usr/bin/g++ 100
-RUN apt clean && \
-    rm -rf /var/lib/apt/lists/* && \
-    if [ -f /etc/apt/apt.conf.d/proxy.conf ]; then rm /etc/apt/apt.conf.d/proxy.conf; fi
-
-RUN useradd -m ubuntu && \
-    echo 'ubuntu ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers
+    make \
+    libjpeg-dev \
+    libpng-dev \
+    && rm -rf /var/lib/apt/lists/*
+RUN useradd -m ubuntu
+RUN echo 'ubuntu ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers
 USER ubuntu
 WORKDIR /home/ubuntu
 
 RUN curl -fsSL -v -o miniconda.sh -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh  && \
-    bash miniconda.sh -b -p ./miniconda3 && \
+    bash miniconda.sh -b -p ~/miniconda3 && \
     rm miniconda.sh && \
-    echo "source ~/miniconda3/bin/activate" >> ./.bashrc
+    echo "source ~/miniconda3/bin/activate" >> ~/.bashrc
 
-FROM base AS dev
-COPY --chown=ubuntu:ubuntu . ./intel-extension-for-pytorch
-RUN cp ./intel-extension-for-pytorch/scripts/compile_bundle.sh ./ && \
-    sed -i "s/VER_IPEX=.*/VER_IPEX=/" compile_bundle.sh
-RUN . ./miniconda3/bin/activate && \
-    conda create -y -n compile_py310 python=3.10 && conda activate compile_py310 && \
+RUN curl -fsSL -v -o compile_bundle.sh -O https://github.com/intel/intel-extension-for-pytorch/raw/v2.1.0%2Bcpu/scripts/compile_bundle.sh && \
+    . ~/miniconda3/bin/activate && \
+    conda create -y -n py310 python=3.10 && \
+    conda activate py310 && \
     bash compile_bundle.sh && \
-    cd intel-extension-for-pytorch && \
-    python -m pip install pyyaml && \
-    VER_TORCH=$(python tools/yaml_utils.py -f dependency_version.yml -d pytorch -k version) && \
-    VER_TORCHVISION=$(python tools/yaml_utils.py -f dependency_version.yml -d torchvision -k version) && \
-    VER_TORCHAUDIO=$(python tools/yaml_utils.py -f dependency_version.yml -d torchaudio -k version) && \
-    python -m pip uninstall -y pyyaml && \
-    cd .. && \
-    echo ${VER_TORCH} | grep "dev" > /dev/null; TORCH_DEV=$?; URL_NIGHTLY=""; if [ ${TORCH_DEV} -eq 0 ]; then URL_NIGHTLY="nightly/"; fi; echo "#!/bin/bash\npython -m pip install torch==${VER_TORCH} torchvision==${VER_TORCHVISION} torchaudio==${VER_TORCHAUDIO} --index-url https://download.pytorch.org/whl/${URL_NIGHTLY}cpu" > torch_install.sh
-
-FROM base AS deploy
-COPY --from=dev --chown=ubuntu:ubuntu /home/ubuntu/intel-extension-for-pytorch/dist ./wheels
-COPY --from=dev --chown=ubuntu:ubuntu /home/ubuntu/torch_install.sh .
-RUN . ./miniconda3/bin/activate && \
-    conda create -y -n py310 python=3.10 && conda activate py310 && \
-    bash ./torch_install.sh && rm ./torch_install.sh && \
-    python -m pip install ./wheels/*.whl && \
-    python -m pip install intel-openmp && \
+    rm -rf intel-extension-for-pytorch llvm-project compile_bundle.sh && \
     conda install -y jemalloc gperftools -c conda-forge && \
-    python -m pip cache purge && \
-    conda clean -a -y && \
-    rm -rf ./wheels && \
-    echo "conda activate py310" >> ./.bashrc && \
-    echo "export LD_PRELOAD=${CONDA_PREFIX}/lib/libstdc++.so" >> ./.bashrc && \
-    echo "echo \"**Note:** For better performance, please consider to launch workloads with command 'ipexrun'.\"" >> ./.bashrc
+    python -m pip uninstall -y mkl-static mkl-include && \
+    python -m pip install intel-openmp && \
+    echo "conda activate py310" >> ~/.bashrc && \
+    echo "export LD_PRELOAD=${CONDA_PREFIX}/lib/libstdc++.so" >> ~/.bashrc && \
+    echo "echo \"**Note:** For better performance, please consider to launch workloads with command 'ipexrun'.\"" >> ~/.bashrc
diff --git a/docker/Dockerfile.prebuilt b/docker/Dockerfile.prebuilt
index f88a69530..8a6b22a75 100644
--- a/docker/Dockerfile.prebuilt
+++ b/docker/Dockerfile.prebuilt
@@ -27,10 +27,10 @@ RUN ${PYTHON} -m pip --no-cache-dir install --upgrade \
 # Some TF tools expect a "python" binary
 RUN ln -s $(which ${PYTHON}) /usr/local/bin/python
 
-ARG IPEX_VERSION=2.1.100
-ARG PYTORCH_VERSION=2.1.1
-ARG TORCHAUDIO_VERSION=2.1.1
-ARG TORCHVISION_VERSION=0.16.1
+ARG IPEX_VERSION=2.1.0
+ARG PYTORCH_VERSION=2.1.0
+ARG TORCHAUDIO_VERSION=2.1.0
+ARG TORCHVISION_VERSION=0.16.0
 ARG TORCH_CPU_URL=https://download.pytorch.org/whl/cpu/torch_stable.html
 
 RUN \
diff --git a/docker/README.md b/docker/README.md
index 3a83b7554..8e8e16c92 100644
--- a/docker/README.md
+++ b/docker/README.md
@@ -10,28 +10,14 @@
 
   ```console
   $ cd $DOCKERFILE_DIR
-  $ DOCKER_BUILDKIT=1 docker build -f Dockerfile.prebuilt -t intel-extension-for-pytorch:2.1.100 .
+  $ DOCKER_BUILDKIT=1 docker build -f Dockerfile.prebuilt -t intel-extension-for-pytorch:prebuilt .
+  $ docker run --rm intel-extension-for-pytorch:prebuilt python -c "import torch; import intel_extension_for_pytorch as ipex; print('torch:', torch.__version__,' ipex:',ipex.__version__)"
   ```
 
   Run the following commands to build a `conda` based container with Intel® Extension for PyTorch\* compiled from source:
 
   ```console
-  $ git clone https://github.com/intel/intel-extension-for-pytorch.git
-  $ cd intel-extension-for-pytorch
-  $ git submodule sync
-  $ git submodule update --init --recursive
-  $ cd ..
-  $ DOCKER_BUILDKIT=1 docker build -f docker/Dockerfile.compile -t intel-extension-for-pytorch:2.1.100 .
-  ```
-
-* Sanity Test
-
-  When a docker image is built out, Run the command below to launch into a container:
-  ```console
-  $ docker run --rm -it intel-extension-for-pytorch:2.1.100 bash
-  ```
-
-  Then run the command below inside the container to verify correct installation.
-  ```console
-  # python -c "import torch; import intel_extension_for_pytorch as ipex; print('torch:', torch.__version__,' ipex: ',ipex.__version__)"
+  $ cd $DOCKERFILE_DIR
+  $ DOCKER_BUILDKIT=1 docker build -f Dockerfile.compile -t intel-extension-for-pytorch:compile .
+  $ docker run --rm intel-extension-for-pytorch:compile python -c "import torch; import intel_extension_for_pytorch as ipex; print('torch:', torch.__version__,' ipex:',ipex.__version__)"
   ```
diff --git a/docs/conf.py b/docs/conf.py
index 4aa96931c..02236f523 100644
--- a/docs/conf.py
+++ b/docs/conf.py
@@ -18,7 +18,7 @@ sys.path.insert(0, os.path.abspath('.'))
 
 # -- Project information -----------------------------------------------------
 
-project = 'Intel&#174 Extension for PyTorch*'
+project = 'intel_extension_for_pytorch'
 copyright = 'Intel(R)'
 author = ''
 
diff --git a/docs/index.rst b/docs/index.rst
index ca0ec0a98..51f532a4c 100644
--- a/docs/index.rst
+++ b/docs/index.rst
@@ -2,35 +2,14 @@
    :description: This website introduces Intel® Extension for PyTorch*
    :keywords: Intel optimization, PyTorch, Intel® Extension for PyTorch*, GPU, discrete GPU, Intel discrete GPU
 
-Intel® Extension for PyTorch*
-#############################
+Welcome to Intel® Extension for PyTorch* Documentation
+######################################################
 
-Intel® Extension for PyTorch* extends PyTorch* with the latest performance optimizations for Intel hardware. 
-Optimizations take advantage of Intel® Advanced Vector Extensions 512 (Intel® AVX-512) Vector Neural Network Instructions (VNNI) and Intel® Advanced Matrix Extensions (Intel® AMX) on Intel CPUs as well as Intel X\ :sup:`e`\ Matrix Extensions (XMX) AI engines on Intel discrete GPUs. 
-Moreover, Intel® Extension for PyTorch* provides easy GPU acceleration for Intel discrete GPUs through the PyTorch* ``xpu`` device.
+Intel® Extension for PyTorch* extends PyTorch* with up-to-date features optimizations for an extra performance boost on Intel hardware. Optimizations take advantage of AVX-512 Vector Neural Network Instructions (AVX512 VNNI) and Intel® Advanced Matrix Extensions (Intel® AMX) on Intel CPUs as well as Intel X\ :sup:`e`\  Matrix Extensions (XMX) AI engines on Intel discrete GPUs. Moreover, through PyTorch* `xpu` device, Intel® Extension for PyTorch* provides easy GPU acceleration for Intel discrete GPUs with PyTorch*.
 
-In the current technological landscape, Generative AI (GenAI) workloads and models have gained widespread attention and popularity. Large Language Models (LLMs) have emerged as the dominant models driving these GenAI applications. Starting from 2.1.0, specific optimizations for certain 
-LLMs are introduced in the Intel® Extension for PyTorch*. For more information on LLM optimizations, refer to the `Large Language Models (LLM) <tutorials/llm.html>`_ section.
+Intel® Extension for PyTorch* provides optimizations for both eager mode and graph mode, however, compared to eager mode, graph mode in PyTorch* normally yields better performance from optimization techniques, such as operation fusion. Intel® Extension for PyTorch* amplifies them with more comprehensive graph optimizations. Therefore we recommend you to take advantage of Intel® Extension for PyTorch* with `TorchScript <https://pytorch.org/docs/stable/jit.html>`_ whenever your workload supports it. You could choose to run with `torch.jit.trace()` function or `torch.jit.script()` function, but based on our evaluation, `torch.jit.trace()` supports more workloads so we recommend you to use `torch.jit.trace()` as your first choice.
 
-The extension can be loaded as a Python module for Python programs or linked as a C++ library for C++ programs. In Python scripts, users can enable it dynamically by importing ``intel_extension_for_pytorch``.
-
-.. note:: 
-   
-   - GPU features are not included in CPU-only packages.
-   - Optimizations for CPU-only may have a newer code base due to different development schedules.
-
-Intel® Extension for PyTorch* has been released as an open–source project at `Github <https://github.com/intel/intel-extension-for-pytorch>`_. You can find the source code and instructions on how to get started at:
-
-- **CPU**: `CPU main branch <https://github.com/intel/intel-extension-for-pytorch/tree/main>`_ |  `Quick Start <https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/getting_started>`_ 
-- **XPU**: `XPU main branch <https://github.com/intel/intel-extension-for-pytorch/tree/xpu-main>`_ | `Quick Start <https://intel.github.io/intel-extension-for-pytorch/xpu/latest/tutorials/getting_started>`_
-
-You can find more information about the product at:
-
-- `Features <https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features>`_
-- `Performance <https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance>`_ 
-
-Architecture
-------------
+The extension can be loaded as a Python module for Python programs or linked as a C++ library for C++ programs. In Python scripts users can enable it dynamically by importing `intel_extension_for_pytorch`.
 
 Intel® Extension for PyTorch* is structured as shown in the following figure:
 
@@ -39,62 +18,26 @@ Intel® Extension for PyTorch* is structured as shown in the following figure:
   :align: center
   :alt: Architecture of Intel® Extension for PyTorch*
 
-  Architecture of Intel® Extension for PyTorch*
+|
 
-- **Eager Mode**: In the eager mode, the PyTorch frontend is extended with custom Python modules (such as fusion modules), optimal optimizers, and INT8 quantization APIs. Further performance improvement is achieved by converting eager-mode models into graph mode using extended graph fusion passes. 
-- **Graph Mode**: In the graph mode, fusions reduce operator/kernel invocation overhead, resulting in improved performance. Compared to the eager mode, the graph mode in PyTorch* normally yields better performance from the optimization techniques like operation fusion. Intel® Extension for PyTorch* amplifies them with more comprehensive graph optimizations. Both PyTorch ``Torchscript`` and ``TorchDynamo`` graph modes are supported. With ``Torchscript``, we recommend using ``torch.jit.trace()`` as your preferred option, as it generally supports a wider range of workloads compared to ``torch.jit.script()``. With ``TorchDynamo``, ipex backend is available to provide good performances.
-- **CPU Optimization**: On CPU, Intel® Extension for PyTorch* automatically dispatches operators to underlying kernels based on detected instruction set architecture (ISA). The extension leverages vectorization and matrix acceleration units available on Intel hardware. The runtime extension offers finer-grained thread runtime control and weight sharing for increased efficiency.
-- **GPU Optimization**: On GPU, optimized operators and kernels are implemented and registered through PyTorch dispatching mechanism. These operators and kernels are accelerated from native vectorization feature and matrix calculation feature of Intel GPU hardware. Intel® Extension for PyTorch* for GPU utilizes the `DPC++ <https://github.com/intel/llvm#oneapi-dpc-compiler>`_ compiler that supports the latest `SYCL* <https://registry.khronos.org/SYCL/specs/sycl-2020/html/sycl-2020.html>`_ standard and also a number of extensions to the SYCL* standard, which can be found in the `sycl/doc/extensions <https://github.com/intel/llvm/tree/sycl/sycl/doc/extensions>`_ directory. 
+Optimizations for both eager mode and graph mode contribute to extra performance accelerations with the extension. In eager mode, the PyTorch frontend is extended with custom Python modules (such as fusion modules), optimal optimizers, and INT8 quantization APIs. Further performance boost is available by converting the eager-mode model into graph mode via extended graph fusion passes. In the graph mode, the fusions reduce operator/kernel invocation overheads, and thus increase performance. On CPU, Intel® Extension for PyTorch* dispatches the operators into their underlying kernels automatically based on ISA that it detects and leverages vectorization and matrix acceleration units available on Intel hardware. Intel® Extension for PyTorch* runtime extension brings better efficiency with finer-grained thread runtime control and weight sharing. On GPU, optimized operators and kernels are implemented and registered through PyTorch dispatching mechanism. These operators and kernels are accelerated from native vectorization feature and matrix calculation feature of Intel GPU hardware. Intel® Extension for PyTorch* for GPU utilizes the `DPC++ <https://github.com/intel/llvm#oneapi-dpc-compiler>`_ compiler that supports the latest `SYCL* <https://registry.khronos.org/SYCL/specs/sycl-2020/html/sycl-2020.html>`_ standard and also a number of extensions to the SYCL* standard, which can be found in the `sycl/doc/extensions <https://github.com/intel/llvm/tree/sycl/sycl/doc/extensions>`_ directory.
 
+.. note:: GPU features are not included in CPU only packages.
 
-Support
--------
-The team tracks bugs and enhancement requests using `GitHub issues <https://github.com/intel/intel-extension-for-pytorch/issues/>`_. Before submitting a suggestion or bug report, search the existing GitHub issues to see if your issue has already been reported.
+Intel® Extension for PyTorch* has been released as an open–source project at `Github <https://github.com/intel/intel-extension-for-pytorch>`_. Source code is available at `xpu-master branch <https://github.com/intel/intel-extension-for-pytorch/tree/xpu-master>`_. Check `the tutorial <https://intel.github.io/intel-extension-for-pytorch/xpu/latest/>`_ for detailed information. Due to different development schedule, optimizations for CPU only might have a newer code base. Source code is available at `master branch <https://github.com/intel/intel-extension-for-pytorch/tree/master>`_. Check `the CPU tutorial <https://intel.github.io/intel-extension-for-pytorch/cpu/latest/>`_ for detailed information on the CPU side.
 
 .. toctree::
-   :caption: ABOUT
-   :maxdepth: 3
    :hidden:
+   :maxdepth: 1
 
-   tutorials/introduction
+   tutorials/getting_started
    tutorials/features
-   Large Language Models (LLM)<tutorials/llm>
-   tutorials/performance
    tutorials/releases
-   tutorials/known_issues
-   tutorials/blogs_publications
-   tutorials/license
-
-.. toctree::
-   :maxdepth: 3
-   :caption: GET STARTED
-   :hidden:
-
    tutorials/installation
-   tutorials/getting_started
    tutorials/examples
-   tutorials/cheat_sheet
-
-.. toctree::
-   :maxdepth: 3
-   :caption: DEVELOPER REFERENCE
-   :hidden:
-
    tutorials/api_doc
-   
-.. toctree::
-   :maxdepth: 3
-   :caption: PERFORMANCE TUNING
-   :hidden:
-
-   tutorials/performance_tuning/tuning_guide
-   tutorials/performance_tuning/launch_script
-   tutorials/performance_tuning/torchserve   
-   
-.. toctree::
-   :maxdepth: 3
-   :caption: CONTRIBUTING GUIDE
-   :hidden:
-
+   tutorials/performance_tuning
+   tutorials/performance
+   tutorials/blogs_publications
    tutorials/contribution
-
+   tutorials/license
diff --git a/docs/tutorials/api_doc.rst b/docs/tutorials/api_doc.rst
index 82586c534..707bc1945 100644
--- a/docs/tutorials/api_doc.rst
+++ b/docs/tutorials/api_doc.rst
@@ -6,7 +6,6 @@ General
 
 .. currentmodule:: intel_extension_for_pytorch
 .. autofunction:: optimize
-.. autofunction:: optimize_transformers
 .. autoclass:: verbose
 
 Fast Bert (Experimental)
@@ -25,7 +24,6 @@ Quantization
 ************
 
 .. automodule:: intel_extension_for_pytorch.quantization
-.. autofunction:: get_smooth_quant_qconfig_mapping
 .. autofunction:: prepare
 .. autofunction:: convert
 
diff --git a/docs/tutorials/blogs_publications.md b/docs/tutorials/blogs_publications.md
index 8a60f8ff3..1d900ec1f 100644
--- a/docs/tutorials/blogs_publications.md
+++ b/docs/tutorials/blogs_publications.md
@@ -1,8 +1,6 @@
 Blogs & Publications
 ====================
 
-* [Accelerate Llama 2 with Intel AI Hardware and Software Optimizations, Jul 2023](https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html)
-* [Accelerate PyTorch\* Training and Inference Performance using Intel® AMX, Jul 2023](https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-pytorch-training-inference-on-amx.html)
 * [Intel® Deep Learning Boost (Intel® DL Boost) - Improve Inference Performance of Hugging Face BERT Base Model in Google Cloud Platform (GCP) Technology Guide, Apr 2023](https://networkbuilders.intel.com/solutionslibrary/intel-deep-learning-boost-intel-dl-boost-improve-inference-performance-of-hugging-face-bert-base-model-in-google-cloud-platform-gcp-technology-guide)
 * [Get Started with Intel® Extension for PyTorch\* on GPU | Intel Software, Mar 2023](https://www.youtube.com/watch?v=Id-rE2Q7xZ0&t=1s)
 * [Accelerate PyTorch\* INT8 Inference with New “X86” Quantization Backend on X86 CPUs, Mar 2023](https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-pytorch-int8-inf-with-new-x86-backend.html)
diff --git a/docs/tutorials/contribution.md b/docs/tutorials/contribution.md
index 94c7bed35..b52f6f9d5 100644
--- a/docs/tutorials/contribution.md
+++ b/docs/tutorials/contribution.md
@@ -197,4 +197,4 @@ To build the documentation:
 
 #### Tips
 
-The `.rst` source files live in [docs/tutorials](https://github.com/intel/intel-extension-for-pytorch/tree/main/docs/tutorials). Some of the `.rst` files pull in docstrings from Intel® Extension for PyTorch\* Python code (for example, via the `autofunction` or `autoclass` directives). To shorten doc build times, it is helpful to remove the files you are not working on, only keeping the base `index.rst` file and the files you are editing. The Sphinx build will produce missing file warnings but will still complete.
+The `.rst` source files live in [docs/tutorials](https://github.com/intel/intel-extension-for-pytorch/tree/master/docs/tutorials). Some of the `.rst` files pull in docstrings from Intel® Extension for PyTorch\* Python code (for example, via the `autofunction` or `autoclass` directives). To shorten doc build times, it is helpful to remove the files you are not working on, only keeping the base `index.rst` file and the files you are editing. The Sphinx build will produce missing file warnings but will still complete.
diff --git a/docs/tutorials/examples.md b/docs/tutorials/examples.md
index 77abf3f21..0f49b1ab5 100644
--- a/docs/tutorials/examples.md
+++ b/docs/tutorials/examples.md
@@ -1,40 +1,20 @@
 Examples
 ========
 
-These examples will guide you through using the Intel® Extension for PyTorch\* on Intel CPUs.
+**_NOTE:_** Check individual feature page for examples of feature usage. All features are listed in the [feature page](./features.rst).
 
-You can also refer to the [Features](./features.rst) section to get the examples and usage instructions related to particular features.
+**_NOTE:_** Feature examples and examples below are available at Github source tree, under `examples` directory.
 
-The source code for these examples, as well as the feature examples, can be found in the GitHub source tree under the `examples` directory.
+## Training
 
-- [Python](#python) examples demonstrate usage of Python APIs:
+### Single-instance Training
 
-  - [Training](#training)
-  - [Inference](#inference)
+#### Code Changes Highlight
 
-- [C++](#c) examples demonstrate usage of C++ APIs
-- [Intel® AI Reference Models](#intel-ai-reference-models) provide out-of-the-box use cases, demonstrating the performance benefits achievable with Intel Extension for PyTorch\*
+There is only a line of code change required to use Intel® Extension for PyTorch\* on training, as shown:
+1. `ipex.optimize` function applies optimizations against the model object, as well as an optimizer object.
 
-**Prerequisites**:
-Before running these examples, please note the following:
-
-- To run the examples, install the `torchvision` and `transformers` Python packages.
-- Examples using the BFloat16 data type require machines with the  Intel® Advanced Vector Extensions 512 (Intel® AVX-512) BF16 and Intel® Advanced Matrix Extensions (Intel® AMX) BF16 instruction sets.
-
-
-## Python
-
-### Training
-
-#### Single-instance Training
-
-To use Intel® Extension for PyTorch\* on training, you need to make the following changes in your code:
-
-1. Import `intel_extension_for_pytorch` as `ipex`.
-2. Invoke the `ipex.optimize` function to apply optimizations against the model and optimizer objects, as shown below:
-
-
-```python
+```
 ...
 import torch
 import intel_extension_for_pytorch as ipex
@@ -47,34 +27,30 @@ model.train()
 model, optimizer = ipex.optimize(model, optimizer=optimizer)
 # For BFloat16
 model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)
-# Invoke the code below to enable experimental feature torch.compile
-model = torch.compile(model, backend="ipex")
 ...
 optimizer.zero_grad()
 output = model(data)
 ...
 ```
 
-Below you can find complete code examples demonstrating how to use the extension on training for different data types:
-
-##### Float32
+#### Complete - Float32
 
 [//]: # (marker_train_single_fp32_complete)
 [//]: # (marker_train_single_fp32_complete)
 
-##### BFloat16
+#### Complete - BFloat16
 
 [//]: # (marker_train_single_bf16_complete)
 [//]: # (marker_train_single_bf16_complete)
 
-#### Distributed Training
+### Distributed Training
 
-Distributed training with PyTorch DDP is accelerated by oneAPI Collective Communications Library Bindings for Pytorch\* (oneCCL Bindings for Pytorch\*). The extension supports FP32 and BF16 data types. More detailed information and examples are available at the [Github repo](https://github.com/intel/torch-ccl).
+Distributed training with PyTorch DDP is accelerated by oneAPI Collective Communications Library Bindings for Pytorch\* (oneCCL Bindings for Pytorch\*). The extension supports FP32 and BF16 data types. More detailed information and examples are available at its [Github repo](https://github.com/intel/torch-ccl).
 
 **Note:** When performing distributed training with BF16 data type, use oneCCL Bindings for Pytorch\*. Due to a PyTorch limitation, distributed training with BF16 data type with Intel® Extension for PyTorch\* is not supported.
 
 [//]: # (marker_train_ddp_complete)
-```python
+```
 import os
 import torch
 import torch.distributed as dist
@@ -133,107 +109,93 @@ torch.save({
 ```
 [//]: # (marker_train_ddp_complete)
 
-### Inference
+## Inference
 
 The `optimize` function of Intel® Extension for PyTorch\* applies optimizations to the model, bringing additional performance boosts. For both computer vision workloads and NLP workloads, we recommend applying the `optimize` function against the model object.
 
-#### Float32
+### Float32
 
-##### Imperative Mode
+#### Imperative Mode
 
-###### Resnet50
+##### Resnet50
 
 [//]: # (marker_inf_rn50_imp_fp32)
 [//]: # (marker_inf_rn50_imp_fp32)
 
-###### BERT
+##### BERT
 
 [//]: # (marker_inf_bert_imp_fp32)
 [//]: # (marker_inf_bert_imp_fp32)
 
-##### TorchScript Mode
+#### TorchScript Mode
 
-We recommend using Intel® Extension for PyTorch\* with [TorchScript](https://pytorch.org/docs/stable/jit.html) for further optimizations.
+We recommend you take advantage of Intel® Extension for PyTorch\* with [TorchScript](https://pytorch.org/docs/stable/jit.html) for further optimizations.
 
-###### Resnet50
+##### Resnet50
 
 [//]: # (marker_inf_rn50_ts_fp32)
 [//]: # (marker_inf_rn50_ts_fp32)
 
-###### BERT
+##### BERT
 
 [//]: # (marker_inf_bert_ts_fp32)
 [//]: # (marker_inf_bert_ts_fp32)
 
-##### TorchDynamo Mode (Experimental, _NEW feature from 2.0.0_)
+#### TorchDynamo Mode (Experimental, _NEW feature from 2.0.0_)
 
-###### Resnet50
+##### Resnet50
 
 [//]: # (marker_inf_rn50_dynamo_fp32)
 [//]: # (marker_inf_rn50_dynamo_fp32)
 
-###### BERT
+##### BERT
 
 [//]: # (marker_inf_bert_dynamo_fp32)
 [//]: # (marker_inf_bert_dynamo_fp32)
 
-*Note:* In TorchDynamo mode, since the native PyTorch operators like `aten::convolution` and `aten::linear` are well supported and optimized in `ipex` backend, we need to disable weights prepacking by setting `weights_prepack=False` in `ipex.optimize()`.
+### BFloat16
 
-#### BFloat16
-
-The `optimize` function works for both Float32 and BFloat16 data type. For BFloat16 data type, set the `dtype` parameter to `torch.bfloat16`.
+Similar to running with FP32, the `optimize` function also works for BFloat16 data type. The only difference is setting `dtype` parameter to `torch.bfloat16`.
 We recommend using Auto Mixed Precision (AMP) with BFloat16 data type.
 
-##### Imperative Mode
+#### Imperative Mode
 
-###### Resnet50
+##### Resnet50
 
 [//]: # (marker_inf_rn50_imp_bf16)
 [//]: # (marker_inf_rn50_imp_bf16)
 
-###### BERT
+##### BERT
 
 [//]: # (marker_inf_bert_imp_bf16)
 [//]: # (marker_inf_bert_imp_bf16)
 
-##### TorchScript Mode
+#### TorchScript Mode
 
-We recommend using Intel® Extension for PyTorch\* with [TorchScript](https://pytorch.org/docs/stable/jit.html) for further optimizations.
+We recommend you take advantage of Intel® Extension for PyTorch\* with [TorchScript](https://pytorch.org/docs/stable/jit.html) for further optimizations.
 
-###### Resnet50
+##### Resnet50
 
 [//]: # (marker_inf_rn50_ts_bf16)
 [//]: # (marker_inf_rn50_ts_bf16)
 
-###### BERT
+##### BERT
 
 [//]: # (marker_inf_bert_ts_bf16)
 [//]: # (marker_inf_bert_ts_bf16)
 
-##### TorchDynamo Mode (Experimental, _NEW feature from 2.0.0_)
-
-###### Resnet50
-
-[//]: # (marker_inf_rn50_dynamo_bf16)
-[//]: # (marker_inf_rn50_dynamo_bf16)
-
-###### BERT
-
-[//]: # (marker_inf_bert_dynamo_bf16)
-[//]: # (marker_inf_bert_dynamo_bf16)
-
-#### Fast Bert (*Experimental*)
+### Fast Bert (*Experimental*)
 
 [//]: # (marker_inf_bert_fast_bf16)
 [//]: # (marker_inf_bert_fast_bf16)
 
-#### INT8
+### INT8
 
 Starting from Intel® Extension for PyTorch\* 1.12.0, quantization feature supports both static and dynamic modes.
 
-##### Calibration
+#### Calibration
 
-###### Static Quantization
+##### Static Quantization
 
 Please follow the steps below to perform static calibration:
 
@@ -249,7 +211,7 @@ Please follow the steps below to perform static calibration:
 [//]: # (marker_int8_static)
 [//]: # (marker_int8_static)
 
-###### Dynamic Quantization
+##### Dynamic Quantization
 
 Please follow the steps below to perform static calibration:
 
@@ -264,7 +226,7 @@ Please follow the steps below to perform static calibration:
 [//]: # (marker_int8_dynamic)
 [//]: # (marker_int8_dynamic)
 
-##### Deployment
+#### Deployment
 
 For deployment, the INT8 model is loaded from the local file and can be used directly on the inference.
 
@@ -311,8 +273,8 @@ If *Found IPEX* is shown as with a dynamic library path, the extension had been
 
 ```bash
 $ cmake -DCMAKE_PREFIX_PATH=/workspace/libtorch ..
--- The C compiler identification is GNU XX.X.X
--- The CXX compiler identification is GNU XX.X.X
+-- The C compiler identification is GNU 11.2.1
+-- The CXX compiler identification is GNU 11.2.1
 -- Detecting C compiler ABI info
 -- Detecting C compiler ABI info - done
 -- Check for working C compiler: /usr/bin/cc - skipped
@@ -348,7 +310,6 @@ $ ldd example-app
         ...
 ```
 
-## Intel® AI Reference Models
-
-Use cases that have already been optimized by Intel engineers are available at [Intel® AI Reference Models](https://github.com/IntelAI/models/tree/pytorch-r2.1.100-models) (former Model Zoo). A number of PyTorch use cases for benchmarking are also available in the [benchmarks](https://github.com/IntelAI/models/tree/pytorch-r2.1.100-models/benchmarks#pytorch-use-cases). You can get performance benefits out-of-the-box by simply running scripts in the Intel® AI Reference Models.
+## Model Zoo
 
+Use cases that had already been optimized by Intel engineers are available at [Model Zoo for Intel® Architecture](https://github.com/IntelAI/models/tree/pytorch-r2.0.100-models). A bunch of PyTorch use cases for benchmarking are also available on the [GitHub page](https://github.com/IntelAI/models/tree/pytorch-r2.0.100-models/benchmarks#pytorch-use-cases). You can get performance benefits out-of-box by simply running scipts in the Model Zoo.
diff --git a/docs/tutorials/features.rst b/docs/tutorials/features.rst
index dee7fd8ec..062750ea6 100644
--- a/docs/tutorials/features.rst
+++ b/docs/tutorials/features.rst
@@ -1,14 +1,12 @@
 Features
 ========
 
-This section provides a detailed overview of supported features.
-
-Easy-to-use Python API
+Ease-of-use Python API
 ----------------------
 
 With only two or three clauses added to your original code, Intel® Extension for PyTorch\* provides simple frontend Python APIs and utilities to get performance optimizations such as graph optimization and operator optimization.
 
-Check the `API Documentation`_ for API functions description and `Examples <examples.md>`_ for usage guidance.
+Check the `API Documentation`_ for details of API functions. `Examples <examples.md>`_ are also available.
 
 .. note::
 
@@ -17,44 +15,30 @@ Check the `API Documentation`_ for API functions description and `Examples <exam
   ``intel_extension_for_pytorch`` (for versions 1.10.0 and later). Use the
   correct package name depending on the version you are using.
 
-
-Large Language Models (LLM, *NEW feature from 2.1.0*)
------------------------------------------------------
-
-In the current technological landscape, Generative AI (GenAI) workloads and models have gained widespread attention and popularity. Large Language Models (LLMs) have emerged as the dominant models driving these GenAI applications. Starting from 2.1.0, specific optimizations for certain LLM models are 
-introduced in the Intel® Extension for PyTorch*.
-
-For more detailed information, check `LLM Optimizations Overview <./llm.html>`_.
+Here are detailed discussions of specific feature topics, summarized in the rest
+of this document:
 
 torch.compile (Experimental, *NEW feature from 2.0.0*)
 ------------------------------------------------------
 
-PyTorch* 2.0 introduces a new feature ``torch.compile`` to speed up PyTorch* code. It makes PyTorch code run faster by JIT-compiling of PyTorch code into optimized kernels. Intel® Extension for PyTorch\* enables a backend, ``ipex``, in the ``torch.compile`` to optimize generation of the graph model.
-
-To use the feature, import the Intel® Extension for PyTorch* and set the backend parameter of the ``torch.compile`` to ``ipex``.
+PyTorch* 2.0 introduces a new feature, `torch.compile`, to speed up PyTorch* code. It makes PyTorch code run faster by JIT-compiling PyTorch code into optimized kernels, all while requiring minimal code changes. Intel® Extension for PyTorch\* enables a backend, `ipex`, in the `torch.compile` to optimize generation of the graph model.
 
-With ``torch.compile`` backend set to ``ipex``, the following will happen:
-
-1. Register Intel® Extension for PyTorch\* operators to Inductor.
-2. Custom fusions at FX graph level, e.g., the migration of existing TorchScript-based fusion kernels in IPEX to inductor, pattern-based fusions to achieve peak performance.
-
-While optimizations with ``torch.compile`` apply to backend, invocation of the ``ipex.optimize`` function is highly recommended as well to apply optimizations in frontend.
+Usage is as simple as importing Intel® Extension for PyTorch\* and setting `backend` parameter of the `torch.compile` to `ipex`. While optimizations with `torch.compile` applies to backend, invocation of `ipex.optimize` function is highly recommended as well to apply optimizations in frontend.
 
 .. code-block:: python
 
    import torch
    import intel_extension_for_pytorch as ipex
    ...
-   model = ipex.optimize(model, weights_prepack=False)
+   model = ipex.optimize(model)
    model = torch.compile(model, backend='ipex')
-   ...
 
 ISA Dynamic Dispatching
 -----------------------
 
 Intel® Extension for PyTorch\* features dynamic dispatching functionality to automatically adapt execution binaries to the most advanced instruction set available on your machine.
 
-For details, refer to `ISA Dynamic Dispatching <features/isa_dynamic_dispatch.md>`_.
+For more detailed information, check `ISA Dynamic Dispatching <features/isa_dynamic_dispatch.md>`_.
 
 .. toctree::
    :hidden:
@@ -67,7 +51,7 @@ Auto Channels Last
 
 Comparing to the default NCHW memory format, using channels_last (NHWC) memory format could further accelerate convolutional neural networks. In Intel® Extension for PyTorch*, NHWC memory format has been enabled for most key CPU operators. More detailed information is available at `Channels Last <features/nhwc.md>`_.
 
-Intel® Extension for PyTorch* automatically converts a model to channels last memory format when users optimize the model with ``ipex.optimize(model)``. With this feature, there is no need to manually apply ``model=model.to(memory_format=torch.channels_last)`` anymore. More detailed information is available at `Auto Channels Last <features/auto_channels_last.md>`_.
+Intel® Extension for PyTorch* automatically converts a model to channels last memory format when users optimize the model with `ipex.optimize(model)`. With this feature users won't need to manually apply `model=model.to(memory_format=torch.channels_last)` any more. More detailed information is available at `Auto Channels Last <features/auto_channels_last.md>`_.
 
 .. toctree::
    :hidden:
@@ -81,9 +65,7 @@ Auto Mixed Precision (AMP)
 
 Low precision data type BFloat16 has been natively supported on 3rd Generation Xeon® Scalable Processors (aka Cooper Lake) with AVX512 instruction set. It will also be supported on the next generation of Intel® Xeon® Scalable Processors with Intel® Advanced Matrix Extensions (Intel® AMX) instruction set providing further boosted performance. The support of Auto Mixed Precision (AMP) with BFloat16 for CPU and BFloat16 optimization of operators has been enabled in Intel® Extension for PyTorch\*, and partially upstreamed to PyTorch master branch. These optimizations will be landed in PyTorch master through PRs that are being submitted and reviewed.
 
-Prefer to use `torch.cpu.amp.autocast()` instead of `torch.autocast(device_name="cpu")`.
-
-For details, refer to `Auto Mixed Precision (AMP) <features/amp.md>`_.
+For more detailed information, check `Auto Mixed Precision (AMP) <features/amp.md>`_.
 
 Bfloat16 computation can be conducted on platforms with AVX512 instruction set. On platforms with `AVX512 BFloat16 instruction <https://www.intel.com/content/www/us/en/developer/articles/technical/intel-deep-learning-boost-new-instruction-bfloat16.html>`_, there will be an additional performance boost.
 
@@ -99,7 +81,7 @@ Graph Optimization
 To further optimize TorchScript performance, Intel® Extension for PyTorch\* supports transparent fusion of frequently used operator patterns such as Conv2D+ReLU and Linear+ReLU.
 For more detailed information, check `Graph Optimization <features/graph_optimization.md>`_.
 
-Compared to eager mode, graph mode in PyTorch normally yields better performance from optimization methodologies such as operator fusion. Intel® Extension for PyTorch* provides further optimizations in graph mode. We recommend taking advantage of Intel® Extension for PyTorch* with `TorchScript <https://pytorch.org/docs/stable/jit.html>`_. You may wish to run with the ``torch.jit.trace()`` function first, since it generally works better with Intel® Extension for PyTorch* than using the ``torch.jit.script()`` function. More detailed information can be found at the `pytorch.org website <https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#tracing-modules>`_.
+Compared to eager mode, graph mode in PyTorch normally yields better performance from optimization methodologies such as operator fusion. Intel® Extension for PyTorch* provides further optimizations in graph mode. We recommend you take advantage of Intel® Extension for PyTorch* with `TorchScript <https://pytorch.org/docs/stable/jit.html>`_. You may wish to run with the `torch.jit.trace()` function first, since it generally works better with Intel® Extension for PyTorch* than using the `torch.jit.script()` function. More detailed information can be found at the `pytorch.org website <https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#tracing-modules>`_.
 
 .. toctree::
    :hidden:
@@ -122,19 +104,18 @@ Intel® Extension for PyTorch* also optimizes operators and implements several c
 .. autoclass:: MergedEmbeddingBag
 .. autoclass:: MergedEmbeddingBagWithSGD
 
-**Auto kernel selection** is a feature that enables users to tune for better performance with GEMM operations. We aim to provide good default performance by leveraging the best of math libraries and enabling `weights_prepack`. The feature was tested with broad set of models. If you want to try other options, you can use `auto_kernel_selection` toggle in `ipex.optimize()` to switch, and you can disable `weights_prepack` in `ipex.optimize()` if you are more concerned about the memory footprint than performance gain. However, in most cases, we recommend sticking with the default settings for the best experience.
-
+**Auto kernel selection** is a feature that enables users to tune for better performance with GEMM operations. It is provided as parameter –auto_kernel_selection, with boolean value, of the ipex.optimize() function. By default, the GEMM kernel is computed with oneMKL primitives. However, under certain circumstances oneDNN primitives run faster. Users are able to set –auto_kernel_selection to True to run GEMM kernels with oneDNN primitives.” -> "We aim to provide good default performance by leveraging the best of math libraries and enabled weights_prepack, and it has been verified with broad set of models. If you would like to try other alternatives, you can use auto_kernel_selection toggle in ipex.optimize to switch, and you can disable weights_preack in ipex.optimize if you are concerning the memory footprint more than performance gain. However in majority cases, keeping default is what we recommend.
 
 Optimizer Optimization
 ----------------------
 
 Optimizers are one of key parts of the training workloads. Intel® Extension for PyTorch* brings two types of optimizations to optimizers:
 
-1. Operator fusion for the computation in the optimizers.
-2. SplitSGD for BF16 training, which reduces the memory footprint of the master weights by half.
+1.	Operator fusion for the computation in the optimizers.
+2.	SplitSGD for BF16 training, which reduces the memory footprint of the master weights by half.
 
 
-For details, refer to `Optimizer Fusion <features/optimizer_fusion.md>`_ and `Split SGD <features/split_sgd.html>`_ 
+For more detailed information, check `Optimizer Fusion <features/optimizer_fusion.md>`_ and `Split SGD <features/split_sgd.html>`_ 
 
 .. toctree::
    :hidden:
@@ -194,7 +175,7 @@ For more detailed information, check `Codeless Optimization <features/codeless_o
 Graph Capture (Experimental, *NEW feature from 1.13.0*)
 -------------------------------------------------------
 
-Since graph mode is key for deployment performance, this feature automatically captures graphs based on set of technologies that PyTorch supports, such as TorchScript and TorchDynamo. Users won't need to learn and try different PyTorch APIs to capture graphs, instead, they can turn on a new boolean flag `--graph_mode` (default off) in `ipex.optimize()` to get the best of graph optimization.
+Since graph mode is key for deployment performance, this feature automatically captures graphs based on set of technologies that PyTorch supports, such as TorchScript and TorchDynamo. Users won't need to learn and try different PyTorch APIs to capture graphs, instead, they can turn on a new boolean flag `--graph_mode` (default off) in `ipex.optimize` to get the best of graph optimization.
 
 For more detailed information, check `Graph Capture <features/graph_capture.md>`_.
 
@@ -220,7 +201,7 @@ For more detailed information, check `HyperTune <features/hypertune.md>`_.
 Fast BERT Optimization (Experimental, *NEW feature from 2.0.0*)
 ---------------------------------------------------------------
 
-Intel proposed a technique to speed up BERT workloads. Implementation is integrated into Intel® Extension for PyTorch\*. An API `ipex.fast_bert()` is provided for a simple usage.
+Intel proposed a technique to speed up BERT workloads. Implementation is integrated into Intel® Extension for PyTorch\*. An API `ipex.fast_bert` is provided for a simple usage.
 
 For more detailed information, check `Fast BERT <features/fast_bert.md>`_.
 
diff --git a/docs/tutorials/features/amp.md b/docs/tutorials/features/amp.md
index 610291dab..879816589 100644
--- a/docs/tutorials/features/amp.md
+++ b/docs/tutorials/features/amp.md
@@ -7,8 +7,6 @@ Auto Mixed Precision (AMP)
 
 `torch.cpu.amp` only supports `torch.bfloat16`. It is the default lower precision floating point data type when `torch.cpu.amp` is enabled. `torch.cpu.amp` primarily benefits when running on Intel CPU with BFloat16 instruction set support.
 
-Prefer to use `torch.cpu.amp.autocast()` instead of `torch.autocast(device_name="cpu")`.
-
 ## Use Case
 
 The following simple network should show a speedup with mixed precision.
diff --git a/docs/tutorials/features/fast_bert.md b/docs/tutorials/features/fast_bert.md
index b62647133..f84bbcc74 100644
--- a/docs/tutorials/features/fast_bert.md
+++ b/docs/tutorials/features/fast_bert.md
@@ -9,7 +9,7 @@ The Implementation is integrated into Intel® Extension for PyTorch\*. BERT coul
 
 ### Prerequisite
 
-- Transformers 4.6.0 ~ 4.31.0
+- Transformers 4.6.0 ~ 4.20.0
 
 ### Usage Example
 
diff --git a/docs/tutorials/getting_started.md b/docs/tutorials/getting_started.md
index 3ab91a0ca..09f7de64b 100644
--- a/docs/tutorials/getting_started.md
+++ b/docs/tutorials/getting_started.md
@@ -1,19 +1,36 @@
-# Quick Start
+# Getting Started
 
-The following instructions assume you have installed the Intel® Extension for PyTorch\*. For installation instructions, refer to [Installation](../../../index.html#installation?platform=cpu&version=v2.1.0%2Bcpu).
+## Installation
 
-To start using the Intel® Extension for PyTorch\* in your code, you need to make the following changes:
+Prebuilt wheel files are released for multiple Python versions. You can install them simply with the following pip command.
 
-1. Import the extension with `import intel_extension_for_pytorch as ipex`.
-2. Invoke the `optimize()` function to apply optimizations.
-3. Convert the imperative model to a graph model.
-    - For TorchScript, invoke `torch.jit.trace()` and `torch.jit.freeze()`
-    - For TorchDynamo, invoke `torch.compile(model, backend="ipex")`(*Experimental feature*)
+```bash
+python -m pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu
+python -m pip install intel_extension_for_pytorch
+```
+
+You can run a simple sanity test to double confirm if the correct version is installed, and if the software stack can get correct hardware information onboard your system.
+
+```bash
+python -c "import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__);"
+```
+
+More detailed instructions can be found at [Installation Guide](./installation.md).
 
-**Important:** It is highly recommended to `import intel_extension_for_pytorch` right after `import torch`, prior to importing other packages.
 
-The example below demostrates how to use the Intel® Extension for PyTorch\*:
+## Coding
 
+Intel® Extension for PyTorch\* doesn't require complex code changes to get it working. Usage is as simple as several-line code change.
+
+In general, APIs invocation should follow orders below.
+
+1. `import intel_extension_for_pytorch as ipex`
+2. Invoke `optimize()` function to apply optimizations.
+3. Convert the imperative model to a graph model.
+    - For TorchScript, invoke `torch.jit.trace()` and `torch.jit.freeze()`.
+    - For TorchDynamo, invoke `torch.compile(model, backend="ipex")`. (*Experimental feature*, FP32 ONLY)
+
+**Note:** It is highly recommended to `import intel_extension_for_pytorch` right after `import torch`, prior to importing other packages.
 
 ```python
 import torch
@@ -35,7 +52,7 @@ with torch.no_grad(), torch.cpu.amp.autocast():
 ##########################################
 
 ############## TorchDynamo ###############
-model = ipex.optimize(model, weights_prepack=False)
+model = ipex.optimize(model)
 
 model = torch.compile(model, backend="ipex")
 with torch.no_grad():
@@ -43,6 +60,4 @@ with torch.no_grad():
 ##########################################
 ```
 
-More examples, including training and usage of low precision data types are available in the [Examples](./examples.md) section.
-
-In [Cheat Sheet](cheat_sheet.md), you can find more commands that can help you start using the Intel® Extension for PyTorch\*.
+More examples, including training and usage of low precision data types are available at [Examples](./examples.md).
diff --git a/docs/tutorials/installation.md b/docs/tutorials/installation.md
index 4fc3bb0ce..33f84fbb2 100644
--- a/docs/tutorials/installation.md
+++ b/docs/tutorials/installation.md
@@ -1,8 +1,179 @@
-Installation
-============
+Installation Guide
+==================
 
-Select your preferences and follow the installation instructions provided on the [Installation page](../../../index.html#installation?platform=cpu&version=v2.1.0%2Bcpu).
+## System Requirements
 
-After successful installation, refer to the [Quick Start](getting_started.md) and [Examples](examples.md) sections to start using the extension in your code.
+|Category|Content|
+|--|--|
+|Compiler|Recommend using GCC 11|
+|Operating System|CentOS 7, RHEL 8, Rocky Linux 8.5, Ubuntu newer than 18.04|
+|Python|See prebuilt wheel files availability matrix below|
 
-**NOTE:** For detailed instructions on installing and setting up the environment for Large Language Models (LLM), as well as example scripts, refer to the [LLM best practices](https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.0%2Bcpu/examples/cpu/inference/python/llm).
+* Intel® Extension for PyTorch\* is functional on systems with AVX2 instruction set support (such as Intel® Core™ Processor Family and Intel® Xeon® Processor formerly Broadwell). However, it is highly recommended to run on systems with AVX-512 and above instructions support for optimal performance (such as Intel® Xeon® Scalable Processors).
+
+## Install PyTorch
+
+Make sure PyTorch is installed so that the extension will work properly. For each PyTorch release, we have a corresponding release of the extension. Here are the PyTorch versions that we support and the mapping relationship:
+
+|PyTorch Version|Extension Version|
+|--|--|
+|[v2.0.\*](https://github.com/pytorch/pytorch/tree/v2.0.1 "v2.0.1")|[v2.0.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v2.0.100+cpu)|
+|[v1.13.\*](https://github.com/pytorch/pytorch/tree/v1.13.0 "v1.13.0")|[v1.13.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v1.13.100+cpu)|
+|[v1.12.\*](https://github.com/pytorch/pytorch/tree/v1.12.0 "v1.12.0")|[v1.12.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v1.12.300)|
+|[v1.11.\*](https://github.com/pytorch/pytorch/tree/v1.11.0 "v1.11.0")|[v1.11.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v1.11.200)|
+|[v1.10.\*](https://github.com/pytorch/pytorch/tree/v1.10.0 "v1.10.0")|[v1.10.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v1.10.100)|
+|[v1.9.0](https://github.com/pytorch/pytorch/tree/v1.9.0 "v1.9.0")|[v1.9.0](https://github.com/intel/intel-extension-for-pytorch/tree/v1.9.0)|
+|[v1.8.0](https://github.com/pytorch/pytorch/tree/v1.8.0 "v1.8.0")|[v1.8.0](https://github.com/intel/intel-extension-for-pytorch/tree/v1.8.0)|
+|[v1.7.0](https://github.com/pytorch/pytorch/tree/v1.7.0 "v1.7.0")|[v1.2.0](https://github.com/intel/intel-extension-for-pytorch/tree/v1.2.0)|
+|[v1.5.0-rc3](https://github.com/pytorch/pytorch/tree/v1.5.0-rc3 "v1.5.0-rc3")|[v1.1.0](https://github.com/intel/intel-extension-for-pytorch/tree/v1.1.0)|
+|[v1.5.0-rc3](https://github.com/pytorch/pytorch/tree/v1.5.0-rc3 "v1.5.0-rc3")|[v1.0.2](https://github.com/intel/intel-extension-for-pytorch/tree/v1.0.2)|
+|[v1.5.0-rc3](https://github.com/pytorch/pytorch/tree/v1.5.0-rc3 "v1.5.0-rc3")|[v1.0.1](https://github.com/intel/intel-extension-for-pytorch/tree/v1.0.1)|
+|[v1.5.0-rc3](https://github.com/pytorch/pytorch/tree/v1.5.0-rc3 "v1.5.0-rc3")|[v1.0.0](https://github.com/intel/intel-extension-for-pytorch/tree/v1.0.0)|
+
+Please install CPU version of PyTorch through its official channel. For more details, refer to [pytorch.org](https://pytorch.org/get-started/locally/).
+
+---
+
+**Note:**
+
+For the extension version earlier than 1.8.0, a patch has to be manually applied to PyTorch source code. Check that version's installation guide.
+
+From 1.8.0, compiling PyTorch from source is not required. If you still want to compile PyTorch, follow these [installation instructions](https://github.com/pytorch/pytorch#installation). Make sure to check out the correct PyTorch version according to the table above.
+
+---
+
+## Install via wheel file
+
+Prebuilt wheel files availability matrix for Python versions
+
+| Extension Version | Python 3.6 | Python 3.7 | Python 3.8 | Python 3.9 | Python 3.10 | Python 3.11 |
+| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
+| 2.0.100 |  |  | ✔️ | ✔️ | ✔️ | ✔️ |
+| 2.0.0 |  |  | ✔️ | ✔️ | ✔️ | ✔️ |
+| 1.13.100 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.13.0 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.12.300 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.12.100 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.12.0 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.11.200 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.11.0 |  | ✔️ | ✔️ | ✔️ | ✔️ |  |
+| 1.10.100 | ✔️ | ✔️ | ✔️ | ✔️ |  |  |
+| 1.10.0 | ✔️ | ✔️ | ✔️ | ✔️ |  |  |
+| 1.9.0 | ✔️ | ✔️ | ✔️ | ✔️ |  |  |
+| 1.8.0 |  | ✔️ |  |  |  |  |
+
+**Note:** Intel® Extension for PyTorch\* has PyTorch version requirement. Check the mapping table above.
+
+Starting from 1.11.0, you can use normal pip command to install the package with the latest version.
+
+```
+python -m pip install intel_extension_for_pytorch
+```
+
+Alternatively, you can also install the latest version with the following commands:
+
+```
+python -m pip install intel_extension_for_pytorch -f https://developer.intel.com/ipex-whl-stable-cpu
+```
+
+**Note:** For versions before 1.10.0, use package name `torch_ipex`, rather than `intel_extension_for_pytorch`.
+
+**Note:** To install a history package with a specific version, run with the following command:
+
+```
+python -m pip install <package_name>==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu
+```
+
+## Install via source compilation
+
+To ensure a smooth compilation, a script is provided in the Github repo. If you would like to compile the binaries from source, it is highly recommended to utilize this script.
+
+```bash
+$ wget https://raw.githubusercontent.com/intel/intel-extension-for-pytorch/master/scripts/compile_bundle.sh
+$ bash compile_bundle.sh
+```
+
+**Note:** Recommend to use the `compile_bundle.sh` script in a clean docker container.
+
+**Note:** Use the `compile_bundle.sh` script under a `conda` environment.
+
+**Note:** Depends on what applications are available on your OS, you probably need to install some Linux commands, like `git`, etc. Installation of these Linux commands are not included in this script.
+
+**Note:** The `compile_bundle.sh` script downloads source code of llvm-project and Intel® Extension for PyTorch\* into individual folders in its directory. You can consider to create a specific folder to use this script. Wheel files will be generated under `dist` folder of each source code directory. Besides, compilation progress is dumped into a log file `build.log` in source code directory. The log file is helpful to identify errors occurred during compilation. Should any failure happened, after addressing the issue, you can simply run the `compile_bundle.sh` script again with the same command.
+
+```bash
+$ mkdir ipex_bundle
+$ cd ipex_bundle
+$ wget .../compile_bundle.sh
+$ bash compile_bundle.sh
+$ ls
+compile_bundle.sh  intel_extension_for_pytorch  llvm-project
+$ tree -L 3 .
+.
+├── intel_extension_for_pytorch
+│   ├── dist
+│   │   └── intel_extension_for_pytorch-....whl
+│   ├ build.log
+│   └ ...
+└── llvm-project
+    └ ...
+```
+
+## Install via Docker container
+
+### Build Docker container from Dockerfile
+
+Run the following commands to build the `pip` based deployment container:
+
+```console
+$ cd docker
+$ DOCKER_BUILDKIT=1 docker build -f Dockerfile.pip -t intel-extension-for-pytorch:pip .
+$ docker run --rm intel-extension-for-pytorch:pip python -c "import torch; import intel_extension_for_pytorch as ipex; print('torch:', torch.__version__,' ipex:',ipex.__version__)"
+```
+
+Run the following commands to build the `conda` based development container:
+
+```console
+$ cd docker
+$ DOCKER_BUILDKIT=1 docker build -f Dockerfile.conda -t intel-extension-for-pytorch:conda .
+$ docker run --rm intel-extension-for-pytorch:conda python -c "import torch; import intel_extension_for_pytorch as ipex; print('torch:', torch.__version__,' ipex:',ipex.__version__)"
+```
+
+### Get docker container from dockerhub
+
+Pre-built docker images are available at [DockerHub](https://hub.docker.com/r/intel/intel-optimized-pytorch/tags).
+
+Run the following command to pull the image to your local machine.
+
+```console
+docker pull intel/intel-optimized-pytorch:latest
+```
+
+## Install C++ SDK
+
+|Version|Pre-cxx11 ABI|cxx11 ABI|
+|--|--|--|
+| 2.0.100 | [libintel-ext-pt-2.0.100+cpu.run](https://intel-extension-for-pytorch.s3.amazonaws.com/libipex/cpu/libintel-ext-pt-2.0.100%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-2.0.100+cpu.run](https://intel-extension-for-pytorch.s3.amazonaws.com/libipex/cpu/libintel-ext-pt-cxx11-abi-2.0.100%2Bcpu.run) |
+| 2.0.0 | [libintel-ext-pt-2.0.0+cpu.run](https://intel-extension-for-pytorch.s3.amazonaws.com/libipex/cpu/libintel-ext-pt-2.0.0%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-2.0.0+cpu.run](https://intel-extension-for-pytorch.s3.amazonaws.com/libipex/cpu/libintel-ext-pt-cxx11-abi-2.0.0%2Bcpu.run) |
+| 1.13.100 | [libintel-ext-pt-1.13.100+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.13.100%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.13.100+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.13.100%2Bcpu.run) |
+| 1.13.0 | [libintel-ext-pt-1.13.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.13.0%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.13.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.13.0%2Bcpu.run) |
+| 1.12.300 | [libintel-ext-pt-1.12.300+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.12.300%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.12.300+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.12.300%2Bcpu.run) |
+| 1.12.100 | [libintel-ext-pt-1.12.100+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.12.100%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.12.100+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.12.100%2Bcpu.run) |
+| 1.12.0 | [libintel-ext-pt-1.12.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.12.0%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.12.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.12.0%2Bcpu.run) |
+| 1.11.200 | [libintel-ext-pt-1.11.200+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-shared-with-deps-1.11.200%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.11.200+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-shared-with-deps-1.11.200%2Bcpu.run) |
+| 1.11.0 | [libintel-ext-pt-1.11.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-1.11.0%2Bcpu.run) | [libintel-ext-pt-cxx11-abi-1.11.0+cpu.run](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libintel-ext-pt-cxx11-abi-1.11.0%2Bcpu.run) |
+| 1.10.100 | [libtorch-shared-with-deps-1.10.0%2Bcpu-intel-ext-pt-cpu-1.10.100.zip](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libtorch-shared-with-deps-1.10.0%2Bcpu-intel-ext-pt-cpu-1.10.100.zip) | [libtorch-cxx11-abi-shared-with-deps-1.10.0%2Bcpu-intel-ext-pt-cpu-1.10.100.zip](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/libtorch-cxx11-abi-shared-with-deps-1.10.0%2Bcpu-intel-ext-pt-cpu-1.10.100.zip) |
+| 1.10.0 | [intel-ext-pt-cpu-libtorch-shared-with-deps-1.10.0+cpu.zip](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/intel-ext-pt-cpu-libtorch-shared-with-deps-1.10.0%2Bcpu.zip) | [intel-ext-pt-cpu-libtorch-cxx11-abi-shared-with-deps-1.10.0+cpu.zip](https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/libipex/cpu/intel-ext-pt-cpu-libtorch-cxx11-abi-shared-with-deps-1.10.0%2Bcpu.zip) |
+
+**Usage:** For version newer than 1.11.0, download one run file above according to your scenario, run the following command to install it and follow the [C++ example](./examples.md#c).
+```
+bash <libintel-ext-pt-name>.run install <libtorch_path>
+```
+
+You can get full usage help message by running the run file alone, as the following command.
+
+```
+bash <libintel-ext-pt-name>.run
+```
+
+**Usage:** For version before 1.11.0, download one zip file above according to your scenario, unzip it and follow the [C++ example](./examples.md#c).
diff --git a/docs/tutorials/license.md b/docs/tutorials/license.md
index de2fc8838..3ea34ef6c 100644
--- a/docs/tutorials/license.md
+++ b/docs/tutorials/license.md
@@ -5,5 +5,5 @@ Intel® Extension for PyTorch\* is licensed under [Apache License Version 2.0](h
 
 Apache License Version 2.0:
 
-[Intel® Extension for PyTorch\* LICENSE](https://github.com/intel/intel-extension-for-pytorch/blob/main/LICENSE.txt)
+[Intel® Extension for PyTorch\* LICENSE](https://github.com/intel/intel-extension-for-pytorch/blob/master/LICENSE.txt)
 
diff --git a/docs/tutorials/performance.md b/docs/tutorials/performance.md
index b3754111b..f2ebf16c7 100644
--- a/docs/tutorials/performance.md
+++ b/docs/tutorials/performance.md
@@ -9,45 +9,6 @@ This page shows performance boost with Intel® Extension for PyTorch\* on severa
 
 Find the latest performance data for 4th gen Intel® Xeon® Scalable processors and 3rd gen Intel® Xeon® processors, including detailed hardware and software configurations, at [Intel® Developer Zone article](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/performance.html).
 
-## LLM Performance
-
-We benchmarked LLaMA2 7B, 13B, GPT-J 6B with test input token length set to 256 and 1024 respectively. The tests were carried out on AWS M7i and M6i instances. CPUs of M6i instances are 3rd Gen Intel® Xeon® Processors which do not have AMX instructions for BF16 computing acceleration, so we take FP32 precision for benchmarking instead of BF16 on M6i instances.
-
-![LLaMA2 7B Results](../../images/performance/m7i_m6i_comp_llama7b.png)
-
-![LLaMA2 13B Results](../../images/performance/m7i_m6i_comp_llama13b.png)
-
-![GPT-J 6B Results](../../images/performance/m7i_m6i_comp_gptj6b.png)
-
-The LLM inference performances on M7i and M6i instances are compared based on the above results. M7i, with the 4th Gen Xeon® processors, has a remarkable performance advantage over M6i with the 3rd Gen Xeon® processors.
-
-M7i performance boost ratio over M6i for non-quantized (BF16 or FP32) models:
-
-|            | Speedup | Throughput |
-|:----------:|:-------:|:----------:|
-|  LLaMA2 7B |  2.47x  |    2.62x   |
-| LLaMA2 13B |  2.57x  |    2.62x   |
-|  GPT-J 6B  |  2.58x  |    2.85x   |
-
-M7i performance boost ratio over M6i for INT8 quantized models:
-
-|            | Speedup | Throughput |
-|:----------:|:-------:|:----------:|
-|  LLaMA2 7B |  1.27x  |    1.38x   |
-| LLaMA2 13B |  1.27x  |    1.27x   |
-|  GPT-J 6B  |  1.29x  |    1.36x   |
-
-We can also conclude that **with a larger batch size the capacity of the model service can be improved at the cost of longer response latency for the individual sessions**. The following table exhibits that for INT8 quantized LLaMA2-7b model on M7i instances, input batch_size=8 would increase the total throughput by 6.47x compared with batch_size=1, whereas P90 token latency gets 1.26x longer.
-
-| Batch size | Decoder latency | Total tokens per sec |
-|:----------:|:---------------:|:--------------------:|
-|      1     |        39       |         26.32        |
-|      8     |        49       |        170.21        |
-|            |                 |                      |
-|***Ratio*** |      1.26x      |         6.47x        |
-
-*Note:* Measured by Intel on 17th Aug 2023; M7i.16xLarge, M6i.16xLarge instances in US-west-2. OS-Ubuntu 22.04-lts, kernel 6.20.0-1009-aws, SW: PyTorch* 2.1 and Intel® Extension for PyTorch* 2.1/llm_feature_branch.
-
 ## INT8 with v1.11
 
 ### Performance Numbers
diff --git a/docs/tutorials/performance_tuning.rst b/docs/tutorials/performance_tuning.rst
new file mode 100644
index 000000000..b2f0aa99b
--- /dev/null
+++ b/docs/tutorials/performance_tuning.rst
@@ -0,0 +1,17 @@
+Performance Tuning Guide
+========================
+
+Intel® Extension for PyTorch\* should yield a satisfying performance with its default configuration for general use cases. To squeeze usage of hardware resources further, there are still several configurations that users can tune. This page shows tutorials for performance tuning guides, as well as an introduction of an easy-to-use tool.
+
+- `Performance Tuning Guide <performance_tuning/tuning_guide.html>`_
+- `Launch Script Usage Guide <performance_tuning/launch_script.html>`_
+- `TorchServe with Intel® Extension for PyTorch* <performance_tuning/torchserve.html>`_
+- `Known Issues <performance_tuning/known_issues.html>`_
+
+.. toctree::
+   :hidden:
+
+   performance_tuning/tuning_guide
+   performance_tuning/launch_script
+   performance_tuning/torchserve
+   performance_tuning/known_issues
diff --git a/docs/tutorials/performance_tuning/known_issues.md b/docs/tutorials/performance_tuning/known_issues.md
new file mode 100644
index 000000000..a93d246a4
--- /dev/null
+++ b/docs/tutorials/performance_tuning/known_issues.md
@@ -0,0 +1,117 @@
+Known Issues
+============
+
+## Usage
+
+- There might be Python packages having PyTorch as their hard dependency. If you installed `+cpu` version of PyTorch, installation of these packages might replace the `+cpu` version with the default version released on Pypi.org. If anything goes wrong, please reinstall the `+cpu` version back.
+
+- If you found the workload runs with Intel® Extension for PyTorch\* occupies a remarkably large amount of memory, you can try to reduce the occupied memory size by setting the `--weights_prepack` parameter of the `ipex.optimize()` function to `False`.
+
+- If inference is done with a custom function, `conv+bn` folding feature of the `ipex.optimize()` function doesn't work.
+
+  ```
+  import torch
+  import intel_pytorch_extension as ipex
+
+  class Module(torch.nn.Module):
+      def __init__(self):
+          super(Module, self).__init__()
+          self.conv = torch.nn.Conv2d(1, 10, 5, 1)
+          self.bn = torch.nn.BatchNorm2d(10)
+          self.relu = torch.nn.ReLU()
+
+      def forward(self, x):
+          x = self.conv(x)
+          x = self.bn(x)
+          x = self.relu(x)
+          return x
+
+      def inference(self, x):
+          return self.forward(x)
+
+  if __name__ == '__main__':
+      m = Module()
+      m.eval()
+      m = ipex.optimize(m, dtype=torch.float32, level="O0")
+      d = torch.rand(1, 1, 112, 112)
+      with torch.no_grad():
+        m.inference(d)
+  ```
+
+  This is a PyTorch FX limitation. You can avoid this error by calling `m = ipex.optimize(m, level="O0")`, which doesn't apply ipex optimization, or disable `conv+bn` folding by calling `m = ipex.optimize(m, level="O1", conv_bn_folding=False)`.
+
+## TorchDynamo
+
+- The support of torch.compile() with ipex as the backend is still an experimental feature. If the workload fails to run or demonstrates poor performance, you can use the `torch.jit` APIs and graph optimization APIs of ipex. Currently, the below HuggingFace models fail to run using torch.compile() with ipex backend due to memory issues:
+  - masked-language-modeling+xlm-roberta-base
+  - casual-language-modeling+gpt2
+  - casual-language-modeling+xlm-roberta-base
+  - summarization+t5-base
+  - text-classification+allenai-longformer-base-409
+
+## Dynamic Shape
+
+- When working with an NLP model inference with dynamic input data length appling with TorchScript (either `torch.jit.trace` or `torch.jit.script`), performance with Intel® Extension for PyTorch\* is possible to be less than that without Intel® Extension for PyTorch\*. In this case, adding the workarounds below would help solve this issue.
+  - Python interface
+    ```python
+    torch._C._jit_set_texpr_fuser_enabled(False)
+    ```
+  - C++ interface
+    ```c++
+    #include <torch/csrc/jit/passes/tensorexpr_fuser.h>
+    torch::jit::setTensorExprFuserEnabled(false);
+    ```
+
+## INT8
+
+- Low performance with INT8 support for dynamic shapes
+
+  The support for dynamic shapes in Intel® Extension for PyTorch\* INT8 integration is still work in progress. When the input shapes are dynamic, for example inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch\* INT8 path may slow down the model inference. In this case, use stock PyTorch INT8 functionality.
+
+  **Note**: Using Runtime Extension feature if batch size cannot be divided by number of streams, because mini batch size on each stream are not equivalent, scripts run into this issues.
+
+- Supporting of EmbeddingBag with INT8 when bag size > 1 is working in progress.
+
+- `RuntimeError: Overflow when unpacking long` when a tensor's min max value exceeds int range while performing int8 calibration. Please customize QConfig to use min-max calibration method.
+
+- For models with dynamic control flow, please try dynamic quantization. Users are likely to get performance gain for GEMM models.
+
+- Calibrating with quantize_per_tensor, when benchmarking with 1 OpenMP\* thread, results might be incorrect with large tensors (find more detailed info [here](https://github.com/pytorch/pytorch/issues/80501). Editing your code following the pseudocode below can workaround this issue, if you do need to explicitly set OMP_NUM_THREAEDS=1 for benchmarking. However, there could be a performance regression if oneDNN graph compiler prototype feature is utilized.
+
+  Workaround pseudocode:
+  ```
+  # perform convert/trace/freeze with omp_num_threads > 1(N)
+  torch.set_num_threads(N)
+  prepared_model = prepare(model, input)
+  converted_model = convert(prepared_model)
+  traced_model = torch.jit.trace(converted_model, input)
+  freezed_model = torch.jit.freeze(traced_model)
+  # run freezed model to apply optimization pass
+  freezed_model(input)
+  
+  # benchmarking with omp_num_threads = 1
+  torch.set_num_threads(1)
+  run_benchmark(freezed_model, input)
+  ```
+
+## BFloat16
+
+- BF16 AMP(auto-mixed-precision) runs abnormally with the extension on the AVX2-only machine if the topology contains `Conv`, `Matmul`, `Linear`, and `BatchNormalization`
+
+## Runtime Extension
+
+- Runtime extension of MultiStreamModule doesn't support DLRM inference, since the input of DLRM (EmbeddingBag specifically) can't be simplely batch split.
+
+- Runtime extension of MultiStreamModule has poor performance of RNNT Inference comparing with native throughput mode. Only part of the RNNT models (joint_net specifically) can be jit traced into graph. However, in one batch inference, `joint_net` is invoked multi times. It increases the overhead of MultiStreamModule as input batch split, thread synchronization and output concat.
+
+## Correctness
+
+- Incorrect Conv and Linear result if the number of OMP threads is changed at runtime
+
+  The oneDNN memory layout depends on the number of OMP threads, which requires the caller to detect the changes for the # of OMP threads while this release has not implemented it yet.
+
+## Float32 Training
+
+- Low throughput with DLRM FP32 Train
+
+  A 'Sparse Add' [PR](https://github.com/pytorch/pytorch/pull/23057) is pending on review. The issue will be fixed when the PR is merged.
diff --git a/docs/tutorials/performance_tuning/tuning_guide.md b/docs/tutorials/performance_tuning/tuning_guide.md
index b178106bf..6dadcfb7c 100644
--- a/docs/tutorials/performance_tuning/tuning_guide.md
+++ b/docs/tutorials/performance_tuning/tuning_guide.md
@@ -37,7 +37,7 @@ On the Intel® Xeon® Scalable Processors with Intel® C620 Series Chipsets, (fo
 
 <div align="center">
 
-![Block Diagram of the Intel® Xeon® processor Scalable family microarchitecture](../../../images/performance_tuning_guide/block_diagram_xeon_architecture.png)
+![Block Diagram of the Intel® Xeon® processor Scalable family microarchitecture](https://software.intel.com/content/dam/develop/external/us/en/images/xeon-processor-scalable-family-tech-overview-fig03-737410.png)
 
 Figure 1: Block Diagram of the Intel® Xeon® processor Scalable family microarchitecture.
 
@@ -47,7 +47,7 @@ Usually, a CPU chip is called a socket. A typical two-socket configuration is il
 
 <div align="center">
 
-![Typical two-socket configuration](../../../images/performance_tuning_guide/two_socket_config.png)
+![Typical two-socket configuration](https://software.intel.com/content/dam/develop/external/us/en/images/xeon-processor-scalable-family-tech-overview-fig06-737410.png)
 
 Figure 2: Typical two-socket configuration.
 
diff --git a/docs/tutorials/releases.md b/docs/tutorials/releases.md
index 0bc6189df..a6f4a9ced 100644
--- a/docs/tutorials/releases.md
+++ b/docs/tutorials/releases.md
@@ -1,53 +1,6 @@
 Releases
 =============
 
-## 2.1.100
-
-### Highlights
-
-- Improved the performance of BF16 LLM generation inference: [#2253](https://github.com/intel/intel-extension-for-pytorch/commit/99aa54f757de6c7d98f704edc6f8a83650fb1541) [#2251](https://github.com/intel/intel-extension-for-pytorch/commit/1d5e83d85c3aaf7c00323d7cb4019b40849dd2ed) [#2236](https://github.com/intel/intel-extension-for-pytorch/commit/be349962f3362f8afde4f083ec04d335245992bb) [#2278](https://github.com/intel/intel-extension-for-pytorch/commit/066c3bff417df084fa8e1d48375c0e1404320e95)
-
-- Added the optimization for Codegen: [#2257](https://github.com/intel/intel-extension-for-pytorch/commit/7c598e42e5b7899f284616c05c6896bf9d8bd2b8)
-
-- Provided the dockerfile and updated the related doc to improve the UX for LLM users: [#2229](https://github.com/intel/intel-extension-for-pytorch/commit/11484c3ebad9f868d0179a46de3d1330d9011822) [#2195](https://github.com/intel/intel-extension-for-pytorch/commit/0cd25021952bddcf5a364da45dfbefd4a0c77af4) [#2299](https://github.com/intel/intel-extension-for-pytorch/commit/76a42e516a68539752a3a8ab9aeb814d28c44cf8) [#2315](https://github.com/intel/intel-extension-for-pytorch/commit/4091bb5c0bf5f3c3ce5fbece291b44159a7fbf5c) [#2283](https://github.com/intel/intel-extension-for-pytorch/commit/e5ed8270d4d89bf68757f967676db57292c71920)
-
-- Improved the accuracy of the quantization path of LLMs: [#2280](https://github.com/intel/intel-extension-for-pytorch/commit/abc4c4e160cec3c792f5316e358173b8722a786e) [#2292](https://github.com/intel/intel-extension-for-pytorch/commit/4e212e41affa2ed07ffaf57bf10e9781113bc101) [#2275](https://github.com/intel/intel-extension-for-pytorch/commit/ed5957eb3b6190ad0be728656674f0a2a3b89158) [#2319](https://github.com/intel/intel-extension-for-pytorch/commit/1dae69de39408bc0ad245f4914d5f60e008a6eb3)
-
-- Misc fix and enhancement: [#2198](https://github.com/intel/intel-extension-for-pytorch/commit/ed1deccb86403e12e895227045d558117c5ea0fe) [#2264](https://github.com/intel/intel-extension-for-pytorch/commit/5dedcd6eb7bbf70dc92f0c20962fb2340e42e76f) [#2290](https://github.com/intel/intel-extension-for-pytorch/commit/c6e46cecd899317acfd2bd2a44a3f17b3cc1ce69)
-
-**Full Changelog**: https://github.com/intel/intel-extension-for-pytorch/compare/v2.1.0+cpu...v2.1.100+cpu
-
-## 2.1.0
-
-### Highlights
-
-- **Large Language Model (LLM) optimization (Experimental)**: Intel® Extension for PyTorch\* provides a lot of specific optimizations for LLMs in this new release. In operator level, we provide highly efficient GEMM kernel to speedup Linear layer and customized operators to reduce the memory footprint. To better trade-off the performance and accuracy, different low-precision solutions e.g., smoothQuant for INT8 and weight-only-quantization for INT4 and INT8 are also enabled. Besides, tensor parallel can also be adopt to get lower latency for LLMs.
-
-  A new API function, `ipex.optimize_transformers`, is designed to optimize transformer-based models within frontend Python modules, with a particular focus on Large Language Models (LLMs). It provides optimizations for both model-wise and content-generation-wise. You just need to invoke the `ipex.optimize_transformers` function instead of the `ipex.optimize` function to apply all optimizations transparently. More detailed information can be found at [Large Language Model optimizations overview](./llm.rst).
-
-  Specifically, this new release includes the support of [SmoothQuant]( https://arxiv.org/abs/2211.10438) and weight only quantization (both INT8 weight and INT4 weight) as to provide better performance and accuracy for low precision scenarios.
-
-  A typical usage of this new feature is quite simple as below:
-
-  ```python
-  import torch
-  import intel_extension_for_pytorch as ipex
-  ...
-  model = ipex.optimize_transformers(model, dtype=dtype)
-  ```
-
-- **torch.compile backend optimization with PyTorch Inductor (Experimental)**: We optimized Intel® Extension for PyTorch to leverage PyTorch Inductor’s capability when working as a backend of torch.compile, which can better utilize torch.compile’s power of graph capture, Inductor’s scalable fusion capability, and still keep customized optimization from Intel® Extension for PyTorch.
-
-- **performance optimization of static quantization under dynamic shape**: We optimized the static quantization performance of Intel® Extension for PyTorch for dynamic shapes. The usage is the same as the workflow of running static shapes while inputs of variable shapes could be provided during runtime.
-
-- **Bug fixing and other optimization**
-    - Optimized the runtime memory usage [#1563](https://github.com/intel/intel-extension-for-pytorch/commit/a821c0aef97ee6252d2bfbe6a75b6085f78bcc59)
-    - Fixed the excessive size of the saved model [#1677](https://github.com/intel/intel-extension-for-pytorch/commit/39f2d0f4e91c6007cb58566b63e06b72d7b17ce4) [#1688](https://github.com/intel/intel-extension-for-pytorch/commit/58adee5b043a52e0c0a60320d48eae82de557074)
-    - Supported shared parameters in `ipex.optimize` [#1664](https://github.com/intel/intel-extension-for-pytorch/commit/4fa37949385db88b854eb60ab6de7178706cdcfe)
-    - Enabled the optimization of LARS fusion [#1695](https://github.com/intel/intel-extension-for-pytorch/commit/e5b169e8d1e06558bb366eeaf4c793a382bc2d62)
-    - Supported dictionary input in `ipex.quantization.prepare` [#1682](https://github.com/intel/intel-extension-for-pytorch/commit/30b70e4b0bd8c3d1b2be55147ebd74fbfebe6093)
-    - Updated oneDNN to v3.3 [#2137](https://github.com/intel/intel-extension-for-pytorch/commit/4dc4bb5f9d1cfb9f958893a410f7332be4b5f783)
-
 ## 2.0.100
 
 ### Highlights
@@ -96,7 +49,7 @@ model = torch.compile(model, backend='ipex')
 
 ### Known Issues
 
-Please check at [Known Issues webpage](./known_issues.md).
+Please check at [Known Issues webpage](./performance_tuning/known_issues.md).
 
 ## 1.13.100
 
@@ -198,7 +151,7 @@ We are pleased to announce the release of Intel® Extension for PyTorch\* 1.13.0
 
 ### Known Issues
 
-Please check at [Known Issues webpage](./known_issues.md).
+Please check at [Known Issues webpage](./performance_tuning/known_issues.md).
 
 ## 1.12.300
 
diff --git a/examples/cpu/inference/cpp/example-app.cpp b/examples/cpu/inference/cpp/example-app.cpp
index 9d867e176..c0615a1df 100644
--- a/examples/cpu/inference/cpp/example-app.cpp
+++ b/examples/cpu/inference/cpp/example-app.cpp
@@ -17,7 +17,6 @@ int main(int argc, const char* argv[]) {
 
   at::Tensor output = module.forward(inputs).toTensor();
   std::cout << output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) << std::endl;
-  std::cout << "Execution finished" << std::endl;
 
   return 0;
 }
diff --git a/examples/cpu/inference/python/bert_general_inference_script.py b/examples/cpu/inference/python/bert_general_inference_script.py
index 0e0d3be89..18ee81935 100644
--- a/examples/cpu/inference/python/bert_general_inference_script.py
+++ b/examples/cpu/inference/python/bert_general_inference_script.py
@@ -4,23 +4,23 @@ from transformers import BertModel
 def inference(model, data):
     with torch.no_grad():
         # warm up
-        for _ in range(10):
+        for _ in range(100):
             model(data)
 
         # measure
         import time
         start = time.time()
-        for _ in range(10):
+        for _ in range(100):
             model(data)
         end = time.time()
-        print('Inference took {:.2f} ms in average'.format((end - start) / 10 * 1000))
+        print('Inference took {:.2f} ms in average'.format((end - start) / 100 * 1000))
 
 def main(args):
     model = BertModel.from_pretrained(args.model_name)
     model.eval()
 
     vocab_size = model.config.vocab_size
-    batch_size = 128
+    batch_size = 1
     seq_length = 512
     data = torch.randint(vocab_size, size=[batch_size, seq_length])
 
@@ -30,11 +30,29 @@ def main(args):
         model = ipex.optimize(model, dtype=torch.float32)
     elif args.dtype == 'bfloat16':
         model = ipex.optimize(model, dtype=torch.bfloat16)
+    else:  # int8
+        from intel_extension_for_pytorch.quantization import prepare, convert
+
+        if args.quantization == 'static':
+            qconfig = ipex.quantization.default_static_qconfig
+            model = prepare(model, qconfig, example_inputs=data, inplace=False)
+
+            # calibration
+            n_iter = 100
+            for i in range(n_iter):
+                model(data)
+
+            model = convert(model)
+        else:
+            qconfig = ipex.quantization.default_dynamic_qconfig
+            model = prepare(model, qconfig, example_inputs=data)
+            model = convert(model)
 
     with torch.cpu.amp.autocast(enabled=args.dtype == 'bfloat16'):
-        with torch.no_grad():
-            model = torch.jit.trace(model, data, check_trace=False, strict=False)
-            model = torch.jit.freeze(model)
+        if args.torchscript:
+            with torch.no_grad():
+                model = torch.jit.trace(model, data, check_trace=False, strict=False)
+                model = torch.jit.freeze(model)
 
         inference(model, data)
 
@@ -42,7 +60,9 @@ if __name__ == '__main__':
     import argparse
     parser = argparse.ArgumentParser()
     parser.add_argument("--model_name", default="bert-base-multilingual-cased")
-    parser.add_argument('--dtype', default='float32', choices=['float32', 'bfloat16'])
+    parser.add_argument('--dtype', default='float32', choices=['float32', 'bfloat16', 'int8'])
+    parser.add_argument("--torchscript", default=False, action="store_true")
+    parser.add_argument('--quantization', default='static', choices=['static', 'dynamic'])
 
     main(parser.parse_args())
 
diff --git a/examples/cpu/inference/python/bert_imperative_mode_inference_bf16.py b/examples/cpu/inference/python/bert_imperative_mode_inference_bf16.py
index fd207d2ca..2df639d58 100644
--- a/examples/cpu/inference/python/bert_imperative_mode_inference_bf16.py
+++ b/examples/cpu/inference/python/bert_imperative_mode_inference_bf16.py
@@ -5,7 +5,7 @@ model = BertModel.from_pretrained("bert-base-uncased")
 model.eval()
 
 vocab_size = model.config.vocab_size
-batch_size = 128
+batch_size = 1
 seq_length = 512
 data = torch.randint(vocab_size, size=[batch_size, seq_length])
 
diff --git a/examples/cpu/inference/python/bert_imperative_mode_inference_fp32.py b/examples/cpu/inference/python/bert_imperative_mode_inference_fp32.py
index 255ad5f4f..a5f36d19f 100644
--- a/examples/cpu/inference/python/bert_imperative_mode_inference_fp32.py
+++ b/examples/cpu/inference/python/bert_imperative_mode_inference_fp32.py
@@ -5,7 +5,7 @@ model = BertModel.from_pretrained("bert-base-uncased")
 model.eval()
 
 vocab_size = model.config.vocab_size
-batch_size = 128
+batch_size = 1
 seq_length = 512
 data = torch.randint(vocab_size, size=[batch_size, seq_length])
 
diff --git a/examples/cpu/inference/python/bert_torchdynamo_mode_inference_fp32.py b/examples/cpu/inference/python/bert_torchdynamo_mode_inference_fp32.py
index cb586bd4d..0811d44b9 100644
--- a/examples/cpu/inference/python/bert_torchdynamo_mode_inference_fp32.py
+++ b/examples/cpu/inference/python/bert_torchdynamo_mode_inference_fp32.py
@@ -5,14 +5,14 @@ model = BertModel.from_pretrained("bert-base-uncased")
 model.eval()
 
 vocab_size = model.config.vocab_size
-batch_size = 128
+batch_size = 1
 seq_length = 512
 data = torch.randint(vocab_size, size=[batch_size, seq_length])
 
 # Experimental Feature
 #################### code changes ####################  # noqa F401
 import intel_extension_for_pytorch as ipex
-model = ipex.optimize(model, weights_prepack=False)
+model = ipex.optimize(model)
 model = torch.compile(model, backend="ipex")
 ######################################################  # noqa F401
 
diff --git a/examples/cpu/inference/python/bert_torchscript_mode_inference_bf16.py b/examples/cpu/inference/python/bert_torchscript_mode_inference_bf16.py
index 02c384e6a..130c8b191 100644
--- a/examples/cpu/inference/python/bert_torchscript_mode_inference_bf16.py
+++ b/examples/cpu/inference/python/bert_torchscript_mode_inference_bf16.py
@@ -5,7 +5,7 @@ model = BertModel.from_pretrained("bert-base-uncased")
 model.eval()
 
 vocab_size = model.config.vocab_size
-batch_size = 128
+batch_size = 1
 seq_length = 512
 data = torch.randint(vocab_size, size=[batch_size, seq_length])
 
diff --git a/examples/cpu/inference/python/bert_torchscript_mode_inference_fp32.py b/examples/cpu/inference/python/bert_torchscript_mode_inference_fp32.py
index 98b64dc9b..f75281e8b 100644
--- a/examples/cpu/inference/python/bert_torchscript_mode_inference_fp32.py
+++ b/examples/cpu/inference/python/bert_torchscript_mode_inference_fp32.py
@@ -5,7 +5,7 @@ model = BertModel.from_pretrained("bert-base-uncased")
 model.eval()
 
 vocab_size = model.config.vocab_size
-batch_size = 128
+batch_size = 1
 seq_length = 512
 data = torch.randint(vocab_size, size=[batch_size, seq_length])
 
diff --git a/examples/cpu/inference/python/int8_calibration_dynamic.py b/examples/cpu/inference/python/int8_calibration_dynamic.py
index aaeb4ea21..c5dd39c88 100644
--- a/examples/cpu/inference/python/int8_calibration_dynamic.py
+++ b/examples/cpu/inference/python/int8_calibration_dynamic.py
@@ -11,7 +11,7 @@ model = BertModel.from_pretrained("bert-base-uncased")
 model.eval()
 
 vocab_size = model.config.vocab_size
-batch_size = 128
+batch_size = 1
 seq_length = 512
 data = torch.randint(vocab_size, size=[batch_size, seq_length])
 #########################  # noqa F401
diff --git a/examples/cpu/inference/python/int8_calibration_static.py b/examples/cpu/inference/python/int8_calibration_static.py
index fc8964312..dabed13aa 100644
--- a/examples/cpu/inference/python/int8_calibration_static.py
+++ b/examples/cpu/inference/python/int8_calibration_static.py
@@ -8,7 +8,7 @@ from intel_extension_for_pytorch.quantization import prepare, convert
 import torchvision.models as models
 model = models.resnet50(weights='ResNet50_Weights.DEFAULT')
 model.eval()
-data = torch.rand(128, 3, 224, 224)
+data = torch.rand(1, 3, 224, 224)
 #########################  # noqa F401
 
 qconfig_mapping = ipex.quantization.default_static_qconfig_mapping
diff --git a/examples/cpu/inference/python/int8_deployment.py b/examples/cpu/inference/python/int8_deployment.py
index bf8c954a0..b8d51a12f 100644
--- a/examples/cpu/inference/python/int8_deployment.py
+++ b/examples/cpu/inference/python/int8_deployment.py
@@ -6,7 +6,7 @@ import intel_extension_for_pytorch as ipex              # noqa F401
 model = torch.jit.load('quantized_model.pt')
 model.eval()
 model = torch.jit.freeze(model)
-data = torch.rand(128, 3, 224, 224)
+data = torch.rand(1, 3, 224, 224)
 
 with torch.no_grad():
     model(data)
diff --git a/examples/cpu/inference/python/llm/README.md b/examples/cpu/inference/python/llm/README.md
index 7f7a6fc2a..51c95bf53 100644
--- a/examples/cpu/inference/python/llm/README.md
+++ b/examples/cpu/inference/python/llm/README.md
@@ -1,367 +1,151 @@
 # Text Generation
-
-We provide the inference benchmarking scripts for large language models (LLMs) text generation, by which several popular models in LLM family, including GPT-J, LLaMA, GPT-Neox, OPT, Falcon, CodeGen, are optimized.<br/>
+We provide the inference benchmarking scripts for large language models text generation.<br/>
+Support large language models, such as GPT-J, LLaMA, GPT-Neox, OPT, Falcon.<br/>
 The scripts include both single instance and distributed (DeepSpeed) use cases.<br/>
 The scripts cover model generation inference with low precions cases for different models with best perf and accuracy (bf16 AMP，static quantization and weight only quantization).<br/>
 
-# Optimized Model List
-
-| MODEL FAMILY | Verified <MODEL ID> (Huggingface hub)| FP32/BF16 | Weight only quantzation INT8 | Weight only quantization INT4| Static quantization INT8 |
-|---|:---:|:---:|:---:|:---:|:---:|
-|LLAMA| "meta-llama/Llama-2-7b-hf", "meta-llama/Llama-2-13b-hf", "meta-llama/Llama-2-70b-hf" | ✅ | ✅ | ✅ | ✅ |
-|GPT-J| "EleutherAI/gpt-j-6b" | ✅ | ✅ | ✅ | ✅ |
-|GPT-NEOX| "EleutherAI/gpt-neox-20b" | ✅ | ✅ | ✅ | ❎ \*\* |
-|FALCON\*|"tiiuae/falcon-40b" | ✅ | ✅ |  ✅ | ❎ \*\*|
-|OPT|"facebook/opt-30b", "facebook/opt-1.3b"| ✅ | ✅ |  ✅ | ❎ \*\*|
-|CodeGen|"Salesforce/codegen-2B-multi"| ✅ | ✅ |  ✅ | ❎ \*\*|
-
-\* For Falcon models from remote hub, we need to modify the config.json to use the modeling_falcon.py in transformers. Therefore, in the following scripts, we need to pass an extra configuration file like "--config-file=model_config/tiiuae_falcon-40b_config.json". This is optional for FP32/BF16 but needed for quantizations.
-
-\*\* For GPT-NEOX/FALCON/OPT/CodeGen models, the accuracy recipes of static quantization INT8 are not ready thus they will be skipped in our coverage.
-
-*Note:* The above verified models (including other models in the same model family, like "codellama/CodeLlama-7b-hf" from LLAMA family) are well supported with all optimizations like indirect access KV cache, fused ROPE, and prepacked Linear (fp32/bf16). For other LLM families, we are working in progress to cover those optimizations, which will expand the model list above.
-
-# Models to be Optimized
-
-We are working on the optimizations of a wider range of popular LLMs. Models like BLOOM, ChatGLM2/ChatGLM3, T5, BaiChuan/BaiChuan2, StarCoder and CodeLlama are to be optimized in the next release, and more models like Dolly2, MPT, QWen, Mistral, etc. are on the way.
-
-# Environment Setup
-
-1\. Get the Intel® Extension for PyTorch\* source code
-
-```bash
-git clone https://github.com/intel/intel-extension-for-pytorch.git
-cd intel-extension-for-pytorch
-git checkout v2.1.100+cpu
-git submodule sync
-git submodule update --init --recursive
-```
-
-2\.a. It is highly recommended to build a Docker container from the provided `Dockerfile`.
-
-```bash
-# Build an image with the provided Dockerfile by compiling Intel® Extension for PyTorch\* from source
-DOCKER_BUILDKIT=1 docker build -f examples/cpu/inference/python/llm/Dockerfile --build-arg COMPILE=ON -t ipex-llm:2.1.100 .
-
-# Build an image with the provided Dockerfile by installing from Intel® Extension for PyTorch\* prebuilt wheel files
-DOCKER_BUILDKIT=1 docker build -f examples/cpu/inference/python/llm/Dockerfile -t ipex-llm:2.1.100 .
-
-# Run the container with command below
-docker run --rm -it --privileged ipex-llm:2.1.100 bash
-
-# When the command prompt shows inside the docker container, enter llm examples directory
-cd llm
-```
-
-2\.b. Alternatively, you can take advantage of a provided environment configuration script to setup an environment without using a docker container.
-
-```bash
-# GCC 12.3 is required. Installation can be taken care of by the environment configuration script.
-# Create a conda environment
-conda create -n llm python=3.10 -y
-conda activate llm
-
-# Setup the environment with the provided script
-cd examples/cpu/inference/python/llm
-bash ./tools/env_setup.sh 7
-```
-
-3\. Once an environment is configured with either method above, set necessary environment variables with an environment variables activation script and download the sample `prompt.json`.
-
-```bash
-# Activate environment variables
-source ./tools/env_activate.sh
-```
-
-# Run Models Generations
-
-| Benchmark mode | FP32/BF16 | Weight only quantzation INT8 | Weight only quantization INT4 | Static quantization INT8 |
-|---|:---:|:---:|:---:|:---:|
-|Single instance | ✅ | ✅ | ✅ | ✅ |
-| Distributed (autotp) |  ✅ | ✅ | ❎ | ❎ |
-
-You can run LLM with a one-click Python script "run.py" for all inference cases.
-```
-python run.py --help # for more detailed usages
-```
-| Key args of run.py | Notes |
-|---|:---:|
-| generation | default: beam search (beam size = 4), "--greedy" for greedy search |
-| input tokens | default: 32, provide fixed sizes for input prompt size, use "--input-tokens" for [32, 64, 128, 256, 512, 1024, 2016, 2017, 2048, 4096, 8192]; if "--input-tokens" is not used, use "--prompt" to choose other strings as inputs|
-| output tokens | default: 32, use "--max-new-tokens" to choose any other size |
-| batch size |  default: 1, use "--batch-size" to choose any other size |
-| token latency |  enable "--token-latency" to print out the first or next token latency |
-| generation iterations |  use "--num-iter" and "--num-warmup" to control the repeated iterations of generation, default: 100-iter/10-warmup |
-
-*Note:* You may need to log in your HuggingFace account to access the model files. Please refer to [HuggingFace login](https://huggingface.co/docs/huggingface_hub/quick-start#login).
-
-## Example usages of one-click Python script
-
-### Quick start example commands for benchmarking LLaMA2-7b
-
-```bash
-# The following "OMP_NUM_THREADS" and "numactl" settings are based on the assumption that
-# the target server has 56 physical cores per numa socket, and we benchmark with 1 socket.
-# Please adjust the settings per your hardware.
-
-# Running FP32 model
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py --benchmark -m meta-llama/Llama-2-7b-hf --dtype float32
-
-# Running FP32 model with IPEX
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py --benchmark -m meta-llama/Llama-2-7b-hf --dtype float32 --ipex --deployment-mode
-
-# Running BF16 model with IPEX
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py --benchmark -m meta-llama/Llama-2-7b-hf --dtype bfloat16 --ipex --deployment-mode
-
-# INT8 static quantization
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py  --benchmark -m meta-llama/Llama-2-7b-hf --ipex-smooth-quant --qconfig-summary-file <path to "llama-2-7b_qconfig.json"> --output-dir "saved_results" --int8-bf16-mixed
-
-# INT8 weight-only quantization
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py  --benchmark -m meta-llama/Llama-2-7b-hf --ipex-weight-only-quantization  --output-dir "saved_results" --int8-bf16-mixed
-
-# INT4 weight-only quantization
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py  --benchmark -m meta-llama/Llama-2-7b-hf --ipex-weight-only-quantization  --output-dir "saved_results" --int8-bf16-mixed --gptq
-
-# Distributed inference in BF16
-deepspeed --bind_cores_to_rank  run.py --benchmark -m meta-llama/Llama-2-7b-hf --dtype bfloat16 --ipex --deployment-mode --autotp --shard-model
-
-# Distributed inference in weight-only quantization mode
-deepspeed --bind_cores_to_rank  run.py --benchmark -m meta-llama/Llama-2-7b-hf --ipex --ipex-weight-only-quantization --output-dir "saved_results" --int8-bf16-mixed --autotp --shard-model --deployment-mode
-```
-
-### Quick start example commands for accuracy test with LLaMA2-7b
-
-Check [Advanced Usage](#accuracy-test) for details.
-
-For the quantized models used in accuracy tests below, we can reuse the model files that are named "best_model.pt" in the "--output-dir" path ([generated during inference performance tests above](#generation_sq)).
-
-```bash
-# The following "OMP_NUM_THREADS" and "numactl" settings are based on the assumption that
-# the target server has 56 physical cores per numa socket, and we benchmark with 1 socket.
-# Please adjust the settings per your hardware.
-
-# run_accuracy.py script is inside single_instance directory.
+## Setup
+```bash
+WORK_DIR=$PWD
+# GCC 12.3 is required, please set it firstly
+# Create environment (conda recommended)
+conda create -n llm python=3.9 -y
+# install deps
+conda install cmake ninja mkl mkl-include -y
+conda install gperftools -c conda-forge -y
+
+# Install PyTorch 2.1 release
+python -m pip install torch==2.1 --index-url https://download.pytorch.org/whl/cpu
+
+# Install IPEX 2.1 release
+python -m pip install intel_extension_for_pytorch
+
+# Used for accuracy test only
+git clone https://github.com/EleutherAI/lm-evaluation-harness
+cd lm-evaluation-harness
+pip install -e .
+
+# Install transformers
+pip install transformers==4.31.0
+# Install others deps
+pip install cpuid accelerate datasets sentencepiece protobuf==3.20.3
+
+# Setup environment variables for performance on Xeon
+export LD_PRELOAD=${CONDA_PREFIX}/lib/libstdc++.so.6
+export KMP_BLOCKTIME=INF
+export KMP_TPAUSE=0
+export KMP_SETTINGS=1
+export KMP_AFFINITY=granularity=fine,compact,1,0
+export KMP_FORJOIN_BARRIER_PATTERN=dist,dist
+export KMP_PLAIN_BARRIER_PATTERN=dist,dist
+export KMP_REDUCTION_BARRIER_PATTERN=dist,dist
+export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libiomp5.so # Intel OpenMP
+# Tcmalloc is a recommended malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support.
+export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libtcmalloc.so
+
+# [Optional] install neural-compressor for GPT-J static quantization and running GPTQ (see below)
+pip install neural-compressor==2.3.1
+
+# [Optional] The following is only for DeepSpeed case
+#Install oneccl-bind-pt(also named torch-ccl)
+git clone https://github.com/intel/torch-ccl.git
+cd torch-ccl && git checkout ccl_torch_dev_0905
+git submodule sync && git submodule update --init --recursive
+python setup.py install
+cd ../
+#Install DeepSpeed
+git clone https://github.com/delock/DeepSpeedSYCLSupport
+cd DeepSpeedSYCLSupport
+git checkout gma/run-opt-branch
+python -m pip install -r requirements/requirements.txt
+python setup.py install
+cd ../
+#Install OneCCL
+git clone https://github.com/oneapi-src/oneCCL.git
+cd oneCCL
+mkdir build
+cd build
+cmake ..
+make -j install
+source _install/env/setvars.sh
+cd ../..
+
+# Get the sample prompt.json
+# Make sure the downloaded prompt.json file is under the same directory as that of the python scripts mentioned above.
+wget https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/prompt.json
+
+```
+
+## Supported Model List
+```
+<MODEL ID> in
+(1) "EleutherAI/gpt-j-6b" (model id from transformers Hub)
+(2) "EleutherAI/gpt-neox-20b" (model id from transformers Hub)
+(3) Llama 2 Model directory path
+(4) "facebook/opt-30b" (model id from transformers Hub)
+(5) "tiiuae/falcon-40b" (model id from transformers Hub)
+Note: Above models are well supported with all optimizations like indirect access KV cache, fused ROPE, and prepacked TPP Linear (fp32/bf16). For other LLM models, we could still run with this BKC, and may get parts of optimizations like prepacked TPP Linear (fp32/bf16), and we are working in progress to cover all optimizations to these other LLM models, which will expand the model list above.
+```
+* Llama 2 model conversion steps:
+    1) [transformers conversion tool](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py) (Verified [meta-llama/Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat) and [meta-llama/Llama-2-13b-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)).
+    2) Follow [instructions](https://github.com/facebookresearch/llama#access-on-hugging-face) to download model files for conversion.
+    3) Decompress the downloaded model file.
+    4) Follow [instructions](https://github.com/facebookresearch/llama-recipes#model-conversion-to-hugging-face) to convert the model.
+    5) Launch example scripts with the place holder <MODEL_ID> substituted by the --output_dir argument value of the conversion script.
+* For Falcon model from remote hub, we need to modify the config.json to use the modeling_falcon.py in transformers. Therefore, in the following scripts, we need to pass an extra configuration file like "--config-file=model_config/tiiuae_falcon-40b_config.json". This is optional for BF16, but must for quantization benchmark.
+
+## Single Instance Performance
+```bash
+# Get prompt file to the path of scripts
+export WORK_DIR=./
 cd single_instance
+mv PATH/TO/prompt.json ./
+# bfloat16 benchmark
+OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run_generation.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --deployment-mode
 
-# Running FP32 model
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run_accuracy.py --accuracy-only -m meta-llama/Llama-2-7b-hf --dtype float32 --ipex --jit --tasks lambada_openai
-
-# Running BF16 model
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run_accuracy.py --accuracy-only -m meta-llama/Llama-2-7b-hf --dtype bfloat16 --ipex --jit --tasks lambada_openai
-
-# Quantization
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run_accuracy.py -m meta-llama/Llama-2-7b-hf --quantized-model-path "./saved_results/best_model.pt" --dtype int8 --accuracy-only --jit --tasks lambada_openai --int8-bf16-mixed
-
-
-# run_accuracy_with_deepspeed.py script is inside distributed directory.
-cd distributed
-unset KMP_AFFINITY
-
-# Distributed inference in FP32
-deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model  meta-llama/Llama-2-7b-hf --dtype float32 --ipex --jit --tasks lambada_openai --accuracy-only
-
-# Distributed inference in BF16
-deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model  meta-llama/Llama-2-7b-hf --dtype bfloat16 --ipex --jit --tasks lambada_openai --accuracy-only
-
-# Distributed inference with Weight-Only Quantization
-deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model  meta-llama/Llama-2-7b-hf --int8-bf16-mixed --ipex --jit --tasks lambada_openai --accuracy-only --ipex-weight-only-quantization
-```
-
-### Single Instance inference
-
-#### FP32:
-
-```bash
-# general command:
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run.py --benchmark -m <MODEL_ID> --dtype float32 --ipex --deployment-mode
-
-# An example of llama2 7b model:
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py --benchmark -m meta-llama/Llama-2-7b-hf --dtype float32 --ipex --deployment-mode
-```
-
-#### BF16:
-
-```bash
-# general command:
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --deployment-mode
-
-# An example of llama2 7b model:
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py --benchmark -m meta-llama/Llama-2-7b-hf --dtype bfloat16 --ipex --deployment-mode
-```
-
-#### Static quantization (int8):
-
-```bash
-# general command:
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run.py  --benchmark -m <MODEL_ID> --ipex-smooth-quant --qconfig-summary-file <path to the qconfig of the model_id> --output-dir "saved_results" --int8-bf16-mixed
-# Note: by default, we use "--int8-bf16-mixed" to run int8 mixed bf16 inference with peak performance of static quantization, if you observe accuracy drops, please use "--int8" instead to run int8 mixed fp32.
-
-# An example of llama2 7b model:
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py  --benchmark -m meta-llama/Llama-2-7b-hf --ipex-smooth-quant --qconfig-summary-file <path to "llama-2-7b_qconfig.json"> --output-dir "saved_results" --int8-bf16-mixed
-```
-
-- We provide the downloading links of tuned static quantization qconfig summary files with good quality: ["meta-llama/Llama-2-7b-hf"](https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/llama-2-7b_qconfig.json), ["meta-llama/Llama-2-7b-chat-hf"](https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/llama-2-7b-chat_qconfig.json), ["meta-llama/Llama-2-13b-hf"](https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/llama-2-13b_qconfig.json) and ["EleutherAI/gpt-j-6b"](https://intel-extension-for-pytorch.s3.amazonaws.com/miscellaneous/llm/gpt-j-6b_qconfig.json).
-
-- For other models' qconfig recipes, you can just try to run your model_id and use IPEX default recipes by removing "--qconfig-summary-file <path to specific model qconfig>". If IPEX default recipes are not good enough for accuracy requirements, please refer to the [Intel® Neural Compressor tutorial](https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md#validated-models) for more tuned recipes.
-
-#### Weight-only quantization:
-
-```bash
-# int8 general command:
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run.py  --benchmark -m <MODEL_ID> --ipex-weight-only-quantization  --output-dir "saved_results" --int8-bf16-mixed
-# for GPT-NEOX Weight-only quantizations, using "--int8" instead of "--int8-bf16-mixed" for accuracy concerns.
-
-# An example of llama2 7b model:
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py  --benchmark -m meta-llama/Llama-2-7b-hf --ipex-weight-only-quantization  --output-dir "saved_results" --int8-bf16-mixed
-
-
-# int4 general command:
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run.py  --benchmark -m <MODEL_ID> --ipex-weight-only-quantization --gptq --output-dir "saved_results" --int8-bf16-mixed
-# for GPT-NEOX Weight-only quantizations, using "--int8" instead of "--int8-bf16-mixed" for accuracy concerns.
-
-# An example of llama2 7b model:
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run.py  --benchmark -m meta-llama/Llama-2-7b-hf --ipex-weight-only-quantization  --output-dir "saved_results" --int8-bf16-mixed --gptq
-```
-
-*Notes:*
-
-(1) [_numactl_](https://linux.die.net/man/8/numactl) is used to specify memory and cores of your hardware to get better performance. _\<node N\>_ specifies the [numa](https://en.wikipedia.org/wiki/Non-uniform_memory_access) node id (e.g., 0 to use the memory from the first numa node). _\<physical cores list\>_ specifies phsysical cores which you are using from the _\<node N\>_ numa node (e.g., 0-56 from the first numa node). You can use [_lscpu_](https://man7.org/linux/man-pages/man1/lscpu.1.html) command in Linux to check the numa node information.
-
-(2) The _\<MODEL_ID\>_ (e.g., "meta-llama/Llama-2-13b-hf") specifies the model you will run. we provide some _Verified \<MODEL ID\>_ in the [Optimized Model List](#optimized-model-list). You can also try other models from [HuggingFace Models](https://huggingface.co/models).
-
-(3) <a name="generation_sq">for all quantization benchmarks</a>, both quantization and inference stages will be triggered by default. For quantization stage, it will auto-generate the quantized model named "best_model.pt" in the "--output-dir" path, and for inference stage, it will launch the inference with the quantized model "best_model.pt".  For inference-only benchmarks (avoid the repeating quantization stage), you can also reuse these quantized models for by adding "--quantized-model-path <output_dir + "best_model.pt">" . Besides, specific for static quantization, if not using "--qconfig-summary-file", a qconfig recipe will also be generated in the "--output-dir" path, which could be reused as well (to generate the quantized model "best_model.pt").
-
-(4) for Falcon quantizations, "--config-file <CONFIG_FILE>" is needed in the command and the example of <CONFIG_FILE> is provided here: "utils/model_config/tiiuae_falcon-40b_config.json".
-
-### Distributed inference with DeepSpeed (autoTP)
-
-#### Prepare:
-
-```bash
-unset KMP_AFFINITY
-```
-
-In the DeepSpeed cases below, we recommend "--shard-model" to shard model weight sizes more even for better memory usage when running with DeepSpeed.
+# quantization benchmark
+#To run quantization performance, you need to firstly get the quantized model with the following step (1) and then run the performance benchmark with the following step (2)
+## (1) Do quantization to get the quantized model 
+## note: llama/gptj we have both IPEX smooth quant and weight-only-quantization, while for rest models, we recommend weight-only-quantization
+mkdir saved_results
 
-If using "--shard-model", it will save a copy of the shard model weights file in the path of "--output-dir" (default path is "./saved_results" if not provided).
-If you have used "--shard-model" and generated such a shard model path, in further repeated benchmarks, please remove "--shard-model", and replace "-m <MODEL_ID>" with "-m <shard model path>" to skip the repeated shard steps.
+## GPT-J quantization
+python run_gpt-j_quantization.py --ipex-smooth-quant --output-dir "saved_results" --int8-bf16-mixed -m <GPTJ MODEL_ID>
+## Llama 2 quantization
+python run_llama_quantization.py --ipex-smooth-quant --output-dir "saved_results" --int8-bf16-mixed -m <LLAMA MODEL_ID>
+## GPT-NEOX quantization
+python run_gpt-neox_quantization.py --ipex-weight-only-quantization --output-dir "saved_results" --int8 -m <GPT-NEOX MODEL_ID>
+## Falcon quantization (example of config-file: utils/model_config/tiiuae_falcon-40b_config.json)
+python run_falcon_quantization.py --ipex-weight-only-quantization --output-dir "saved_results"  --int8-bf16-mixed -m <FALCON MODEL_ID> --config-file <CONFIG_FILE>
+## OPT quantization
+python run_opt_quantization.py --ipex-weight-only-quantization --output-dir "saved_results"  --int8-bf16-mixed -m <OPT MODEL_ID> 
 
-Besides, the standalone shard model function/scripts are also provided in the [Advanced Usage](#how-to-shard-model-for-distributed-tests-with-deepspeed-autotp) section, in case you would like to generate the shard model weights files in advance before running distributed inference.
+## (2) Run quantization performance test (note that GPT-NEOX uses --int8 instead of --int8-bf16-mixed)
+OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python run_<MODEL>_quantization.py -m <MODEL_ID> --quantized-model-path "./saved_results/best_model.pt" --benchmark --int8-bf16-mixed
 
-#### FP32:
-```bash
-# general command:
-deepspeed --bind_cores_to_rank  run.py --benchmark -m <MODEL_ID> --dtype float32 --ipex --deployment-mode --autotp --shard-model
-
-# An example of llama2 7b model:
-deepspeed --bind_cores_to_rank  run.py --benchmark -m meta-llama/Llama-2-7b-hf --dtype float32 --ipex --deployment-mode --autotp --shard-model
 ```
 
-*Note:* By default, we use "--shard-model" for better memory usage, if your model path is already sharded, please remove "--shard-model"
-
-#### BF16:
-
-```bash
-# general command:
-deepspeed --bind_cores_to_rank  run.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --deployment-mode --autotp --shard-model
-
-# An example of llama2 7b model:
-deepspeed --bind_cores_to_rank  run.py --benchmark -m meta-llama/Llama-2-7b-hf --dtype bfloat16 --ipex --deployment-mode --autotp --shard-model
-```
-
-#### Weight-only quantization:
-
-```bash
-# int8 general command:
-deepspeed --bind_cores_to_rank run.py  --benchmark -m <MODEL_ID> --ipex --ipex-weight-only-quantization --output-dir "./saved_results" --int8-bf16-mixed --autotp --shard-model --deployment-mode
-# The command will shard the model, quantize the model by weight-only quantization then run the benchmark. The quantized model won't be saved.
-# for Falcon quantizations, "--config-file <CONFIG_FILE>" is needed and example of <CONFIG_FILE>: "utils/model_config/tiiuae_falcon-40b_config.json".
-# for GPT-NEOX weight-only quantizations, using "--int8" instead of "--int8-bf16-mixed", and add "--dtype float32" for accuracy concerns.
-
-# An example of llama2 7b model:
-deepspeed --bind_cores_to_rank  run.py --benchmark -m meta-llama/Llama-2-7b-hf --ipex --ipex-weight-only-quantization --output-dir "saved_results" --int8-bf16-mixed --autotp --shard-model --deployment-mode
-```
-
-## Instructions for Running LLM with Intel® Xeon® CPU Max Series
-
-Intel® Xeon® CPU Max Series are equipped with high bandwidth memory (HBM), which further accelerates LLM inference. For the common case that HBM and DDR are both installed in a Xeon® CPU Max Series server, the memory mode can be configured to Flat Mode or Cache Mode. Details about memory modes can be found at Section 3.1 in [the Xeon® CPU Max Series Configuration Guide](https://cdrdv2-public.intel.com/769060/354227-intel-xeon-cpu-max-series-configuration-and-tuning-guide.pdf).
-
-### Single Instance Inference with Xeon® CPU Max Series
-
-#### Cache Mode HBM
-
-In cache mode, only DDR address space is visible to software and HBM functions as a transparent memory-side cache for DDR. Therefore the usage is the same with [the common usage](#single-instance-inference).
-
-#### Flat Mode HBM
-
-In flat mode, HBM and DDR are exposed to software as separate address spaces in this mode. Therefore we need to check the `HBM_NODE_INDEX` of interest with commands like `lscpu`, then the LLM inference invoking command would be like:
-
-```bash
-# general command:
-OMP_NUM_THREADS=<HBM node cores num> numactl -m <HBM_NODE_INDEX> -C <HBM cores list> python run.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --deployment-mode
-
-# An example of llama2 7b model with HBM numa node index being 2:
-OMP_NUM_THREADS=56 numactl -m 2 -C 0-55 python run.py --benchmark -m meta-llama/Llama-2-7b-hf --dtype bfloat16 --ipex --deployment-mode
-```
-
-*Note:* For some very large models we may get an "OOM Error" due to HBM capacity limitations. In this case we can change `-m` argument for `numactl` to `-p` in the above command to enable the model inference with the larger DDR memory.
-
-```bash
-# general command:
-OMP_NUM_THREADS=<HBM node cores num> numactl -p <HBM_NODE_INDEX> -C <HBM cores list> python run.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --deployment-mode
-
-# An example of llama2 7b model with HBM numa node index being 2:
-OMP_NUM_THREADS=56 numactl -p 2 -C 0-55 python run.py --benchmark -m meta-llama/Llama-2-7b-hf --dtype bfloat16 --ipex --deployment-mode
-```
-
-### Distributed Inference with Xeon® CPU Max Series
-
-As HBM has memory capacity limitations, we need to shard the model in advance with DDR memory. Please follow [the example](#how-to-shard-model-for-distributed-tests-with-deepspeed-autotp).
-
-Then we can invoke distributed inference with `deepspeed` command:
-
-```bash
-# general command:
-deepspeed --bind_cores_to_rank  run.py --benchmark -m <SHARDED_MODEL_PATH> --dtype bfloat16 --ipex --deployment-mode --autotp
-```
-
-As the model has been sharded, we specify `SHARDED_MODEL_PATH` for `-m` argument instead of original model name or path, and `--shard-model` argument is not needed.
-
-## Performance Results
-
-The performance results on AWS instances can be found [here](../../../../../docs/tutorials/performance.md#llm-performance).
-
-# Advanced Usage
-
-## Weight-only quantization with low precision checkpoint (Experimental)
-
-Using INT4 weights can further improve performance by reducing memory bandwidth. However, direct per-channel quantization of weights to INT4 probably results in poor accuracy. Some algorithms can modify weights through calibration before quantizing weights to minimize accuracy drop. GPTQ is one of such algorithms. You may generate modified weights and quantization info (scales, zero points) for a certain model with a dataset by such algorithms. The results are saved as a `state_dict` in a `.pt` file. We provided a script here to run GPTQ (Intel® Neural Compressor 2.3.1 is required).
-
-Here is how to use it:
+## Weight only quantization with low precision checkpoint (Experimental)
+Using INT4 weights can further improve performance by reducing memory bandwidth. However, direct per-channel quantization of weights to INT4 probably results in poor accuracy. Some algorithms can modify weights through calibration before quantizing weights to minimize accuracy drop. GPTQ is one of such algorithms. You may generate modified weights and quantization info (scales, zero points) for a certain model with a some dataset by such algorithms. The results are saved as a `state_dict` in a `.pt` file. We provided a script here to run GPTQ (Intel(R) Neural Compressor 2.3.1 is required).
 
+Here is an example:
 ```bash
 # Step 1: Generate modified weights and quantization info
 python utils/run_gptq.py --model <MODEL_ID> --output-dir ./saved_results
-# Please note that tiiuae/falcon-40b is not supported yet
 ```
-
-It may take a few hours to finish. Modified weights and their quantization info are stored in `gptq_checkpoint_g128.pt`, where g128 means group size for input channel is 128 by default. Group size controls the granularity of quantization of weight along input channel. The dataset for calibration is NeelNanda/pile-10k by default. To use other dataset, such as lambada, you may use `--dataset <dataset id>` to specify.
-
+It may take a few hours to finish. Modified weights and their quantization info are stored in `gptq_checkpoint.pt`.
 Then generate model for weight only quantization with INT4 weights and run tasks.
-
 ```bash
 # Step 2: Generate quantized model with INT4 weights
 # Provide checkpoint file name by --low-precision-checkpoint <file name>
-python single_instance/run_<MODEL_ID>_quantization.py --ipex-weight-only-quantization --output-dir "saved_results" --int8-bf16-mixed -m <MODEL_ID> --low-precision-checkpoint "saved_results/gptq_checkpoint_g128.pt"
+python single_instance/run_<MODEL_ID>_quantization.py --ipex-weight-only-quantization --output-dir "saved_results" --int8-bf16-mixed -m <MODEL_ID> --low-precision-checkpoint "saved_results/gptq_checkpoint.pt"
 
 # Step 3: Run quantized model for latency benchmark
 # For GPT-NEOX, use --int8 instead of --int8-bf16-mixed
 OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python single_instance/run_<MODEL_ID>_quantization.py -m <MODEL_ID> --quantized-model-path "./saved_results/best_model.pt" --benchmark --int8-bf16-mixed
 # To run accuracy tests, please follow the instructions in the **Single Instance Accuracy** part
 ```
-
 If the checkpoint is generated by some other methods and have different keys in the state_dict, you will need to specify the keys for weight, scales, zero points and bias. Bias is optional in the state_dict while others are required. Default keys are:
-
 ```python
 {
     "weight_key": "packed_weight",
@@ -370,9 +154,7 @@ If the checkpoint is generated by some other methods and have different keys in
     "bias_key": "bias",
 }
 ```
-
 You need to make a config dict like above and pass it to `ipex.optimize_transformers` together with the state_dict from the checkpoint as a tuple `(state_dict, config_dict)`. You will need to modify the example script.
-
 ```python
 low_precision_checkpoint = torch.load(args.low_precision_checkpoint)
 config_dict = {
@@ -391,124 +173,60 @@ user_model = ipex.optimize_transformers(
     deployment_mode=False,
 )
 ```
-
-**Example**
-
-Intel® Extension for PyTorch\* with INT4 weight only quantization has been used in latest MLPerf submission (August 2023) to fully maximize the power of Intel® Xeon®, and also shows good accuracy as comparing with FP32. This example is a simplified version of the MLPerf task. It will download a finetuned FP32 GPT-J model used for MLPerf submission, quantize the model to INT4 and run a text summarization task on the `cnn_dailymail` dataset. The example runs for 1000 samples, which is a good approximation of the results for the entire dataset and saves time.
-
-```bash
-pip install evaluate nltk absl-py rouge_score
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> bash single_instance/run_int4_gpt-j_on_cnndailymail.sh
-```
-
-Please note that 100 GB disk space, 100 GB memory and Internet access are needed to run this example. The example will run for a few hours depending on your hardware and network condition. The example is verified on the 4th Generation Intel® Xeon® Scalable (Sapphire Rapids) platform. You may get different results on older platforms as some new hardware features are unavailable.
-
 **Checkpoint Requirements**
 
 IPEX now only supports some certain cases. Weights must be N by K and per-channel asymmetrically quantized (group size = -1) to UINT4 and then compressed along K axis to `torch.int32`.
-
 Data type of scales can be any floating point types. Shape of scales should be [N] or with additional dimensions whose length is 1, e.g., [N, 1] or [1, N]. Zero points should have the same shape as scales and stored as `torch.int32` but the true data type is UINT4. Bias is optional in the `state_dict` (checkpoint). If it is present, we read bias in the `state_dict`. Otherwise we read bias from the original model. Bias is `None` if it cannot be found in both cases.
 
-## Accuracy test:
-
-We leverage [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) for the accuracy test.
-
-We verify and recommend to test "lambada_openai" task, for more choice, see {TASK_NAME} in this [link](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).
-
-### Single Instance
-
-```bash
-cd ./single_instance
-```
-### FP32:
-
+## Single Instance Accuracy
 ```bash
-# general command:
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run_accuracy.py --accuracy-only -m <MODEL_ID> --dtype float32 --ipex --jit --tasks {TASK_NAME}
-
-# An example of llama2 7b model:
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run_accuracy.py --accuracy-only -m meta-llama/Llama-2-7b-hf --dtype float32 --ipex --jit --tasks lambada_openai
-```
-### BF16:
+Accuracy test {TASK_NAME}, choice in this [link](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md), by default we use "lambada_standard"
 
-```bash
-# general command:
+# bfloat16
 OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <physical cores list> python run_accuracy.py --accuracy-only -m <MODEL_ID> --dtype bfloat16 --ipex --jit --tasks {TASK_NAME}
 
-# An example of llama2 7b model:
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run_accuracy.py --accuracy-only -m meta-llama/Llama-2-7b-hf --dtype bfloat16 --ipex --jit --tasks lambada_openai
+# Quantization as a performance part
+# (1) Do quantization to get the quantized model as mentioned above
+# (2) Run int8 accuracy test (note that GPT-NEOX please remove --int8-bf16-mixed)
+OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python run_accuracy.py --model <MODEL ID> --quantized-model-path "./saved_results/best_model.pt" --dtype int8 --accuracy-only --jit --int8-bf16-mixed --tasks {TASK_NAME}
 ```
-
-### Quantizations:
-
-For the quantized models to be used in accuracy tests, we can reuse the model files that are named "best_model.pt" in the "--output-dir" path ([generated during inference performance tests](#generation_sq)).
-
-```bash
-# general command:
-OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python run_accuracy.py --model <MODEL ID> --quantized-model-path "./saved_results/best_model.pt" --dtype int8 --accuracy-only --jit --tasks {TASK_NAME} --int8-bf16-mixed
-# Please remove  "--int8-bf16-mixed" if your model is quantized without this flag
-
-# An example of llama2 7b model:
-OMP_NUM_THREADS=56 numactl -m 0 -C 0-55 python run_accuracy.py -m meta-llama/Llama-2-7b-hf --quantized-model-path "./saved_results/best_model.pt" --dtype int8 --accuracy-only --jit --tasks lambada_openai --int8-bf16-mixed
+## Shard model for Distributed Performance
 ```
-
-### Distributed with DeepSpeed (autoTP)
-
-### Prepare:
-
-```bash
-# Run distributed accuracy with 2 ranks of one node
-cd ./distributed
-unset KMP_AFFINITY
+# We need to make sure the model is well shard before we test Distributed Performance with DeepSpeed (saving memory usage purpose)
+export WORK_DIR=./
+cd utils
+python create_shard_model.py -m <MODEL ID>  --save-path <SHARD MODEL NEW PATH>
+# After sharding the model, using -m <SHARD MODEL NEW PATH> in later tests.
 ```
-### FP32:
-
+## Distributed Performance with DeepSpeed (autoTP)
 ```bash
-# general command:
-deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model <MODEL_ID> --dtype float32 --ipex --jit --tasks <TASK_NAME> --accuracy-only
+unset KMP_AFFINITY
 
-# An example of llama2 7b model:
-deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model  meta-llama/Llama-2-7b-hf --dtype float32 --ipex --jit --tasks lambada_openai --accuracy-only
-```
-### BF16:
+# Get prompt file to the path of scripts
+export WORK_DIR=./
+cd distributed
+mv PATH/TO/prompt.json ./
 
-```bash
-# general command:
-deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model <MODEL_ID> --dtype bfloat16 --ipex --jit --tasks <TASK_NAME> --accuracy-only
+# Run GPTJ/LLAMA/OPT/Falcon with bfloat16 DeepSpeed
+deepspeed --bind_cores_to_rank run_generation_with_deepspeed.py --benchmark -m <MODEL_ID> --dtype bfloat16 --ipex --deployment-mode
 
-# An example of llama2 7b model:
-deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model  meta-llama/Llama-2-7b-hf --dtype bfloat16 --ipex --jit --tasks lambada_openai --accuracy-only
+# Run GPT-NeoX with ipex weight only quantization
+deepspeed --bind_cores_to_rank run_generation_with_deepspeed.py --benchmark -m EleutherAI/gpt-neox-20b --dtype float32 --ipex --ipex-weight-only-quantization --deployment-mode
 ```
 
-### Weight-only quantization:
-
+## Distributed Accuracy with DeepSpeed (autoTP)
 ```bash
-# general command:
-deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model <MODEL_ID> --int8-bf16-mixed --ipex --jit --tasks <TASK_NAME> --accuracy-only --ipex-weight-only-quantization
-# note that GPT-NEOX please remove "--int8-bf16-mixed" and add "--dtype float32" for accuracy concerns
+# Run distributed accuracy with 2 ranks of one node for bfloat16 with ipex and jit 
+source ${ONECCL_DIR}/build/_install/env/setvars.sh
 
-# An example of llama2 7b model:
-deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model  meta-llama/Llama-2-7b-hf --int8-bf16-mixed --ipex --jit --tasks lambada_openai --accuracy-only --ipex-weight-only-quantization
-```
+export LD_PRELOAD=${CONDA_PREFIX}/lib/libiomp5.so:${CONDA_PREFIX}/lib/libtcmalloc.so
+export LD_LIBRARY_PATH=${ONECCL_DIR}/lib:$LD_LIBRARY_PATH
+unset KMP_AFFINITY
 
-## How to Shard model for Distributed tests with DeepSpeed (autoTP)
+deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model <MODEL_ID> --dtype bfloat16 --ipex --jit --tasks <TASK_NAME> --accuracy-only
 
-To save memory usage, we could shard the model weights under the local path before we launch distributed tests with DeepSpeed.
+# with weight only quantization
 
-```
-cd ./utils
-# general command:
-python create_shard_model.py -m <MODEL ID>  --save-path <SHARD MODEL NEW PATH>
-# After sharding the model, using -m <SHARD MODEL NEW PATH> in later tests
+deepspeed  --num_gpus 2 --master_addr `hostname -I | sed -e 's/\s.*$//'` --bind_cores_to_rank run_accuracy_with_deepspeed.py  --model <MODEL_ID> --int8-bf16-mixed --ipex --jit --tasks <TASK_NAME> --accuracy-only --ipex-weight-only-quantization
 
-# An example of llama2 7b:
-python create_shard_model.py meta-llama/Llama-2-7b-hf --save-path ./local_llama2_7b
 ```
-
-# Miscellaneous Tips
-
-(1) Please be aware that we need to set argument `torchscript=True` when loading model with `from_pretrained()` function if we are running the script in `deployment_mode`.
-
-(2) We can build up LLM services optimized by Intel® Extension for PyTorch\* with Triton Server. Please refer [here](../../../serving/triton/README.md) for best practice.
-
-(3) The LLM inference methods introduced in this page can be well applied for AWS. We can just follow the above instructions and enjoy the boosted performance of LLM with Intel® Extension for PyTorch\* optimizations on the AWS instances.
diff --git a/examples/cpu/inference/python/llm/distributed/run_accuracy_with_deepspeed.py b/examples/cpu/inference/python/llm/distributed/run_accuracy_with_deepspeed.py
index 7e7130dd1..12f676844 100644
--- a/examples/cpu/inference/python/llm/distributed/run_accuracy_with_deepspeed.py
+++ b/examples/cpu/inference/python/llm/distributed/run_accuracy_with_deepspeed.py
@@ -25,7 +25,6 @@ MODEL_CLASSES = {
     "opt": (AutoModelForCausalLM, AutoTokenizer),
     "llama": (AutoModelForCausalLM, LlamaTokenizer),
     "falcon": (AutoModelForCausalLM, AutoTokenizer),
-    "codegen": (AutoModelForCausalLM, AutoTokenizer),
     "auto": (AutoModelForCausalLM, AutoTokenizer),
 }
 
@@ -73,24 +72,17 @@ parser.add_argument(
 )
 parser.add_argument(
     "--lowp-mode",
-    choices=["AUTO", "BF16", "FP32", "INT8", "FP16"],
-    default="AUTO",
+    choices=["BF16", "FP32", "INT8", "FP16"],
+    default="BF16",
     type=str,
-    help="low precision mode for weight only quantization. "
-         "It indicates data type for computation for speedup at the cost "
-         "of accuracy. Unrelated to activation or weight data type."
-         "It is not supported yet to use lowp_mode=INT8 for INT8 weight, "
-         "falling back to lowp_mode=BF16 implicitly in this case."
-         "If set to AUTO, lowp_mode is determined by weight data type: "
-         "lowp_mode=BF16 is used for INT8 weight "
-         "and lowp_mode=INT8 used for INT4 weight",
+    help="low precision mode for weight only quantization",
 )
 parser.add_argument(
     "--weight-dtype",
     choices=["INT8", "INT4"],
     default="INT8",
     type=str,
-    help="weight data type for weight only quantization. Unrelated to activation data type or lowp-mode.",
+    help="weight dtype for weight only quantization",
 )
 
 args = parser.parse_args()
@@ -186,8 +178,8 @@ if args.accuracy_only:
                 )
 
             if world_size == 1 or model_type == "falcon":
-                self.model = model_class[0].from_pretrained(
-                    model_id,
+                model = model_class[0].from_pretrained(
+                    model_name,
                     config=config,
                     low_cpu_mem_usage=True,
                     torch_dtype=load_dtype,
@@ -220,8 +212,8 @@ if args.accuracy_only:
                 print(*msg)
 
             def get_repo_root(model_name_or_path):
-                if os.path.exists(model_name_or_path):
-                    # local path
+                local_prefix = ("/", "./", "../")
+                if model_name_or_path.startswith(local_prefix):
                     return model_name_or_path
                 # checks if online or not
                 if is_offline_mode():
@@ -303,13 +295,8 @@ if args.accuracy_only:
                         lowp_mode = ipex.quantization.WoqLowpMode.NONE
                     elif args.lowp_mode == "FP16":
                         lowp_mode = ipex.quantization.WoqLowpMode.FP16
-                    elif args.lowp_mode == "BF16":
+                    else:
                         lowp_mode = ipex.quantization.WoqLowpMode.BF16
-                    else:  # AUTO
-                        if weight_dtype == torch.quint4x2:
-                            lowp_mode = ipex.quantization.WoqLowpMode.INT8
-                        else:
-                            lowp_mode = ipex.quantization.WoqLowpMode.BF16
 
                     qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
                         weight_dtype=weight_dtype, lowp_mode=lowp_mode
@@ -337,7 +324,7 @@ if args.accuracy_only:
                 for text in inputs:
                     input_ids = text.to(self._device)
                     input_bs = inputs.shape[0] * self.num_beams
-                    if re.search("GPTJ", self.base_model.config.architectures[0]) or re.search("codegen", self.base_model.config.architectures[0], re.IGNORECASE):
+                    if re.search("GPTJ", self.base_model.config.architectures[0]):
                         beam_idx_tmp = torch.zeros(
                             (2048, int(input_bs)), dtype=torch.long
                         ).contiguous()
@@ -580,8 +567,9 @@ if args.accuracy_only:
             if self._with_jit and self.iter == 0:
                 with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
                     enabled=True
-                    if args.int8_bf16_mixed or self._dtype == "bfloat16"
+                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
                     else False,
+                    dtype=torch.bfloat16,
                 ):
                     if self._dtype != "int8":
                         if (
@@ -677,8 +665,9 @@ if args.accuracy_only:
             ):
                 with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
                     enabled=True
-                    if args.int8_bf16_mixed or self._dtype == "bfloat16"
+                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
                     else False,
+                    dtype=torch.bfloat16,
                 ):
                     if self._with_jit:
                         output = self.model(
@@ -693,8 +682,9 @@ if args.accuracy_only:
             else:
                 with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
                     enabled=True
-                    if args.int8_bf16_mixed or self._dtype == "bfloat16"
+                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
                     else False,
+                    dtype=torch.bfloat16,
                 ):
                     if self._with_jit:
                         output = self.model(
diff --git a/examples/cpu/inference/python/llm/distributed/run_generation_with_deepspeed.py b/examples/cpu/inference/python/llm/distributed/run_generation_with_deepspeed.py
index 541574430..0d8a2eaba 100644
--- a/examples/cpu/inference/python/llm/distributed/run_generation_with_deepspeed.py
+++ b/examples/cpu/inference/python/llm/distributed/run_generation_with_deepspeed.py
@@ -25,14 +25,11 @@ from transformers import (
 # supported models now
 MODEL_CLASSES = {
     "gpt-j": (AutoModelForCausalLM, AutoTokenizer),
-    "gptj": (AutoModelForCausalLM, AutoTokenizer),
     "gpt-neox": (AutoModelForCausalLM, AutoTokenizer),
-    "gptneox": (AutoModelForCausalLM, AutoTokenizer),
     "llama": (AutoModelForCausalLM, LlamaTokenizer),
     "opt": (AutoModelForCausalLM, AutoTokenizer),
     "falcon": (AutoModelForCausalLM, AutoTokenizer),
     "chatglm": (AutoModelForCausalLM, AutoTokenizer),
-    "codegen": (AutoModelForCausalLM, AutoTokenizer),
     "auto": (AutoModelForCausalLM, AutoTokenizer),
 }
 
@@ -53,12 +50,22 @@ parser.add_argument(
     default="EleutherAI/gpt-j-6b",
     help="the huggingface mdoel id",
 )
+parser.add_argument(
+    "--device",
+    type=str,
+    choices=["cpu"],
+    help="cpu",
+    default="cpu",
+)
 parser.add_argument(
     "--dtype",
     type=str,
-    help="float16 or bfloat16",
-    choices=["bfloat16", "float32"],
-    default="bfloat16",
+    help="float16 or bfloat16 or int8",
+    choices=["int8", "float16", "bfloat16", "float32"],
+    default="float16",
+)
+parser.add_argument(
+    "--local_rank", required=False, type=int, help="used by dist launchers"
 )
 parser.add_argument(
     "--batch-size", "--batch-size", default=1, type=int, help="batch size"
@@ -83,36 +90,27 @@ parser.add_argument(
     action="store_true",
     help="use ipex weight-only quantization",
 )
-parser.add_argument(
-    "--local_rank", required=False, type=int, help="used by dist launchers"
-)
 parser.add_argument(
     "--int8-bf16-mixed",
     action="store_true",
-    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
+    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like EMR)",
 )
+parser.add_argument("--jit", action="store_true")
 parser.add_argument("--print-memory", action="store_true")
 parser.add_argument("--token-latency", action="store_true")
 parser.add_argument(
     "--lowp-mode",
-    choices=["AUTO", "BF16", "FP32", "INT8", "FP16"],
-    default="AUTO",
+    choices=["BF16", "FP32", "INT8", "FP16"],
+    default="BF16",
     type=str,
-    help="low precision mode for weight only quantization. "
-         "It indicates data type for computation for speedup at the cost "
-         "of accuracy. Unrelated to activation or weight data type."
-         "It is not supported yet to use lowp_mode=INT8 for INT8 weight, "
-         "falling back to lowp_mode=BF16 implicitly in this case."
-         "If set to AUTO, lowp_mode is determined by weight data type: "
-         "lowp_mode=BF16 is used for INT8 weight "
-         "and lowp_mode=INT8 used for INT4 weight",
+    help="low precision mode for weight only quantization",
 )
 parser.add_argument(
     "--weight-dtype",
     choices=["INT8", "INT4"],
     default="INT8",
     type=str,
-    help="weight data type for weight only quantization. Unrelated to activation data type or lowp-mode.",
+    help="weight dtype for weight only quantization",
 )
 parser.add_argument(
     "--config-file", default=None, type=str, help="specific configuration file"
@@ -155,8 +153,8 @@ def print_rank0(*msg):
 
 # Model loading and instantiating on GPUs
 def get_repo_root(model_name_or_path):
-    if os.path.exists(model_name_or_path):
-        # local path
+    local_prefix = ("/", "./", "../")
+    if model_name_or_path.startswith(local_prefix):
         return model_name_or_path
     # checks if online or not
     if is_offline_mode():
@@ -201,10 +199,16 @@ if args.int8_bf16_mixed:
     load_dtype = torch.bfloat16
     infer_dtype = torch.bfloat16
 else:
-    if args.dtype == "bfloat16":
+    if args.dtype == "float16":
+        load_dtype = torch.half
+        infer_dtype = torch.half
+    elif args.dtype == "bfloat16":
         load_dtype = torch.bfloat16
         infer_dtype = torch.bfloat16
-    else:
+    elif args.dtype == "int8":
+        load_dtype = torch.half
+        infer_dtype = torch.int8
+    elif args.dtype == "float32":
         load_dtype = torch.float32
         infer_dtype = torch.float32
 
@@ -246,9 +250,6 @@ else:
 if not hasattr(config, "text_max_length") and args.prompt is None:
     config.text_max_length = int(args.input_tokens) + int(args.max_new_tokens)
 
-if not hasattr(config, "lm_head_generation"):
-    config.lm_head_generation = True
-
 # XXX: can't automatically derive dtype via config's `from_pretrained`
 # dtype = torch.bfloat16 if model_name in ["bigscience/bloom", "bigscience/bigscience-small-testing"] else torch.float16
 
@@ -352,13 +353,8 @@ if args.ipex:
             lowp_mode = ipex.quantization.WoqLowpMode.NONE
         elif args.lowp_mode == "FP16":
             lowp_mode = ipex.quantization.WoqLowpMode.FP16
-        elif args.lowp_mode == "BF16":
+        else:
             lowp_mode = ipex.quantization.WoqLowpMode.BF16
-        else:  # AUTO
-            if weight_dtype == torch.quint4x2:
-                lowp_mode = ipex.quantization.WoqLowpMode.INT8
-            else:
-                lowp_mode = ipex.quantization.WoqLowpMode.BF16
 
         qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
             weight_dtype=weight_dtype, lowp_mode=lowp_mode
@@ -382,10 +378,6 @@ input_sentences = []
 current_path = pathlib.Path(__file__).parent.resolve()
 with open(str(current_path) + "/prompt.json") as f:
     prompt_pool = json.load(f)
-if model_type == "gptj":
-    model_type = "gpt-j"
-if model_type == "gptneox":
-    model_type = "gpt-neox"
 if args.prompt is not None:
     input_sentences.append(args.prompt)
 elif model_type == "auto":
diff --git a/examples/cpu/inference/python/llm/run.py b/examples/cpu/inference/python/llm/run.py
index 4de622383..2f67ed142 100644
--- a/examples/cpu/inference/python/llm/run.py
+++ b/examples/cpu/inference/python/llm/run.py
@@ -54,7 +54,7 @@ def main(args_in: Optional[List[str]] = None) -> None:
     parser.add_argument(
         "--int8-bf16-mixed",
         action="store_true",
-        help="by default static quant is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
+        help="by default static quant is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like EMR)",
     )
     parser.add_argument("--quantized-model-path", default="", help="path to the quantized model file")
     parser.add_argument("--qconfig-summary-file", default="", help="qconfig for static quantization")
diff --git a/examples/cpu/inference/python/llm/single_instance/run_accuracy.py b/examples/cpu/inference/python/llm/single_instance/run_accuracy.py
index 6967ccba6..bf71f9d91 100644
--- a/examples/cpu/inference/python/llm/single_instance/run_accuracy.py
+++ b/examples/cpu/inference/python/llm/single_instance/run_accuracy.py
@@ -17,7 +17,6 @@ MODEL_CLASSES = {
     "opt": (AutoModelForCausalLM, AutoTokenizer),
     "llama": (AutoModelForCausalLM, LlamaTokenizer),
     "falcon": (AutoModelForCausalLM, AutoTokenizer),
-    "codegen": (AutoModelForCausalLM, AutoTokenizer),
     "auto": (AutoModelForCausalLM, AutoTokenizer),
 }
 
@@ -125,27 +124,13 @@ if args.accuracy_only:
                     config, torchscript=with_jit, trust_remote_code=True
                 )
 
-            if self._dtype == "int8":
-                try:
-                    with ipex.OnDevice(dtype=torch.float, device="meta"):
-                        self.model = AutoModelForCausalLM.from_config(self.config)
-                except (RuntimeError, AttributeError) as e:
-                    print('Warning: Loading model to meta device failed:', e)
-                    self.model = model_class[0].from_pretrained(
-                        model_id,
-                        low_cpu_mem_usage=True,
-                        config=self.config,
-                        torch_dtype=load_dtype,
-                        trust_remote_code=True,
-                    )
-            else:
-                self.model = model_class[0].from_pretrained(
-                    model_id,
-                    low_cpu_mem_usage=True,
-                    config=self.config,
-                    torch_dtype=load_dtype,
-                    trust_remote_code=True,
-                )
+            self.model = model_class[0].from_pretrained(
+                model_id,
+                low_cpu_mem_usage=True,
+                config=self.config,
+                torch_dtype=load_dtype,
+                trust_remote_code=True,
+            )
 
             self.model = self.model.eval()
 
@@ -173,7 +158,7 @@ if args.accuracy_only:
                 for text in inputs:
                     input_ids = text.to(self._device)
                     input_bs = inputs.shape[0] * self.num_beams
-                    if re.search("GPTJ", self.base_model.config.architectures[0]) or re.search("codegen", self.base_model.config.architectures[0], re.IGNORECASE):
+                    if re.search("GPTJ", self.base_model.config.architectures[0]):
                         beam_idx_tmp = torch.zeros(
                             (2048, int(input_bs)), dtype=torch.long
                         ).contiguous()
@@ -417,8 +402,9 @@ if args.accuracy_only:
             if self._with_jit and self.iter == 0:
                 with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
                     enabled=True
-                    if args.int8_bf16_mixed or self._dtype == "bfloat16"
+                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
                     else False,
+                    dtype=torch.bfloat16,
                 ):
                     if self._dtype != "int8":
                         if (
@@ -514,8 +500,9 @@ if args.accuracy_only:
             ):
                 with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
                     enabled=True
-                    if args.int8_bf16_mixed or self._dtype == "bfloat16"
+                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
                     else False,
+                    dtype=torch.bfloat16,
                 ):
                     if self._with_jit:
                         output = self.model(
@@ -530,8 +517,9 @@ if args.accuracy_only:
             else:
                 with torch.inference_mode(), torch.no_grad(), torch.cpu.amp.autocast(
                     enabled=True
-                    if args.int8_bf16_mixed or self._dtype == "bfloat16"
+                    if args.int8_bf16_mixed or self._dtype == torch.bfloat16
                     else False,
+                    dtype=torch.bfloat16,
                 ):
                     if self._with_jit:
                         output = self.model(
diff --git a/examples/cpu/inference/python/llm/single_instance/run_codegen_quantization.py b/examples/cpu/inference/python/llm/single_instance/run_codegen_quantization.py
index 203229f99..7c1d3df94 100644
--- a/examples/cpu/inference/python/llm/single_instance/run_codegen_quantization.py
+++ b/examples/cpu/inference/python/llm/single_instance/run_codegen_quantization.py
@@ -28,7 +28,7 @@ parser.add_argument("--int8", action="store_true")
 parser.add_argument(
     "--int8-bf16-mixed",
     action="store_true",
-    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
+    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like EMR)",
 )
 parser.add_argument("--quantized-model-path", default="./saved_results/best_model.pt")
 parser.add_argument("--benchmark", action="store_true")
diff --git a/examples/cpu/inference/python/llm/single_instance/run_falcon_quantization.py b/examples/cpu/inference/python/llm/single_instance/run_falcon_quantization.py
index 578e94162..7e9a01f75 100644
--- a/examples/cpu/inference/python/llm/single_instance/run_falcon_quantization.py
+++ b/examples/cpu/inference/python/llm/single_instance/run_falcon_quantization.py
@@ -28,7 +28,7 @@ parser.add_argument("--int8", action="store_true")
 parser.add_argument(
     "--int8-bf16-mixed",
     action="store_true",
-    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
+    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like EMR)",
 )
 parser.add_argument("--quantized-model-path", default="./saved_results/best_model.pt")
 parser.add_argument("--benchmark", action="store_true")
@@ -66,17 +66,6 @@ parser.add_argument(
          " data type or lowp-mode. If `--low-precision-checkpoint` is given, weight"
          " data type is always INT4 and this argument is not needed.",
 )
-parser.add_argument(
-    "--group-size",
-    default=-1,
-    type=int,
-    help="For weight-only quantization only. Specifies the group size along"
-         " input channel for block-wise quantization of weight. It must be a"
-         " positive power of 2 or -1. If it is -1, weight is quantized per"
-         " output channel. Otherwise, weight is quantized per block with block size"
-         " = [1, group_size]. If `--low-precision-checkpoint` is given, group"
-         " size is determined automatically and this argument has no effect.",
-)
 parser.add_argument(
     "--low-precision-checkpoint",
     default="",
@@ -85,20 +74,6 @@ parser.add_argument(
          " modified weights, scales, zero points, etc. For better accuracy of weight only"
          " quantization with INT4 weight."
 )
-parser.add_argument(
-    "--act-quant-mode",
-    choices=["PER_TENSOR", "PER_IC_BLOCK", "PER_BATCH", "PER_BATCH_IC_BLOCK"],
-    default="PER_IC_BLOCK",
-    type=str,
-    help="Quantization mode for activation with different granularity. "
-         "For lowp-mode=INT8 only. For other cases, it has no effect. "
-         "Assume the activation tensor has shape batch_size x input_channel. "
-         "PER_TENSOR(0): quantize per tensor; "
-         "PER_IC_BLOCK(1): quantize per group along IC with group size = IC_BLOCK; "
-         "PER_BATCH(2): quantize per batch; "
-         "PER_BATCH_IC_BLOCK(3): quantize per block of size 1 x IC_BLOCK. "
-         "IC_BLOCK is determined by IC automatically."
-)
 args = parser.parse_args()
 
 
@@ -205,25 +180,9 @@ if args.ipex_weight_only_quantization:
         else:
             lowp_mode = ipex.quantization.WoqLowpMode.BF16
 
-    try:
-        act_quant_mode_dict = {
-            "PER_TENSOR": ipex.quantization.WoqActQuantMode.PER_TENSOR,
-            "PER_IC_BLOCK": ipex.quantization.WoqActQuantMode.PER_IC_BLOCK,
-            "PER_BATCH": ipex.quantization.WoqActQuantMode.PER_BATCH,
-            "PER_BATCH_IC_BLOCK": ipex.quantization.WoqActQuantMode.PER_BATCH_IC_BLOCK,
-        }
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            weight_dtype=weight_dtype,
-            lowp_mode=lowp_mode,
-            act_quant_mode=act_quant_mode_dict[args.act_quant_mode],
-            group_size=args.group_size
-        )
-    except:
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            weight_dtype=weight_dtype,
-            lowp_mode=lowp_mode,
-        )
-
+    qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
+        weight_dtype=weight_dtype, lowp_mode=lowp_mode
+    )
     if args.low_precision_checkpoint != "":
         low_precision_checkpoint = torch.load(args.low_precision_checkpoint)
     else:
diff --git a/examples/cpu/inference/python/llm/single_instance/run_generation.py b/examples/cpu/inference/python/llm/single_instance/run_generation.py
index 76dacc489..66a690fcb 100644
--- a/examples/cpu/inference/python/llm/single_instance/run_generation.py
+++ b/examples/cpu/inference/python/llm/single_instance/run_generation.py
@@ -19,7 +19,6 @@ MODEL_CLASSES = {
     "llama": (AutoModelForCausalLM, LlamaTokenizer),
     "opt": (AutoModelForCausalLM, AutoTokenizer),
     "falcon": (AutoModelForCausalLM, AutoTokenizer),
-    "codegen": (AutoModelForCausalLM, AutoTokenizer),
     "auto": (AutoModelForCausalLM, AutoTokenizer),
 }
 
@@ -99,10 +98,6 @@ else:
     )
 if not hasattr(config, "text_max_length") and args.prompt is None:
     config.text_max_length = int(args.input_tokens) + int(args.max_new_tokens)
-
-if not hasattr(config, "lm_head_generation"):
-    config.lm_head_generation = True
-
 model = model_class[0].from_pretrained(
     args.model_id,
     torch_dtype=amp_dtype,
diff --git a/examples/cpu/inference/python/llm/single_instance/run_gpt-j_quantization.py b/examples/cpu/inference/python/llm/single_instance/run_gpt-j_quantization.py
index f21323346..2198a96b9 100644
--- a/examples/cpu/inference/python/llm/single_instance/run_gpt-j_quantization.py
+++ b/examples/cpu/inference/python/llm/single_instance/run_gpt-j_quantization.py
@@ -20,7 +20,6 @@ parser.add_argument(
 parser.add_argument(
     "--max-new-tokens", default=32, type=int, help="output max new tokens"
 )
-parser.add_argument("--dataset", nargs="?", default="NeelNanda/pile-10k")
 parser.add_argument("--output-dir", nargs="?", default="./saved_results")
 parser.add_argument(
     "--ipex-weight-only-quantization",
@@ -29,14 +28,12 @@ parser.add_argument(
 )
 parser.add_argument("--int8", action="store_true")
 parser.add_argument("--ipex-smooth-quant", action="store_true")
-parser.add_argument("--alpha", default=1.0, type=float, help="alpha value for smoothquant")
 parser.add_argument(
     "--int8-bf16-mixed",
     action="store_true",
-    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
+    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like EMR)",
 )
 parser.add_argument("--quantized-model-path", default="./saved_results/best_model.pt")
-parser.add_argument("--qconfig-summary-file", default="", help="qconfig for static quantization")
 parser.add_argument("--benchmark", action="store_true")
 parser.add_argument("--input-tokens", default="32", type=str)
 parser.add_argument("--prompt", default=None, type=str)
@@ -69,38 +66,13 @@ parser.add_argument(
          " data type or lowp-mode. If `--low-precision-checkpoint` is given, weight"
          " data type is always INT4 and this argument is not needed.",
 )
-parser.add_argument(
-    "--group-size",
-    default=-1,
-    type=int,
-    help="For weight-only quantization only. Specifies the group size along"
-         " input channel for block-wise quantization of weight. It must be a"
-         " positive power of 2 or -1. If it is -1, weight is quantized per"
-         " output channel. Otherwise, weight is quantized per block with block size"
-         " = [1, group_size]. If `--low-precision-checkpoint` is given, group"
-         " size is determined automatically and this argument has no effect.",
-)
 parser.add_argument(
     "--low-precision-checkpoint",
     default="",
     type=str,
     help="Low precision checkpoint file generated by calibration, such as GPTQ. It contains"
          " modified weights, scales, zero points, etc. For better accuracy of weight only"
-         " quantization with INT4 weight.",
-)
-parser.add_argument(
-    "--act-quant-mode",
-    choices=["PER_TENSOR", "PER_IC_BLOCK", "PER_BATCH", "PER_BATCH_IC_BLOCK"],
-    default="PER_IC_BLOCK",
-    type=str,
-    help="Quantization mode for activation with different granularity. "
-         "For lowp-mode=INT8 only. For other cases, it has no effect. "
-         "Assume the activation tensor has shape batch_size x input_channel. "
-         "PER_TENSOR(0): quantize per tensor; "
-         "PER_IC_BLOCK(1): quantize per group along IC with group size = IC_BLOCK; "
-         "PER_BATCH(2): quantize per batch; "
-         "PER_BATCH_IC_BLOCK(3): quantize per block of size 1 x IC_BLOCK. "
-         "IC_BLOCK is determined by IC automatically."
+         " quantization with INT4 weight."
 )
 args = parser.parse_args()
 
@@ -196,25 +168,9 @@ if args.ipex_weight_only_quantization:
         else:
             lowp_mode = ipex.quantization.WoqLowpMode.BF16
 
-    try:
-        act_quant_mode_dict = {
-            "PER_TENSOR": ipex.quantization.WoqActQuantMode.PER_TENSOR,
-            "PER_IC_BLOCK": ipex.quantization.WoqActQuantMode.PER_IC_BLOCK,
-            "PER_BATCH": ipex.quantization.WoqActQuantMode.PER_BATCH,
-            "PER_BATCH_IC_BLOCK": ipex.quantization.WoqActQuantMode.PER_BATCH_IC_BLOCK,
-        }
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            weight_dtype=weight_dtype,
-            lowp_mode=lowp_mode,
-            act_quant_mode=act_quant_mode_dict[args.act_quant_mode],
-            group_size=args.group_size
-        )
-    except:
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            weight_dtype=weight_dtype,
-            lowp_mode=lowp_mode,
-        )
-
+    qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
+        weight_dtype=weight_dtype, lowp_mode=lowp_mode
+    )
     if args.low_precision_checkpoint != "":
         low_precision_checkpoint = torch.load(args.low_precision_checkpoint)
     else:
@@ -246,159 +202,153 @@ if args.ipex_weight_only_quantization:
         self_jit.save(args.output_dir + "/best_model.pt")
 
 elif args.ipex_smooth_quant:
-    if args.qconfig_summary_file != "":
-        qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping(alpha=args.alpha)
-        user_model = ipex.optimize_transformers(
-            user_model.eval(),
-            dtype=amp_dtype,
-            quantization_config=qconfig,
-            qconfig_summary_file=args.qconfig_summary_file,
-            inplace=True,
-            deployment_mode=True,
-        )
-        pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True)
-        user_model.trace_graph.save(args.output_dir + "/best_model.pt")
-    else:
-        class Evaluator:
-            def __init__(
-                self, dataset, tokenizer, args, batch_size=1, pad_val=1, pad_max=512
-            ):
-                self.dataset = dataset
-                self.tokenizer = tokenizer
-                self.batch_size = batch_size
-                self.pad_val = pad_val
-                self.pad_max = pad_max
-                self.args = args
-    
-                # tokenize the dataset
-                self.dataset = self.dataset.map(self.tokenize_function, batched=True)
-                self.dataset.set_format(type="torch", columns=["input_ids"])
-    
-            @torch.no_grad()
-            def tokenize_function(self, examples):
-                if "prompt" in examples:
-                    example = self.tokenizer(examples["prompt"])
-                elif "text" in examples:
-                    example = self.tokenizer(examples["text"])
-                elif "code" in examples:
-                    example = self.tokenizer(examples["code"])
-                return example
-    
-            @torch.no_grad()
-            def collate_batch(self, batch):
-                input_ids_padded = []
-                last_ind = []
-                past_key_values = []
-                attention_mask_padded = []
-                position_ids_padded = []
-    
-                for text in batch:
-                    input_ids = text["input_ids"]
-                    input_ids = (
-                        input_ids[: int(self.pad_max)]
-                        if len(input_ids) > int(self.pad_max)
-                        else input_ids
-                    )
-                    pad_len = self.pad_max - input_ids.shape[0]
-                    last_ind.append(input_ids.shape[0] - 1)
-                    attention_mask = torch.ones(len(input_ids))
-                    position_ids = torch.arange(len(input_ids))
-                    input_ids_padded.append(input_ids)
-                    attention_mask_padded.append(attention_mask)
-                    position_ids_padded.append(position_ids)
-                    # dummy past key value
-                    beam_idx_tmp = torch.zeros(
-                        (2048, int(self.args.batch_size * num_beams)), dtype=torch.long
-                    ).contiguous()
-                    past_key_value = [
-                        (
-                            torch.zeros(1, 0, 0, 1, dtype=torch.long).contiguous(),
-                            torch.zeros([1, 16, 1, 256]).contiguous(),
-                            torch.zeros([1, 16, 1, 256]).contiguous(),
-                            beam_idx_tmp,
-                        )
-                        for i in range(28)
-                    ]
-    
-                return (
+
+    class Evaluator:
+        def __init__(
+            self, dataset, tokenizer, args, batch_size=8, pad_val=1, pad_max=196
+        ):
+            self.dataset = dataset
+            self.tokenizer = tokenizer
+            self.batch_size = batch_size
+            self.pad_val = pad_val
+            self.pad_max = pad_max
+            self.args = args
+
+            # tokenize the dataset
+            self.dataset = self.dataset.map(self.tokenize_function, batched=True)
+            self.dataset.set_format(type="torch", columns=["input_ids"])
+
+        @torch.no_grad()
+        def tokenize_function(self, examples):
+            example = self.tokenizer(examples["text"])
+            return example
+
+        @torch.no_grad()
+        def collate_batch(self, batch):
+            input_ids_padded = []
+            last_ind = []
+            past_key_values = []
+            attention_mask_padded = []
+            position_ids_padded = []
+
+            for text in batch:
+                input_ids = text["input_ids"]
+                pad_len = self.pad_max - input_ids.shape[0]
+                last_ind.append(input_ids.shape[0] - 1)
+
+                attention_mask = torch.ones(len(input_ids))
+
+                position_ids = torch.arange(len(input_ids))
+
+                input_ids = pad(input_ids, (0, pad_len), value=self.pad_val)
+                input_ids_padded.append(input_ids)
+                attention_mask = pad(attention_mask, (0, pad_len), value=0)
+                attention_mask_padded.append(attention_mask)
+                position_ids = pad(position_ids, (0, pad_len), value=self.pad_val)
+
+                position_ids_padded.append(position_ids)
+                # dummy past key value
+                beam_idx_tmp = torch.zeros(
+                    (2048, int(self.args.batch_size * num_beams)), dtype=torch.long
+                ).contiguous()
+                past_key_value = [
                     (
-                        torch.vstack(input_ids_padded),
-                        torch.vstack(attention_mask_padded),
-                        torch.vstack(position_ids_padded),
-                        tuple(past_key_value),
-                    ),
-                    torch.tensor(last_ind),
-                )
-    
-        calib_dataset = load_dataset(args.dataset, split="train")
-        calib_evaluator = Evaluator(
-            calib_dataset, tokenizer, args, batch_size=args.batch_size
-        )
-    
-        calib_dataloader = DataLoader(
-            calib_evaluator.dataset,
-            batch_size=args.batch_size,
-            shuffle=False,
-            collate_fn=calib_evaluator.collate_batch,
-        )
-    
-        def calib_func(prepared_model):
-            for i, (
-                (input_ids, attention_mask, position_ids, past_key_values),
-                last_ind,
-            ) in enumerate(calib_dataloader):
-                if i == 512:
-                    break
-                prepared_model(
-                    input_ids=input_ids,
-                    attention_mask=attention_mask,
-                    position_ids=position_ids,
-                    past_key_values=past_key_values,
-                )
-    
-        example_inputs = None
+                        torch.zeros(1, 0, 0, 1, dtype=torch.long).contiguous(),
+                        torch.zeros([1, 16, 1, 256]).contiguous(),
+                        torch.zeros([1, 16, 1, 256]).contiguous(),
+                        beam_idx_tmp,
+                    )
+                    for i in range(28)
+                ]
+
+            return (
+                (
+                    torch.vstack(input_ids_padded),
+                    torch.vstack(attention_mask_padded),
+                    torch.vstack(position_ids_padded),
+                    tuple(past_key_value),
+                ),
+                torch.tensor(last_ind),
+            )
+
+    full_dataset = load_dataset("lambada")
+    dataset = full_dataset["validation"]
+    calib_dataset = full_dataset["train"]
+    evaluator = Evaluator(dataset, tokenizer, args, batch_size=args.batch_size)
+    calib_evaluator = Evaluator(
+        calib_dataset, tokenizer, args, batch_size=args.batch_size
+    )
+
+    calib_dataloader = DataLoader(
+        calib_evaluator.dataset,
+        batch_size=args.batch_size,
+        shuffle=False,
+        collate_fn=evaluator.collate_batch,
+    )
+
+    def calib_func(prepared_model):
         for i, (
             (input_ids, attention_mask, position_ids, past_key_values),
             last_ind,
         ) in enumerate(calib_dataloader):
-            example_inputs = (input_ids, attention_mask, position_ids, past_key_values)
-            break
-        from intel_extension_for_pytorch.quantization import prepare, convert
-    
-        qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping(alpha=args.alpha)
-        user_model = ipex.optimize_transformers(
-            user_model.eval(),
-            dtype=amp_dtype,
-            quantization_config=qconfig,
-            inplace=True,
-            deployment_mode=False,
-        )
-        prepared_model = prepare(
-            user_model.eval(), qconfig, example_inputs=example_inputs, inplace=True
-        )
-        with torch.no_grad():
-            for i, (
-                (input_ids, attention_mask, position_ids, past_key_values),
-                last_ind,
-            ) in enumerate(calib_dataloader):
-                if i == 512:
-                    break
-                prepared_model(
-                    input_ids,
-                    position_ids=position_ids,
-                    attention_mask=attention_mask,
-                    past_key_values=past_key_values,
-                )
-        qconfig_path= args.output_dir + "/best_configure.json"
-        prepared_model.save_qconf_summary(qconf_summary=qconfig_path)
-        with torch.no_grad(), torch.cpu.amp.autocast(
-            enabled=amp_enabled,
-        ):
-            convert_model = convert(prepared_model.eval(), inplace=True).eval()
-            self_jit = torch.jit.trace(convert_model.eval(), example_inputs, strict=False)
-            self_jit = torch.jit.freeze(self_jit.eval())
-            pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True)
-            self_jit.save(args.output_dir + "/best_model.pt")
+            if i == 100:
+                break
+            prepared_model(
+                input_ids=input_ids,
+                attention_mask=attention_mask,
+                position_ids=position_ids,
+                past_key_values=past_key_values,
+            )
+
+    from neural_compressor import PostTrainingQuantConfig, quantization
+
+    qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping()
+    user_model = ipex.optimize_transformers(
+        user_model.eval(),
+        dtype=amp_dtype,
+        quantization_config=qconfig,
+        inplace=True,
+        deployment_mode=False,
+    )
+    op_type_dict = {
+        "add": {"weight": {"dtype": ["fp32"]}, "activation": {"dtype": ["fp32"]}},
+        "linear": {
+            "weight": {
+                "dtype": ["int8"],
+                "scheme": ["sym"],
+                "granularity": ["per_channel"],
+                "algorithm": ["minmax"],
+            },
+            "activation": {
+                "dtype": ["uint8"],
+                "scheme": ["asym"],
+                "granularity": ["per_tensor"],
+                "algorithm": ["kl"],
+            },
+        },
+    }
+
+    excluded_precisions = [] if args.int8_bf16_mixed else ["bf16"]
+    recipes = {"smooth_quant": True, "smooth_quant_args": {"alpha": "auto"}}
+
+    recipes["smooth_quant_args"]["folding"] = True
+
+    print("smooth_quant_args:", recipes)
+    conf = PostTrainingQuantConfig(
+        backend="ipex",
+        excluded_precisions=excluded_precisions,
+        op_type_dict=op_type_dict,
+        recipes=recipes,
+    )
+
+    q_model = quantization.fit(
+        user_model,
+        conf,
+        calib_dataloader=calib_dataloader,
+        calib_func=calib_func,
+    )
+
+    q_model.save(args.output_dir)
 
 if args.benchmark:
     torch._C._jit_set_texpr_fuser_enabled(False)
diff --git a/examples/cpu/inference/python/llm/single_instance/run_gpt-neox_quantization.py b/examples/cpu/inference/python/llm/single_instance/run_gpt-neox_quantization.py
index 489ce9874..e8157476e 100644
--- a/examples/cpu/inference/python/llm/single_instance/run_gpt-neox_quantization.py
+++ b/examples/cpu/inference/python/llm/single_instance/run_gpt-neox_quantization.py
@@ -35,7 +35,7 @@ parser.add_argument("--int8", action="store_true")
 parser.add_argument(
     "--int8-bf16-mixed",
     action="store_true",
-    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
+    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like EMR)",
 )
 parser.add_argument("--quantized-model-path", default="./saved_results/best_model.pt")
 parser.add_argument("--benchmark", action="store_true")
@@ -70,17 +70,6 @@ parser.add_argument(
          " data type or lowp-mode. If `--low-precision-checkpoint` is given, weight"
          " data type is always INT4 and this argument is not needed.",
 )
-parser.add_argument(
-    "--group-size",
-    default=-1,
-    type=int,
-    help="For weight-only quantization only. Specifies the group size along"
-         " input channel for block-wise quantization of weight. It must be a"
-         " positive power of 2 or -1. If it is -1, weight is quantized per"
-         " output channel. Otherwise, weight is quantized per block with block size"
-         " = [1, group_size]. If `--low-precision-checkpoint` is given, group"
-         " size is determined automatically and this argument has no effect.",
-)
 parser.add_argument(
     "--low-precision-checkpoint",
     default="",
@@ -89,20 +78,6 @@ parser.add_argument(
          " modified weights, scales, zero points, etc. For better accuracy of weight only"
          " quantization with INT4 weight."
 )
-parser.add_argument(
-    "--act-quant-mode",
-    choices=["PER_TENSOR", "PER_IC_BLOCK", "PER_BATCH", "PER_BATCH_IC_BLOCK"],
-    default="PER_IC_BLOCK",
-    type=str,
-    help="Quantization mode for activation with different granularity. "
-         "For lowp-mode=INT8 only. For other cases, it has no effect. "
-         "Assume the activation tensor has shape batch_size x input_channel. "
-         "PER_TENSOR(0): quantize per tensor; "
-         "PER_IC_BLOCK(1): quantize per group along IC with group size = IC_BLOCK; "
-         "PER_BATCH(2): quantize per batch; "
-         "PER_BATCH_IC_BLOCK(3): quantize per block of size 1 x IC_BLOCK. "
-         "IC_BLOCK is determined by IC automatically."
-)
 args = parser.parse_args()
 
 
@@ -193,25 +168,9 @@ if args.ipex_weight_only_quantization:
         else:
             lowp_mode = ipex.quantization.WoqLowpMode.BF16
 
-    try:
-        act_quant_mode_dict = {
-            "PER_TENSOR": ipex.quantization.WoqActQuantMode.PER_TENSOR,
-            "PER_IC_BLOCK": ipex.quantization.WoqActQuantMode.PER_IC_BLOCK,
-            "PER_BATCH": ipex.quantization.WoqActQuantMode.PER_BATCH,
-            "PER_BATCH_IC_BLOCK": ipex.quantization.WoqActQuantMode.PER_BATCH_IC_BLOCK,
-        }
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            weight_dtype=weight_dtype,
-            lowp_mode=lowp_mode,
-            act_quant_mode=act_quant_mode_dict[args.act_quant_mode],
-            group_size=args.group_size
-        )
-    except:
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            weight_dtype=weight_dtype,
-            lowp_mode=lowp_mode,
-        )
-
+    qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
+        weight_dtype=weight_dtype, lowp_mode=lowp_mode
+    )
     if args.low_precision_checkpoint != "":
         low_precision_checkpoint = torch.load(args.low_precision_checkpoint)
     else:
diff --git a/examples/cpu/inference/python/llm/single_instance/run_llama_quantization.py b/examples/cpu/inference/python/llm/single_instance/run_llama_quantization.py
index bcaba1a96..9652c8201 100644
--- a/examples/cpu/inference/python/llm/single_instance/run_llama_quantization.py
+++ b/examples/cpu/inference/python/llm/single_instance/run_llama_quantization.py
@@ -20,7 +20,7 @@ parser.add_argument(
 parser.add_argument(
     "--max-new-tokens", default=32, type=int, help="output max new tokens"
 )
-parser.add_argument("--dataset", nargs="?", default="NeelNanda/pile-10k")
+parser.add_argument("--dataset", nargs="?", default="lambada", const="lambada")
 parser.add_argument("--split", nargs="?", default="validation", const="validation")
 parser.add_argument("--output-dir", nargs="?", default="./saved_results")
 parser.add_argument("--ipex-smooth-quant", action="store_true")
@@ -33,17 +33,15 @@ parser.add_argument("--int8", action="store_true")
 parser.add_argument(
     "--int8-bf16-mixed",
     action="store_true",
-    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
+    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like EMR)",
 )
 parser.add_argument("--quantized-model-path", default="./saved_results/best_model.pt")
-parser.add_argument("--qconfig-summary-file", default="", help="qconfig for static quantization")
 parser.add_argument("--benchmark", action="store_true")
 parser.add_argument("--input-tokens", default="32", type=str)
 parser.add_argument("--prompt", default=None, type=str)
 parser.add_argument("--num-iter", default=100, type=int, help="num iter")
 parser.add_argument("--num-warmup", default=10, type=int, help="num warmup")
 parser.add_argument("--batch-size", default=1, type=int, help="batch size")
-parser.add_argument("--alpha", default=0.8, type=float, help="alpha value for smoothquant")
 parser.add_argument("--token-latency", action="store_true")
 parser.add_argument("--greedy", action="store_true")
 parser.add_argument("--profile", action="store_true")
@@ -70,38 +68,13 @@ parser.add_argument(
          " data type or lowp-mode. If `--low-precision-checkpoint` is given, weight"
          " data type is always INT4 and this argument is not needed.",
 )
-parser.add_argument(
-    "--group-size",
-    default=-1,
-    type=int,
-    help="For weight-only quantization only. Specifies the group size along"
-         " input channel for block-wise quantization of weight. It must be a"
-         " positive power of 2 or -1. If it is -1, weight is quantized per"
-         " output channel. Otherwise, weight is quantized per block with block size"
-         " = [1, group_size]. If `--low-precision-checkpoint` is given, group"
-         " size is determined automatically and this argument has no effect.",
-)
 parser.add_argument(
     "--low-precision-checkpoint",
     default="",
     type=str,
     help="Low precision checkpoint file generated by calibration, such as GPTQ. It contains"
          " modified weights, scales, zero points, etc. For better accuracy of weight only"
-         " quantization with INT4 weight.",
-)
-parser.add_argument(
-    "--act-quant-mode",
-    choices=["PER_TENSOR", "PER_IC_BLOCK", "PER_BATCH", "PER_BATCH_IC_BLOCK"],
-    default="PER_IC_BLOCK",
-    type=str,
-    help="Quantization mode for activation with different granularity. "
-         "For lowp-mode=INT8 only. For other cases, it has no effect. "
-         "Assume the activation tensor has shape batch_size x input_channel. "
-         "PER_TENSOR(0): quantize per tensor; "
-         "PER_IC_BLOCK(1): quantize per group along IC with group size = IC_BLOCK; "
-         "PER_BATCH(2): quantize per batch; "
-         "PER_BATCH_IC_BLOCK(3): quantize per block of size 1 x IC_BLOCK. "
-         "IC_BLOCK is determined by IC automatically."
+         " quantization with INT4 weight."
 )
 args = parser.parse_args()
 
@@ -180,139 +153,136 @@ global_past_key_value = [
     for i in range(user_model.config.num_hidden_layers)
 ]
 
+
 if args.ipex_smooth_quant:
-    if args.qconfig_summary_file != "":
-        qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping(alpha=args.alpha)
-        user_model = ipex.optimize_transformers(
-            user_model.eval(),
-            dtype=amp_dtype,
-            quantization_config=qconfig,
-            qconfig_summary_file=args.qconfig_summary_file,
-            inplace=True,
-            deployment_mode=True,
-        )
-        pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True)
-        user_model.trace_graph.save(args.output_dir + "/best_model.pt")
-    else:
-        class Evaluator:
-            def __init__(self, dataset, tokenizer, batch_size=1, pad_val=1, pad_max=512):
-                self.dataset = dataset
-                self.tokenizer = tokenizer
-                self.batch_size = batch_size
-                self.pad_val = pad_val
-                self.pad_max = pad_max
-    
-                # tokenize the dataset
-                self.dataset = self.dataset.map(self.tokenize_function, batched=True)
-                self.dataset.set_format(type="torch", columns=["input_ids"])
-    
-            @torch.no_grad()
-            def tokenize_function(self, examples):
-                if "prompt" in examples:
-                    example = self.tokenizer(examples["prompt"])
-                elif "text" in examples:
-                    example = self.tokenizer(examples["text"])
-                elif "code" in examples:
-                    example = self.tokenizer(examples["code"])
-                return example
-    
-            @torch.no_grad()
-            def collate_batch(self, batch):
-                position_ids_padded = []
-                input_ids_padded = []
-                last_ind = []
-                attention_mask_padded = []
-                for text in batch:
-                    input_ids = text["input_ids"]
-                    input_ids = (
-                        input_ids[: int(self.pad_max)]
-                        if len(input_ids) > int(self.pad_max)
-                        else input_ids
-                    )
-                    last_ind.append(input_ids.shape[0] - 1)
-                    attention_mask = torch.ones(len(input_ids))
-                    position_ids = torch.arange(len(input_ids))
-                    input_ids_padded.append(input_ids)
-                    attention_mask_padded.append(attention_mask)
-                    position_ids_padded.append(position_ids)
-                return (
-                    (
-                        torch.vstack(input_ids_padded),
-                        torch.vstack(attention_mask_padded),
-                        torch.vstack(position_ids_padded),
-                        tuple(global_past_key_value),
-                    ),
-                    torch.tensor(last_ind),
-                )
-    
-        calib_dataset = load_dataset(args.dataset, split="train")
-        user_model.eval()
-        calib_evaluator = Evaluator(calib_dataset, tokenizer, args.batch_size)
-        calib_dataloader = DataLoader(
-            calib_evaluator.dataset,
-            batch_size=args.batch_size,
-            shuffle=False,
-            collate_fn=calib_evaluator.collate_batch,
-        )
-    
-        def calib_func(prepared_model):
-            for i, (
-                (input_ids, attention_mask, position_ids, past_key_values),
-                last_ind,
-            ) in enumerate(calib_dataloader):
-                if i == 512:
-                    break
-                prepared_model(
-                    input_ids,
-                    attention_mask=attention_mask,
-                    position_ids=position_ids,
-                    past_key_values=past_key_values,
+
+    class Evaluator:
+        def __init__(self, dataset, tokenizer, batch_size=8, pad_val=1, pad_max=196):
+            self.dataset = dataset
+            self.tokenizer = tokenizer
+            self.batch_size = batch_size
+            self.pad_val = pad_val
+            self.pad_max = pad_max
+
+            # tokenize the dataset
+            self.dataset = self.dataset.map(self.tokenize_function, batched=True)
+            self.dataset.set_format(type="torch", columns=["input_ids"])
+
+        @torch.no_grad()
+        def tokenize_function(self, examples):
+            example = self.tokenizer(examples["text"])
+            return example
+
+        @torch.no_grad()
+        def collate_batch(self, batch):
+            position_ids_padded = []
+            input_ids_padded = []
+            last_ind = []
+            attention_mask_padded = []
+            for text in batch:
+                # we cut the sentence if it exceeds pad_max, we are using tuned max 196 from gptj model; TODO: tune best pad_max
+                input_ids = (
+                    text["input_ids"]
+                    if text["input_ids"].shape[0] <= self.pad_max
+                    else text["input_ids"][0 : int(self.pad_max - 1)]
                 )
-    
-        example_inputs = None
+                pad_len = self.pad_max - input_ids.shape[0]
+                last_ind.append(input_ids.shape[0] - 1)
+                attention_mask = torch.ones(len(input_ids))
+                position_ids = torch.arange(len(input_ids))
+                input_ids = pad(input_ids, (0, pad_len), value=self.pad_val)
+                input_ids_padded.append(input_ids)
+                attention_mask = pad(attention_mask, (0, pad_len), value=0)
+                attention_mask_padded.append(attention_mask)
+                position_ids = pad(position_ids, (0, pad_len), value=self.pad_val)
+                position_ids_padded.append(position_ids)
+            return (
+                (
+                    torch.vstack(input_ids_padded),
+                    torch.vstack(attention_mask_padded),
+                    torch.vstack(position_ids_padded),
+                    tuple(global_past_key_value),
+                ),
+                torch.tensor(last_ind),
+            )
+
+    full_dataset = load_dataset(args.dataset)
+    dataset = full_dataset["validation"]
+    calib_dataset = full_dataset["train"]
+
+    user_model.eval()
+    evaluator = Evaluator(dataset, tokenizer, args.batch_size)
+    calib_evaluator = Evaluator(calib_dataset, tokenizer, args.batch_size)
+
+    calib_dataloader = DataLoader(
+        calib_evaluator.dataset,
+        batch_size=args.batch_size,
+        shuffle=False,
+        collate_fn=evaluator.collate_batch,
+    )
+
+    test_dataloader = DataLoader(
+        evaluator.dataset,
+        batch_size=args.batch_size,
+        shuffle=False,
+        collate_fn=evaluator.collate_batch,
+    )
+
+    def calib_func(prepared_model):
         for i, (
             (input_ids, attention_mask, position_ids, past_key_values),
             last_ind,
         ) in enumerate(calib_dataloader):
-            example_inputs = (input_ids, attention_mask, position_ids, past_key_values)
-            break
-        from intel_extension_for_pytorch.quantization import prepare, convert
-    
-        qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping(alpha=args.alpha)
-        user_model = ipex.optimize_transformers(
-            user_model.eval(),
-            dtype=amp_dtype,
-            quantization_config=qconfig,
-            inplace=True,
-            deployment_mode=False,
-        )
-        prepared_model = prepare(
-            user_model.eval(), qconfig, example_inputs=example_inputs, inplace=True
-        )
-        with torch.no_grad():
-            for i, (
-                (input_ids, attention_mask, position_ids, past_key_values),
-                last_ind,
-            ) in enumerate(calib_dataloader):
-                if i == 512:
-                    break
-                prepared_model(
-                    input_ids,
-                    position_ids=position_ids,
-                    attention_mask=attention_mask,
-                    past_key_values=past_key_values,
-                )
-        qconfig_path= args.output_dir + "/best_configure.json"
-        prepared_model.save_qconf_summary(qconf_summary=qconfig_path)
-        with torch.no_grad(), torch.cpu.amp.autocast(
-            enabled=amp_enabled,
-        ):
-            convert_model = convert(prepared_model.eval(), inplace=True).eval()
-            self_jit = torch.jit.trace(convert_model.eval(), example_inputs, strict=False)
-            self_jit = torch.jit.freeze(self_jit.eval())
-            pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True)
-            self_jit.save(args.output_dir + "/best_model.pt")
+            if i == 8:
+                break
+            prepared_model(
+                input_ids,
+                attention_mask=attention_mask,
+                position_ids=position_ids,
+                past_key_values=past_key_values,
+            )
+
+    example_inputs = None
+    for i, (
+        (input_ids, attention_mask, position_ids, past_key_values),
+        last_ind,
+    ) in enumerate(calib_dataloader):
+        example_inputs = (input_ids, attention_mask, position_ids, past_key_values)
+        break
+    from intel_extension_for_pytorch.quantization import prepare, convert
 
+    qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping()
+    user_model = ipex.optimize_transformers(
+        user_model.eval(),
+        dtype=amp_dtype,
+        quantization_config=qconfig,
+        inplace=True,
+        deployment_mode=False,
+    )
+    prepared_model = prepare(
+        user_model.eval(), qconfig, example_inputs=example_inputs, inplace=True
+    )
+    with torch.no_grad():
+        for i, (
+            (input_ids, attention_mask, position_ids, past_key_values),
+            last_ind,
+        ) in enumerate(calib_dataloader):
+            if i == 8:
+                break
+            prepared_model(
+                input_ids,
+                position_ids=position_ids,
+                attention_mask=attention_mask,
+                past_key_values=past_key_values,
+            )
+    with torch.no_grad(), torch.cpu.amp.autocast(
+        enabled=amp_enabled,
+    ):
+        convert_model = convert(prepared_model.eval(), inplace=True).eval()
+        self_jit = torch.jit.trace(convert_model.eval(), example_inputs, strict=False)
+        self_jit = torch.jit.freeze(self_jit.eval())
+        pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True)
+        self_jit.save(args.output_dir + "/best_model.pt")
 elif args.ipex_weight_only_quantization:
     weight_dtype = torch.quint4x2 if args.weight_dtype == "INT4" else torch.qint8
 
@@ -329,25 +299,10 @@ elif args.ipex_weight_only_quantization:
             lowp_mode = ipex.quantization.WoqLowpMode.INT8
         else:
             lowp_mode = ipex.quantization.WoqLowpMode.BF16
-    try:
-        act_quant_mode_dict = {
-            "PER_TENSOR": ipex.quantization.WoqActQuantMode.PER_TENSOR,
-            "PER_IC_BLOCK": ipex.quantization.WoqActQuantMode.PER_IC_BLOCK,
-            "PER_BATCH": ipex.quantization.WoqActQuantMode.PER_BATCH,
-            "PER_BATCH_IC_BLOCK": ipex.quantization.WoqActQuantMode.PER_BATCH_IC_BLOCK,
-        }
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            weight_dtype=weight_dtype,
-            lowp_mode=lowp_mode,
-            act_quant_mode=act_quant_mode_dict[args.act_quant_mode],
-            group_size=args.group_size
-        )
-    except:
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            weight_dtype=weight_dtype,
-            lowp_mode=lowp_mode,
-        )
 
+    qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
+        weight_dtype=weight_dtype, lowp_mode=lowp_mode
+    )
     if args.low_precision_checkpoint != "":
         low_precision_checkpoint = torch.load(args.low_precision_checkpoint)
     else:
diff --git a/examples/cpu/inference/python/llm/single_instance/run_opt_quantization.py b/examples/cpu/inference/python/llm/single_instance/run_opt_quantization.py
index ab0dc2f65..51b09c52a 100644
--- a/examples/cpu/inference/python/llm/single_instance/run_opt_quantization.py
+++ b/examples/cpu/inference/python/llm/single_instance/run_opt_quantization.py
@@ -28,7 +28,7 @@ parser.add_argument("--int8", action="store_true")
 parser.add_argument(
     "--int8-bf16-mixed",
     action="store_true",
-    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like SPR)",
+    help="by default it is int8-fp32 mixed, to enable int8 mixed amp bf16 (work on platforms like EMR)",
 )
 parser.add_argument("--quantized-model-path", default="./saved_results/best_model.pt")
 parser.add_argument("--benchmark", action="store_true")
@@ -63,17 +63,6 @@ parser.add_argument(
          " data type or lowp-mode. If `--low-precision-checkpoint` is given, weight"
          " data type is always INT4 and this argument is not needed.",
 )
-parser.add_argument(
-    "--group-size",
-    default=-1,
-    type=int,
-    help="For weight-only quantization only. Specifies the group size along"
-         " input channel for block-wise quantization of weight. It must be a"
-         " positive power of 2 or -1. If it is -1, weight is quantized per"
-         " output channel. Otherwise, weight is quantized per block with block size"
-         " = [1, group_size]. If `--low-precision-checkpoint` is given, group"
-         " size is determined automatically and this argument has no effect.",
-)
 parser.add_argument(
     "--low-precision-checkpoint",
     default="",
@@ -82,20 +71,6 @@ parser.add_argument(
          " modified weights, scales, zero points, etc. For better accuracy of weight only"
          " quantization with INT4 weight."
 )
-parser.add_argument(
-    "--act-quant-mode",
-    choices=["PER_TENSOR", "PER_IC_BLOCK", "PER_BATCH", "PER_BATCH_IC_BLOCK"],
-    default="PER_IC_BLOCK",
-    type=str,
-    help="Quantization mode for activation with different granularity. "
-         "For lowp-mode=INT8 only. For other cases, it has no effect. "
-         "Assume the activation tensor has shape batch_size x input_channel. "
-         "PER_TENSOR(0): quantize per tensor; "
-         "PER_IC_BLOCK(1): quantize per group along IC with group size = IC_BLOCK; "
-         "PER_BATCH(2): quantize per batch; "
-         "PER_BATCH_IC_BLOCK(3): quantize per block of size 1 x IC_BLOCK. "
-         "IC_BLOCK is determined by IC automatically."
-)
 args = parser.parse_args()
 
 
@@ -190,25 +165,9 @@ if args.ipex_weight_only_quantization:
         else:
             lowp_mode = ipex.quantization.WoqLowpMode.BF16
 
-    try:
-        act_quant_mode_dict = {
-            "PER_TENSOR": ipex.quantization.WoqActQuantMode.PER_TENSOR,
-            "PER_IC_BLOCK": ipex.quantization.WoqActQuantMode.PER_IC_BLOCK,
-            "PER_BATCH": ipex.quantization.WoqActQuantMode.PER_BATCH,
-            "PER_BATCH_IC_BLOCK": ipex.quantization.WoqActQuantMode.PER_BATCH_IC_BLOCK,
-        }
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            weight_dtype=weight_dtype,
-            lowp_mode=lowp_mode,
-            act_quant_mode=act_quant_mode_dict[args.act_quant_mode],
-            group_size=args.group_size
-        )
-    except:
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-            weight_dtype=weight_dtype,
-            lowp_mode=lowp_mode,
-        )
-
+    qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
+        weight_dtype=weight_dtype, lowp_mode=lowp_mode
+    )
     if args.low_precision_checkpoint != "":
         low_precision_checkpoint = torch.load(args.low_precision_checkpoint)
     else:
diff --git a/examples/cpu/inference/python/llm/utils/create_shard_model.py b/examples/cpu/inference/python/llm/utils/create_shard_model.py
index c15320f1d..632ccfcfd 100644
--- a/examples/cpu/inference/python/llm/utils/create_shard_model.py
+++ b/examples/cpu/inference/python/llm/utils/create_shard_model.py
@@ -40,29 +40,26 @@ parser.add_argument(
     type=str,
     default="500MB",
 )
-parser.add_argument(
-    "--local_rank", required=False, type=int, default=0, help="used by dist launchers"
-)
 args = parser.parse_args()
 print(args)
-if args.local_rank == 0 :
-    model_type = next(
-        (x for x in MODEL_CLASSES.keys() if x in args.model_id.lower()), "auto"
-    )
-    model_class = MODEL_CLASSES[model_type]
+model_type = next(
+    (x for x in MODEL_CLASSES.keys() if x in args.model_id.lower()), "auto"
+)
+model_class = MODEL_CLASSES[model_type]
 
-    load_dtype = torch.float32
-    if args.dtype == "float16":
-        load_dtype = torch.half
-    elif args.dtype == "bfloat16":
-        load_dtype = torch.bfloat16
+load_dtype = torch.float32
+if args.dtype == "float16":
+    load_dtype = torch.half
+elif args.dtype == "bfloat16":
+    load_dtype = torch.bfloat16
+
+tokenizer = model_class[1].from_pretrained(args.model_id, trust_remote_code=True)
+model = model_class[0].from_pretrained(
+    args.model_id,
+    torch_dtype=load_dtype,
+    low_cpu_mem_usage=True,
+    trust_remote_code=True,
+)
 
-    tokenizer = model_class[1].from_pretrained(args.model_id, trust_remote_code=True)
-    model = model_class[0].from_pretrained(
-        args.model_id,
-        torch_dtype=load_dtype,
-        low_cpu_mem_usage=True,
-        trust_remote_code=True,
-    )
-    model.save_pretrained(save_directory=args.save_path, max_shard_size=args.max_shard_size)
-    tokenizer.save_pretrained(save_directory=args.save_path)
+model.save_pretrained(save_directory=args.save_path, max_shard_size=args.max_shard_size)
+tokenizer.save_pretrained(save_directory=args.save_path)
diff --git a/examples/cpu/inference/python/llm/utils/run_gptq.py b/examples/cpu/inference/python/llm/utils/run_gptq.py
index 9a48120b8..5f871927f 100644
--- a/examples/cpu/inference/python/llm/utils/run_gptq.py
+++ b/examples/cpu/inference/python/llm/utils/run_gptq.py
@@ -19,9 +19,8 @@ parser = argparse.ArgumentParser()
 parser.add_argument(
     "--model", nargs="?", default="EleutherAI/gpt-j-6b"
 )
-parser.add_argument("--dataset", nargs="?", default="NeelNanda/pile-10k")
+parser.add_argument("--dataset", nargs="?", default="lambada", const="lambada")
 parser.add_argument("--output-dir", nargs="?", default="./saved_results")
-parser.add_argument("--group-size", default=128, type=int)
 parser.add_argument("--calib-iters", default=512, type=int,
                     help="calibration iters.")
 args = parser.parse_args()
@@ -101,7 +100,6 @@ class Evaluator:
 def get_user_model():
     from transformers import AutoModelForCausalLM, AutoModel, AutoTokenizer
     torchscript = False
-    model_config = None
     if re.search("llama", args.model.lower()):
         from transformers import LlamaForCausalLM, LlamaTokenizer
         user_model = LlamaForCausalLM.from_pretrained(
@@ -134,8 +132,6 @@ def get_user_model():
             trust_remote_code=True,
         )
         tokenizer = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)
-    elif re.search("falcon", args.model.lower()):
-        assert False, "falcon is not supported yet"
     else:
         user_model = AutoModelForCausalLM.from_pretrained(
             args.model,
@@ -180,7 +176,7 @@ op_type_dict = {
     '.*': {  # re.match
         "weight": {
             'bits': 4,  # only support 4-bit for now
-            'group_size': args.group_size,
+            'group_size': -1,  # only support per-channel for now
             'scheme': 'asym',  # only support asym for now
             'algorithm': 'GPTQ',  # RTN/AWQ/TEQ
         },
@@ -222,6 +218,5 @@ compressed_model = q_model.export_compressed_model(
     scale_dtype=torch.float16,
 )
 Path(args.output_dir).mkdir(parents=True, exist_ok=True)
-output_file_name = f"gptq_checkpoint_g{args.group_size}.pt"
-torch.save(compressed_model.state_dict(), args.output_dir + "/" + output_file_name)
-print('\n Checkpoint saved to', args.output_dir + "/" + output_file_name + "\n")
+torch.save(compressed_model.state_dict(), args.output_dir + "/gptq_checkpoint.pt")
+print('\n Checkpoint saved to', args.output_dir + "/gptq_checkpoint.pt \n")
diff --git a/examples/cpu/inference/python/resnet50_general_inference_script.py b/examples/cpu/inference/python/resnet50_general_inference_script.py
index 0352afe5b..32cf24c1c 100644
--- a/examples/cpu/inference/python/resnet50_general_inference_script.py
+++ b/examples/cpu/inference/python/resnet50_general_inference_script.py
@@ -16,10 +16,10 @@ def inference(model, data):
         print('Inference took {:.2f} ms in average'.format((end - start) / 100 * 1000))
 
 def main(args):
-    model = models.resnet50(weights='ResNet50_Weights.DEFAULT')
+    model = models.resnet50(pretrained=False)
     model.eval()
 
-    data = torch.rand(128, 3, 224, 224)
+    data = torch.rand(1, 3, 224, 224)
 
     import intel_extension_for_pytorch as ipex
 
@@ -33,21 +33,21 @@ def main(args):
     else:  # int8
         from intel_extension_for_pytorch.quantization import prepare, convert
 
-        qconfig = ipex.quantization.default_static_qconfig_mapping
+        qconfig = ipex.quantization.default_static_qconfig
         model = prepare(model, qconfig, example_inputs=data, inplace=False)
 
         # calibration
         n_iter = 100
-        with torch.no_grad():
-            for i in range(n_iter):
-                model(data)
+        for i in range(n_iter):
+            model(data)
 
         model = convert(model)
 
     with torch.cpu.amp.autocast(enabled=args.dtype == 'bfloat16'):
-        with torch.no_grad():
-            model = torch.jit.trace(model, data)
-            model = torch.jit.freeze(model)
+        if args.torchscript:
+            with torch.no_grad():
+                model = torch.jit.trace(model, data)
+                model = torch.jit.freeze(model)
 
         inference(model, data)
 
@@ -55,6 +55,7 @@ if __name__ == '__main__':
     import argparse
     parser = argparse.ArgumentParser()
     parser.add_argument('--dtype', default='float32', choices=['float32', 'bfloat16', 'int8'])
+    parser.add_argument("--torchscript", default=False, action="store_true")
 
     main(parser.parse_args())
 
diff --git a/examples/cpu/inference/python/resnet50_imperative_mode_inference_bf16.py b/examples/cpu/inference/python/resnet50_imperative_mode_inference_bf16.py
index a90e21e4d..fa875e072 100644
--- a/examples/cpu/inference/python/resnet50_imperative_mode_inference_bf16.py
+++ b/examples/cpu/inference/python/resnet50_imperative_mode_inference_bf16.py
@@ -3,7 +3,7 @@ import torchvision.models as models
 
 model = models.resnet50(weights='ResNet50_Weights.DEFAULT')
 model.eval()
-data = torch.rand(128, 3, 224, 224)
+data = torch.rand(1, 3, 224, 224)
 
 #################### code changes ####################  # noqa F401
 import intel_extension_for_pytorch as ipex
diff --git a/examples/cpu/inference/python/resnet50_imperative_mode_inference_fp32.py b/examples/cpu/inference/python/resnet50_imperative_mode_inference_fp32.py
index 30d23f634..f7e039b4e 100644
--- a/examples/cpu/inference/python/resnet50_imperative_mode_inference_fp32.py
+++ b/examples/cpu/inference/python/resnet50_imperative_mode_inference_fp32.py
@@ -3,7 +3,7 @@ import torchvision.models as models
 
 model = models.resnet50(weights='ResNet50_Weights.DEFAULT')
 model.eval()
-data = torch.rand(128, 3, 224, 224)
+data = torch.rand(1, 3, 224, 224)
 
 #################### code changes ####################  # noqa F401
 import intel_extension_for_pytorch as ipex
diff --git a/examples/cpu/inference/python/resnet50_torchdynamo_mode_inference_fp32.py b/examples/cpu/inference/python/resnet50_torchdynamo_mode_inference_fp32.py
index fc1710918..c9ca45ef6 100644
--- a/examples/cpu/inference/python/resnet50_torchdynamo_mode_inference_fp32.py
+++ b/examples/cpu/inference/python/resnet50_torchdynamo_mode_inference_fp32.py
@@ -3,12 +3,12 @@ import torchvision.models as models
 
 model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
 model.eval()
-data = torch.rand(128, 3, 224, 224)
+data = torch.rand(1, 3, 224, 224)
 
 # Experimental Feature
 #################### code changes ####################  # noqa F401
 import intel_extension_for_pytorch as ipex
-model = ipex.optimize(model, weights_prepack=False)
+model = ipex.optimize(model)
 model = torch.compile(model, backend="ipex")
 ######################################################  # noqa F401
 
diff --git a/examples/cpu/inference/python/resnet50_torchscript_mode_inference_bf16.py b/examples/cpu/inference/python/resnet50_torchscript_mode_inference_bf16.py
index 985634559..faf46238e 100644
--- a/examples/cpu/inference/python/resnet50_torchscript_mode_inference_bf16.py
+++ b/examples/cpu/inference/python/resnet50_torchscript_mode_inference_bf16.py
@@ -3,7 +3,7 @@ import torchvision.models as models
 
 model = models.resnet50(weights='ResNet50_Weights.DEFAULT')
 model.eval()
-data = torch.rand(128, 3, 224, 224)
+data = torch.rand(1, 3, 224, 224)
 
 #################### code changes ####################  # noqa F401
 import intel_extension_for_pytorch as ipex
@@ -11,7 +11,7 @@ model = ipex.optimize(model, dtype=torch.bfloat16)
 ######################################################  # noqa F401
 
 with torch.no_grad(), torch.cpu.amp.autocast():
-    model = torch.jit.trace(model, torch.rand(128, 3, 224, 224))
+    model = torch.jit.trace(model, torch.rand(1, 3, 224, 224))
     model = torch.jit.freeze(model)
 
     model(data)
diff --git a/examples/cpu/inference/python/resnet50_torchscript_mode_inference_fp32.py b/examples/cpu/inference/python/resnet50_torchscript_mode_inference_fp32.py
index f69a8cd8a..f4ffe6492 100644
--- a/examples/cpu/inference/python/resnet50_torchscript_mode_inference_fp32.py
+++ b/examples/cpu/inference/python/resnet50_torchscript_mode_inference_fp32.py
@@ -3,7 +3,7 @@ import torchvision.models as models
 
 model = models.resnet50(weights='ResNet50_Weights.DEFAULT')
 model.eval()
-data = torch.rand(128, 3, 224, 224)
+data = torch.rand(1, 3, 224, 224)
 
 #################### code changes ####################  # noqa F401
 import intel_extension_for_pytorch as ipex
@@ -11,7 +11,7 @@ model = ipex.optimize(model)
 ######################################################  # noqa F401
 
 with torch.no_grad():
-    d = torch.rand(128, 3, 224, 224)
+    d = torch.rand(1, 3, 224, 224)
     model = torch.jit.trace(model, d)
     model = torch.jit.freeze(model)
 
diff --git a/examples/cpu/training/single_instance_training_bf16.py b/examples/cpu/training/single_instance_training_bf16.py
index 1c4c2c71b..b1cd9df67 100644
--- a/examples/cpu/training/single_instance_training_bf16.py
+++ b/examples/cpu/training/single_instance_training_bf16.py
@@ -26,10 +26,7 @@ model = torchvision.models.resnet50()
 criterion = torch.nn.CrossEntropyLoss()
 optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)
 model.train()
-
 model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)
-# Uncomment the code below to enable experimental feature torch.compile
-# model = torch.compile(model, backend="ipex")
 
 for batch_idx, (data, target) in enumerate(train_loader):
     optimizer.zero_grad()
@@ -39,7 +36,6 @@ for batch_idx, (data, target) in enumerate(train_loader):
         loss.backward()
     optimizer.step()
     print(batch_idx)
-
 torch.save({
     'model_state_dict': model.state_dict(),
     'optimizer_state_dict': optimizer.state_dict(),
diff --git a/examples/cpu/training/single_instance_training_fp32.py b/examples/cpu/training/single_instance_training_fp32.py
index e765226bc..a8ea9d95c 100644
--- a/examples/cpu/training/single_instance_training_fp32.py
+++ b/examples/cpu/training/single_instance_training_fp32.py
@@ -26,10 +26,7 @@ model = torchvision.models.resnet50()
 criterion = torch.nn.CrossEntropyLoss()
 optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)
 model.train()
-
 model, optimizer = ipex.optimize(model, optimizer=optimizer)
-# Uncomment the code below to enable experimental feature torch.compile
-# model = torch.compile(model, backend="ipex")
 
 for batch_idx, (data, target) in enumerate(train_loader):
     optimizer.zero_grad()
@@ -38,7 +35,6 @@ for batch_idx, (data, target) in enumerate(train_loader):
     loss.backward()
     optimizer.step()
     print(batch_idx)
-
 torch.save({
     'model_state_dict': model.state_dict(),
     'optimizer_state_dict': optimizer.state_dict(),
diff --git a/intel_extension_for_pytorch/__init__.py b/intel_extension_for_pytorch/__init__.py
index ea0055fd0..bcca78c42 100644
--- a/intel_extension_for_pytorch/__init__.py
+++ b/intel_extension_for_pytorch/__init__.py
@@ -19,26 +19,24 @@ import platform
 # Load the extension module
 ################################################################################
 
-if sys.platform == "win32":
-    pfiles_path = os.getenv("ProgramFiles", "C:\\Program Files")
-    py_dll_path = os.path.join(sys.exec_prefix, "Library", "bin")
-    th_dll_path = os.path.join(os.path.dirname(__file__), "bin")
+if sys.platform == 'win32':
+    pfiles_path = os.getenv('ProgramFiles', 'C:\\Program Files')
+    py_dll_path = os.path.join(sys.exec_prefix, 'Library', 'bin')
+    th_dll_path = os.path.join(os.path.dirname(__file__), 'bin')
 
     # When users create a virtualenv that inherits the base environment,
     # we will need to add the corresponding library directory into
     # DLL search directories. Otherwise, it will rely on `PATH` which
     # is dependent on user settings.
     if sys.exec_prefix != sys.base_exec_prefix:
-        base_py_dll_path = os.path.join(sys.base_exec_prefix, "Library", "bin")
+        base_py_dll_path = os.path.join(sys.base_exec_prefix, 'Library', 'bin')
     else:
-        base_py_dll_path = ""
+        base_py_dll_path = ''
 
-    dll_paths = list(
-        filter(os.path.exists, [th_dll_path, py_dll_path, base_py_dll_path])
-    )
+    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, base_py_dll_path]))
 
-    kernel32 = ctypes.WinDLL("kernel32.dll", use_last_error=True)
-    with_load_library_flags = hasattr(kernel32, "AddDllDirectory")
+    kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)
+    with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')
     prev_error_mode = kernel32.SetErrorMode(0x0001)
 
     kernel32.LoadLibraryW.restype = ctypes.c_void_p
@@ -57,16 +55,14 @@ if sys.platform == "win32":
                 raise err
 
     try:
-        ctypes.CDLL("vcruntime140.dll")
-        ctypes.CDLL("msvcp140.dll")
-        ctypes.CDLL("vcruntime140_1.dll")
+        ctypes.CDLL('vcruntime140.dll')
+        ctypes.CDLL('msvcp140.dll')
+        ctypes.CDLL('vcruntime140_1.dll')
     except OSError:
-        print(
-            """Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.
-                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe"""
-        )
+        print('''Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.
+                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe''')
 
-    dlls = glob.glob(os.path.join(th_dll_path, "*.dll"))
+    dlls = glob.glob(os.path.join(th_dll_path, '*.dll'))
     path_patched = False
     for dll in dlls:
         is_loaded = False
@@ -81,7 +77,7 @@ if sys.platform == "win32":
                 is_loaded = True
         if not is_loaded:
             if not path_patched:
-                os.environ["PATH"] = ";".join(dll_paths + [os.environ["PATH"]])
+                os.environ['PATH'] = ';'.join(dll_paths + [os.environ['PATH']])
                 path_patched = True
             res = kernel32.LoadLibraryW(dll)
             if res is None:
diff --git a/intel_extension_for_pytorch/_init_on_device.py b/intel_extension_for_pytorch/_init_on_device.py
index e5f048d8e..b2e4a4c8d 100644
--- a/intel_extension_for_pytorch/_init_on_device.py
+++ b/intel_extension_for_pytorch/_init_on_device.py
@@ -64,7 +64,9 @@ class OnDevice(object):
 
     def get_new_tensor_fn_for_dtype(self, dtype: torch.dtype) -> Callable:
         def new_tensor(cls, *args) -> Tensor:
-            tensor = OnDevice._orig_torch_empty(0, device=self.device).new_empty(*args)
+            tensor = OnDevice._orig_torch_empty(0, device=self.device).new_empty(
+                *args
+            )
             if tensor.is_floating_point():
                 tensor = tensor.to(dtype)
             return tensor
diff --git a/intel_extension_for_pytorch/_meta_registrations.py b/intel_extension_for_pytorch/_meta_registrations.py
index f9b968fd4..ba9008680 100644
--- a/intel_extension_for_pytorch/_meta_registrations.py
+++ b/intel_extension_for_pytorch/_meta_registrations.py
@@ -601,26 +601,12 @@ def meta_masked_multihead_self_attention(
     head_mask,
     attention_mask,
 ):
-    attn_output = query.new_empty(
-        (query.shape[0], query.shape[2], query.shape[1], query.shape[3])
-    )
+    attn_output = query.new_empty((query.shape[0], query.shape[2], query.shape[1], query.shape[3]))
     if query.dtype == torch.bfloat16:
-        attn_output.as_strided_(
-            attn_output.shape,
-            (
-                query.shape[1] * query.shape[2] * query.shape[3],
-                query.shape[3],
-                query.shape[2] * query.shape[3],
-                1,
-            ),
-        )
+        attn_output.as_strided_(attn_output.shape, (query.shape[1] * query.shape[2] * query.shape[3], query.shape[3], query.shape[2] * query.shape[3], 1))
     attn_weights = None
-    key_cache_out = query.new_empty(
-        (key_cache.shape[0], key_cache.shape[1], key.shape[2], key.shape[3])
-    )
-    value_cache_out = query.new_empty(
-        (value_cache.shape[0], value_cache.shape[1], value.shape[2], value.shape[3])
-    )
+    key_cache_out = query.new_empty((key_cache.shape[0], key_cache.shape[1], key.shape[2], key.shape[3]))
+    value_cache_out = query.new_empty((value_cache.shape[0], value_cache.shape[1], value.shape[2], value.shape[3]))
     beam_idx_out = query.new_empty(beam_idx.shape)
     return (attn_output, attn_weights, key_cache_out, value_cache_out, beam_idx_out)
 
diff --git a/intel_extension_for_pytorch/cpu/nn/frozen_batch_norm.py b/intel_extension_for_pytorch/cpu/nn/frozen_batch_norm.py
index 5fbc7d0ab..626c9313f 100644
--- a/intel_extension_for_pytorch/cpu/nn/frozen_batch_norm.py
+++ b/intel_extension_for_pytorch/cpu/nn/frozen_batch_norm.py
@@ -7,9 +7,11 @@ class FrozenBatchNorm2d(nn.Module):
     BatchNorm2d where the batch statistics and the affine parameters are fixed
 
     Args:
-        num_features (int): ``C`` from an expected input of size ``(N, C, H, W)``.
-            Input shape: ``(N, C, H, W)``.
-            Output shape: ``(N, C, H, W)`` (same shape as input).
+        num_features (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
+
+    Shape
+        - Input: :math:`(N, C, H, W)`
+        - Output: :math:`(N, C, H, W)` (same shape as input)
     """
 
     def __init__(
diff --git a/intel_extension_for_pytorch/cpu/nn/interaction.py b/intel_extension_for_pytorch/cpu/nn/interaction.py
index c4020fc77..3c70982f3 100644
--- a/intel_extension_for_pytorch/cpu/nn/interaction.py
+++ b/intel_extension_for_pytorch/cpu/nn/interaction.py
@@ -19,10 +19,12 @@ def interaction(*args):
     ([0.1, 0.2, 0.3] * [-0.1, 0.3, 0.2]^T) =  -0.1 + 0.6 + 0.6 = 1.1
 
     Args:
-        *args: Multiple tensors which represent different features.
-            Input shape: ``N * (B, D)``, where N is the number of different kinds of features,
-            B is the batch size, D is feature size.
-            Output shape: ``(B, D + N * ( N - 1 ) / 2)``.
+        *args: Multiple tensors which represent different features
+
+    Shape
+        - Input: :math:`N * (B, D)`, where N is the number of different kinds of features,
+            B is the batch size, D is feature size
+        - Output: :math:`(B, D + N * ( N - 1 ) / 2)`
     """
 
     if torch.is_grad_enabled():
diff --git a/intel_extension_for_pytorch/cpu/tpp/fused_bert.py b/intel_extension_for_pytorch/cpu/tpp/fused_bert.py
index 41f10c22d..9005e87f6 100644
--- a/intel_extension_for_pytorch/cpu/tpp/fused_bert.py
+++ b/intel_extension_for_pytorch/cpu/tpp/fused_bert.py
@@ -703,7 +703,7 @@ class BertEmbeddingsFunction(torch.autograd.Function):
 class BertEmbeddings(BlockedModule):
     """Construct the embeddings from word, position and token_type embeddings."""
 
-    def __init__(self, config, position_ids_persistent=False):
+    def __init__(self, config):
         super().__init__()
         self.word_embeddings = nn.Embedding(
             config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id
@@ -725,17 +725,11 @@ class BertEmbeddings(BlockedModule):
         self.pad_token_id = config.pad_token_id
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
-        if not position_ids_persistent:
-            self.register_buffer(
-                "position_ids",
-                torch.arange(config.max_position_embeddings).expand((1, -1)),
-                persistent=False,
-            )
-        else:
-            self.register_buffer(
-                "position_ids",
-                torch.arange(config.max_position_embeddings).expand((1, -1)),
-            )
+        self.register_buffer(
+            "position_ids",
+            torch.arange(config.max_position_embeddings).expand((1, -1)),
+            persistent=False,
+        )
         self.position_embedding_type = getattr(
             config, "position_embedding_type", "absolute"
         )
@@ -1249,7 +1243,7 @@ def fast_bert(model, dtype=torch.float, optimizer=None, unpad=False):
     # tpp bert optimization depends on the transformers repo to implementate the related module
     installed_pkg = {pkg.key for pkg in pkg_resources.working_set}
     min_version = "4.6.0"
-    max_version = "4.31.0"
+    max_version = "4.20.0"
     if "transformers" not in installed_pkg:
         raise RuntimeError(
             "Please installed the transformers with version: between {} and {}".format(
@@ -1269,9 +1263,6 @@ def fast_bert(model, dtype=torch.float, optimizer=None, unpad=False):
                 min_version, max_version, trans_version
             )
         )
-    position_ids_persistent = False
-    if version.parse(trans_version) < version.parse("4.31.0"):
-        position_ids_persistent = True
     PT_OPTIMIZER_TO_TPP_OPTIMIZER = {
         torch.optim.AdamW: AdamW,
         transformers.optimization.AdamW: AdamW,
@@ -1306,9 +1297,7 @@ def fast_bert(model, dtype=torch.float, optimizer=None, unpad=False):
         assert isinstance(
             new_model.embeddings, transformers.models.bert.modeling_bert.BertEmbeddings
         )
-        new_model.embeddings = BertEmbeddings(
-            model.config, position_ids_persistent=position_ids_persistent
-        )
+        new_model.embeddings = BertEmbeddings(model.config)
         assert isinstance(
             new_model.encoder, transformers.models.bert.modeling_bert.BertEncoder
         )
@@ -1320,9 +1309,7 @@ def fast_bert(model, dtype=torch.float, optimizer=None, unpad=False):
             new_model.bert.embeddings,
             transformers.models.bert.modeling_bert.BertEmbeddings,
         )
-        new_model.bert.embeddings = BertEmbeddings(
-            model.bert.config, position_ids_persistent=position_ids_persistent
-        )
+        new_model.bert.embeddings = BertEmbeddings(model.bert.config)
         assert isinstance(
             new_model.bert.encoder, transformers.models.bert.modeling_bert.BertEncoder
         )
diff --git a/intel_extension_for_pytorch/cpu/tpp/utils/blocked_layout.py b/intel_extension_for_pytorch/cpu/tpp/utils/blocked_layout.py
index 036a38185..a05d664e9 100644
--- a/intel_extension_for_pytorch/cpu/tpp/utils/blocked_layout.py
+++ b/intel_extension_for_pytorch/cpu/tpp/utils/blocked_layout.py
@@ -273,14 +273,7 @@ class BlockedParameter(torch.nn.Parameter):
         self.data = self._data
 
     def __tensor_flatten__(self):
-        return self._data, [
-            self.requires_grad,
-            self.blocked,
-            self.blocking_param,
-            self.blocking_manager,
-            self.unblocked_dtype,
-            self.blocked_dtype,
-        ]
+        return self._data, [self.requires_grad, self.blocked, self.blocking_param, self.blocking_manager, self.unblocked_dtype, self.blocked_dtype]
 
     @staticmethod
     def __tensor_unflatten__(data, ctx):
diff --git a/intel_extension_for_pytorch/frontend.py b/intel_extension_for_pytorch/frontend.py
index 2003c7a6d..637901ab8 100644
--- a/intel_extension_for_pytorch/frontend.py
+++ b/intel_extension_for_pytorch/frontend.py
@@ -549,14 +549,12 @@ def optimize(
         if dtype == torch.bfloat16:
             assert core.onednn_has_bf16_support(), (
                 "BF16 weight prepack needs the cpu support avx512bw, avx512vl and avx512dq, "
-                + "but the desired instruction sets are not available. "
-                + "Please set dtype to torch.float or set weights_prepack to False."
+                + "please set dtype to torch.float or set weights_prepack to False."
             )
         if dtype == torch.half:
             assert core.onednn_has_fp16_support(), (
                 "FP16 weight prepack needs the cpu support avx512_core_fp16, "
-                + "but the desired instruction sets are not available. "
-                + "Please set dtype to torch.float or set weights_prepack to False."
+                + "please set dtype to torch.float or set weights_prepack to False."
             )
         (
             optimized_model,
diff --git a/intel_extension_for_pytorch/nn/modules/__init__.py b/intel_extension_for_pytorch/nn/modules/__init__.py
index 134afe34c..ce338d0b1 100644
--- a/intel_extension_for_pytorch/nn/modules/__init__.py
+++ b/intel_extension_for_pytorch/nn/modules/__init__.py
@@ -3,5 +3,6 @@ from ...cpu.nn import _roi_align
 from .merged_embeddingbag import MergedEmbeddingBagWithSGD
 from .merged_embeddingbag import MergedEmbeddingBag
 from .merged_embeddingbag import MergedEmbeddingBagWithCat
+from .merged_embeddingbag import MergedEmbWithCat
 from ...cpu.nn.linear_fuse_eltwise import IPEXLinearEltwise
 from .weight_only_quantization import IpexWoqLinear
diff --git a/intel_extension_for_pytorch/nn/modules/merged_embeddingbag.py b/intel_extension_for_pytorch/nn/modules/merged_embeddingbag.py
index 0634bc0b2..46cf4dbe0 100644
--- a/intel_extension_for_pytorch/nn/modules/merged_embeddingbag.py
+++ b/intel_extension_for_pytorch/nn/modules/merged_embeddingbag.py
@@ -46,7 +46,7 @@ def merged_embeddingbag(
 def merged_embeddingbag_with_cat(
     weights,
     indices,
-    offsets,
+    #offsets,
     dense_feature,
 ):
     if torch.is_grad_enabled():
@@ -54,7 +54,7 @@ def merged_embeddingbag_with_cat(
             "do not support training for merged_embeddingbag_with_cat not"
         )
     return torch.ops.torch_ipex.merged_embeddingbag_cat_forward(
-        weights, indices, offsets, dense_feature
+        weights, indices, dense_feature
     )
 
 
@@ -618,3 +618,27 @@ class MergedEmbeddingBagWithCat(MergedEmbeddingBag):
             offsets,
             dense_feature,
         )
+
+class MergedEmbWithCat(nn.Module):
+    def __init__(self,
+                 embedding_dim: int,
+                 num_embeddings_pool: List[int]):
+        super().__init__()
+        self._multi_hot = [3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1]
+        self._embedding_dim = embedding_dim
+        self._num_embeddings = len(num_embeddings_pool)
+        embedding_bags: nn.ModuleList = nn.ModuleList()
+        for num_embeddings in num_embeddings_pool:
+            W = torch.empty(num_embeddings, embedding_dim)
+            EE = torch.nn.EmbeddingBag(
+                num_embeddings=num_embeddings,
+                embedding_dim=embedding_dim,
+                _weight=W,
+                include_last_offset=True,
+                mode="sum")
+            embedding_bags.append(EE)
+        self.weights = torch.nn.ParameterList([e.weight for e in embedding_bags])
+        return
+
+    def forward(self, index: List[torch.Tensor], dense) -> torch.Tensor:
+        return torch.ops.torch_ipex.mlperf_merged_emb_cat(self.weights, index, dense, self._multi_hot)
diff --git a/intel_extension_for_pytorch/nn/modules/weight_only_quantization.py b/intel_extension_for_pytorch/nn/modules/weight_only_quantization.py
index 899e2e34a..2aa4439a7 100644
--- a/intel_extension_for_pytorch/nn/modules/weight_only_quantization.py
+++ b/intel_extension_for_pytorch/nn/modules/weight_only_quantization.py
@@ -1,16 +1,36 @@
 import torch
 from torch import nn
+from torch.ao.nn.quantized.modules.utils import _clamp_weights
+from ...quantization._qconfig import get_weight_only_quant_qconfig_mapping
 from intel_extension_for_pytorch.nn.utils._weight_prepack import (
     may_import_deepspeed_modules,
     _all_reduce_and_bias_add,
     _pre_ipex_gemm,
 )
-from intel_extension_for_pytorch.quantization import (
-    QConfigWoq,
-    quantize_per_channel,
-    quantize_per_block,
-)
 
+# Port from PyTorch with a few changes
+def _quantize_weight(float_wt, observer):
+    wt_scale, wt_zp = observer.calculate_qparams()
+    dtype = observer.dtype
+    if observer.qscheme in [torch.per_tensor_symmetric, torch.per_tensor_affine]:
+        qweight = torch.quantize_per_tensor(
+            float_wt,
+            float(wt_scale), int(wt_zp), dtype)
+        qweight = _clamp_weights(qweight, observer, wt_scale, wt_zp)
+    elif observer.qscheme in [torch.per_channel_symmetric, torch.per_channel_affine]:
+        wt_axis = observer.ch_axis
+        qweight = torch.quantize_per_channel(
+            float_wt,
+            wt_scale.to(torch.double), wt_zp.to(torch.int64), wt_axis, dtype)
+        qweight = _clamp_weights(qweight, observer, wt_scale, wt_zp)
+    elif observer.qscheme in [torch.per_channel_affine_float_qparams]:
+        qweight = torch.quantize_per_channel(
+            float_wt,
+            wt_scale.to(torch.float), wt_zp.to(torch.float), observer.ch_axis, dtype)
+        qweight = _clamp_weights(qweight, observer, wt_scale, wt_zp)
+    else:
+        raise ValueError("Unexpected qscheme " + observer.qscheme)
+    return qweight
 
 class IpexWoqLinear(nn.Module):
     r"""
@@ -29,8 +49,6 @@ class IpexWoqLinear(nn.Module):
         self._op_context = None
         self._lowp_mode = 0
         self._num_concats = 1
-        self._act_quant_mode = 0
-        self._group_size = -1
 
     def pre_ipex_gemm(self, input):
         return input
@@ -41,7 +59,9 @@ class IpexWoqLinear(nn.Module):
     def forward(self, x):
         x = self.pre_ipex_gemm(x)
 
-        Y = torch.ops.torch_ipex.ipex_woq_linear(x, self._op_context.get_data_handle())
+        Y = torch.ops.torch_ipex.ipex_woq_linear(
+            x, self._op_context.get_data_handle()
+        )
 
         return self.post_ipex_gemm(Y)
 
@@ -55,20 +75,15 @@ class IpexWoqLinear(nn.Module):
         extra_repr_str += ", bias={}".format(self.bias)
         extra_repr_str += ", lowp_mode={}".format(self._lowp_mode)
         extra_repr_str += ", num_concats={}".format(self._num_concats)
-        extra_repr_str += ", act_quant_mode={}".format(self._act_quant_mode)
-        extra_repr_str += ", group_size={}".format(self._group_size)
         return extra_repr_str
 
     @classmethod
-    def from_float(cls, mod, scales=None, zero_points=None):
+    def from_float(cls, mod):
         r"""Create a weight-only quantized module from a float module or qparams_dict
 
         Args:
-            mod (Module): an instance of nn.Linear or its subclasses.
-            scales: the scales Tensor for quantizing weight. If it is None,
-                scales are found by min/max of the weight.
-            zero_points: the zero points Tensor for quantizing weight. If it is None,
-                zero points are found by min/max of the weight.
+            mod (Module): a float module, either produced by torch.ao.quantization
+                          utilities or provided by the user
         """
         float_modules = [torch.nn.Linear]
         deepspeed_modules = may_import_deepspeed_modules()
@@ -77,65 +92,49 @@ class IpexWoqLinear(nn.Module):
         if any(issubclass(type(mod), float_module) for float_module in float_modules):
             float_modules.extend([type(mod)])
 
-        assert type(mod) in float_modules, (
-            "IpexWoqLinear.from_float only works for one of"
-            + str([float_mod.__name__ for float_mod in float_modules])
-            + f" or their subclasses, but found {type(mod)}"
-        )
+        assert (
+            type(mod) in float_modules
+        ), "IpexWoqLinear.from_float only works for one of" + str(
+            [float_mod.__name__ for float_mod in float_modules]
+        ) + f" or their subclasses, but found {type(mod)}"
         assert hasattr(mod, "qconfig"), "Input float module must have qconfig defined"
-        qconfig = mod.qconfig
-        if qconfig is None or not isinstance(qconfig, QConfigWoq):
-            return mod
-
-        lowp_mode = qconfig.lowp_mode
-        if qconfig.lowp_mode == 3 and qconfig.weight_dtype != torch.quint4x2:
-            # lowp_mode=3 (INT8) is enabled for INT4 weight only
-            # Fall back to lowp_mode=2 in other case
-            # TODO(Weiwen) Support lowp_mode=3
-            lowp_mode = 2
-            print(
-                "Warning: lowp_mode=3(INT8) is not supported yet in this case. "
-                "Falling back to 2(BF16)."
+        lowp_mode = 0
+        if mod.qconfig is not None and mod.qconfig.weight is not None:
+            weight_observer = mod.qconfig.weight()
+            if hasattr(mod.qconfig, 'lowp_mode'):
+                lowp_mode = mod.qconfig.lowp_mode
+                if mod.qconfig.lowp_mode == 3 and weight_observer.dtype == torch.qint8:
+                    # lowp_mode=3 (INT8) is not yet supported for INT8 weight
+                    # Fall back to lowp_mode=2 in this case
+                    # TODO(Weiwen) Support lowp_mode=3
+                    lowp_mode = 2
+                    print('Warning: lowp_mode=3(INT8) is not supported yet in this case. '
+                          'Falling back to 2(BF16).')
+        else:
+            weight_observer = (
+                get_weight_only_quant_qconfig_mapping().global_qconfig.weight()
             )
-        act_quant_mode = qconfig.act_quant_mode
         num_concats = 1
-        if hasattr(mod, "_num_concats"):
+        if hasattr(mod, '_num_concats'):
             num_concats = mod._num_concats
-        dtype = qconfig.weight_dtype
-        is_int4 = dtype == torch.quint4x2
-        group_size = qconfig.group_size
-
-        if group_size == -1:
-            qweight, scales, zero_points = quantize_per_channel(
-                mod.weight, is_int4, scales, zero_points
-            )
-        else:
-            qweight, scales, zero_points = quantize_per_block(
-                mod.weight, is_int4, group_size, scales, zero_points
-            )
+        dtype = weight_observer.dtype
+        assert dtype in [torch.quint8, torch.qint8, torch.quint4x2], (
+            "The only supported dtypes for "
+            "weight-only quantized linear are quint8, qint8 and quint4x2 got: {}".format(dtype)
+        )
+        weight_observer(mod.weight)
+        qweight = _quantize_weight(mod.weight.float(), weight_observer)
         if not hasattr(mod, "in_features"):
             mod.in_features = mod.weight.size()[1]
         if not hasattr(mod, "out_features"):
             mod.out_features = mod.weight.size()[0]
 
-        qlinear = cls._init_cls(
-            mod,
-            dtype,
-            qweight,
-            scales,
-            zero_points,
-            group_size,
-            lowp_mode,
-            num_concats,
-            act_quant_mode,
-        )
+        qlinear = cls._init_cls(mod, dtype, qweight, lowp_mode, num_concats)
         del qweight
         return qlinear
 
     @classmethod
-    def from_float_and_int4_weight(
-        cls, mod, qweight, scales, zero_points, bias=None, group_size=-1
-    ):
+    def from_float_and_int4_weight(cls, mod, qweight, scales, zero_points, bias=None):
         r"""Create a weight-only quantized module from a float module and int4 weight
 
         Args:
@@ -153,32 +152,23 @@ class IpexWoqLinear(nn.Module):
         if any(issubclass(type(mod), float_module) for float_module in float_modules):
             float_modules.extend([type(mod)])
 
-        assert type(mod) in float_modules, (
-            "IpexWoqLinear.from_float only works for one of"
-            + str([float_mod.__name__ for float_mod in float_modules])
-            + f" or their subclasses, but found {type(mod)}"
-        )
+        assert (
+            type(mod) in float_modules
+        ), "IpexWoqLinear.from_float only works for one of" + str(
+            [float_mod.__name__ for float_mod in float_modules]
+        ) + f" or their subclasses, but found {type(mod)}"
         assert hasattr(mod, "qconfig"), "Input float module must have qconfig defined"
 
         lowp_mode = 0
-        act_quant_mode = 0
-        if mod.qconfig is not None:
-            if hasattr(mod.qconfig, "lowp_mode"):
-                lowp_mode = mod.qconfig.lowp_mode
-            if hasattr(mod.qconfig, "act_quant_mode"):
-                act_quant_mode = mod.qconfig.act_quant_mode
+        if mod.qconfig is not None and hasattr(mod.qconfig, 'lowp_mode'):
+            lowp_mode = mod.qconfig.lowp_mode
         num_concats = 1
-        if hasattr(mod, "_num_concats"):
+        if hasattr(mod, '_num_concats'):
             num_concats = mod._num_concats
 
         w_dtype = qweight.dtype
-        assert w_dtype in [
-            torch.int32,
-            torch.quint4x2,
-            torch.bfloat16,
-            torch.float32,
-        ], "Quantized int4 weight should have data type int32 or quint4x2, but got: {}".format(
-            w_dtype
+        assert w_dtype in [torch.int32, torch.quint4x2, torch.bfloat16, torch.float32], (
+            "Quantized int4 weight should have data type int32 or quint4x2, but got: {}".format(w_dtype)
         )
         if not hasattr(mod, "in_features"):
             mod.in_features = mod.weight.size()[1]
@@ -189,60 +179,23 @@ class IpexWoqLinear(nn.Module):
         if bias is None:
             bias = mod.bias
         qlinear._op_context = torch.ops.ipex_prepack.weight_only_qlinear_prepack_int4(
-            qweight,
-            scales,
-            zero_points,
-            bias,
-            None,
-            group_size,
-            int(lowp_mode),
-            num_concats,
-            act_quant_mode,
+            qweight, scales, zero_points, bias, None, int(lowp_mode), num_concats
         )
         qlinear._lowp_mode = lowp_mode
         qlinear._num_concats = num_concats
-        qlinear._act_quant_mode = act_quant_mode
-        qlinear._group_size = group_size
         del qweight
         return qlinear
 
     @classmethod
-    def _init_cls(
-        cls,
-        mod,
-        dtype,
-        qweight,
-        scales,
-        zero_points,
-        group_size,
-        lowp_mode,
-        num_concats,
-        act_quant_mode,
-    ):
-        qlinear = cls(
-            mod.in_features, mod.out_features, mod.bias is not None, dtype=dtype
-        )
-        is_int4 = dtype == torch.quint4x2
+    def _init_cls(cls, mod, dtype, qweight, lowp_mode, num_concats):
+        qlinear = cls(mod.in_features, mod.out_features, mod.bias is not None, dtype=dtype)
         qlinear._op_context = torch.ops.ipex_prepack.weight_only_qlinear_prepack(
-            qweight,
-            [mod.out_features, mod.in_features],
-            scales,
-            zero_points,
-            mod.bias,
-            None,
-            is_int4,
-            group_size,
-            int(lowp_mode),
-            num_concats,
-            act_quant_mode,
+            qweight, mod.bias, None, int(lowp_mode), num_concats
         )
         qlinear._lowp_mode = lowp_mode
         qlinear._num_concats = num_concats
-        qlinear._act_quant_mode = act_quant_mode
-        qlinear._group_size = group_size
         return qlinear
 
-
 class IpexWoqLinearAllreduce(IpexWoqLinear):
     def __init__(
         self,
@@ -270,33 +223,15 @@ class IpexWoqLinearAllreduce(IpexWoqLinear):
         )
 
     @classmethod
-    def _init_cls(
-        cls,
-        mod,
-        dtype,
-        qweight,
-        scales,
-        zero_points,
-        group_size,
-        lowp_mode,
-        num_concats,
-        act_quant_mode,
-    ):
+    def _init_cls(cls, mod, dtype, qweight, lowp_mode, num_concats):
         qlinear = cls._init_from_mod(mod, dtype)
 
-        is_int4 = dtype == torch.quint4x2
         qlinear._op_context = torch.ops.ipex_prepack.weight_only_qlinear_prepack(
             qweight,
-            [mod.out_features, mod.in_features],
-            scales,
-            zero_points,
             None,  # Set bias to None when prepacking. Please refer to the comment in __init__ of _IPEXLinearAllreduce
             None,  # batch_size
-            is_int4,
-            group_size,
             lowp_mode,
-            num_concats,
-            act_quant_mode,
+            num_concats
         )
 
         return qlinear
diff --git a/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py b/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py
index a014c3e09..bc7258b32 100644
--- a/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py
+++ b/intel_extension_for_pytorch/nn/utils/_parameter_wrapper.py
@@ -561,25 +561,11 @@ class ParameterWrapper(object):
                     torch.float32,
                     torch.bfloat16,
                 ], "Only float, bf16 and fp16 are supported"
-                use_dnnl = True
-
+                use_dnnl = True if not _using_tpp() else False
         module.use_tpp = _using_tpp()
+        module.use_dnnl = use_dnnl
         if not hasattr(module, "out_features"):
             setattr(module, "out_features", module.weight.shape[0])  # noqa: B010
-
-        if module.use_tpp:
-            from intel_extension_for_pytorch.nn.utils import (
-                Apply_TPPLinear_weight_prepack,
-            )
-
-            Apply_TPPLinear_weight_prepack(module, dtype=module.weight.dtype)
-            if module.tpp_fallback:
-                module.use_tpp = False
-            else:
-                self.parameter.data = module.weight.data
-                self.parameter = module.weight
-
-        module.use_dnnl = use_dnnl if not module.use_tpp else False
         if not module.use_tpp:
             # prepare batch size
             module.batch_size_collapsed = None
@@ -597,6 +583,14 @@ class ParameterWrapper(object):
                     module.weight, module.bias, module.batch_size_collapsed
                 )
             self.pack_weight(use_dnnl)
+        else:
+            from intel_extension_for_pytorch.nn.utils import (
+                Apply_TPPLinear_weight_prepack,
+            )
+
+            Apply_TPPLinear_weight_prepack(module, dtype=module.weight.dtype)
+            self.parameter.data = module.weight.data
+            self.parameter = module.weight
 
     def load_cast_and_prepack(self, module, param):
         # load from state dict
diff --git a/intel_extension_for_pytorch/nn/utils/_weight_prepack.py b/intel_extension_for_pytorch/nn/utils/_weight_prepack.py
index 712fd6cb7..cab30f749 100644
--- a/intel_extension_for_pytorch/nn/utils/_weight_prepack.py
+++ b/intel_extension_for_pytorch/nn/utils/_weight_prepack.py
@@ -2,6 +2,7 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import logging
+import os
 import pkg_resources
 from intel_extension_for_pytorch import optim
 from intel_extension_for_pytorch.cpu.tpp.utils.blocked_layout import (
@@ -93,19 +94,28 @@ installed_pkg = {pkg.key for pkg in pkg_resources.working_set}
 if "deepspeed" in installed_pkg:
     from deepspeed import comm
 
-    def _all_reduce(self):
+    def _all_reduce(self, reduceOp, tag, ranks, group_size):
         comm.inference_all_reduce(self, async_op=False)
         return self
 
     ds_comm = torch.library.Library("deepspeed_comm", "DEF")
-    ds_comm.define("all_reduce(Tensor self) -> Tensor")
+    ds_comm.define(
+        "all_reduce(Tensor self, str reduceOp, str tag, int[] ranks, int group_size) -> Tensor"
+    )
     ds_comm_lib_cpu = torch.library.Library("deepspeed_comm", "IMPL", "CPU")
     ds_comm_lib_cpu.impl("all_reduce", _all_reduce)
 
 
 def _all_reduce_and_bias_add(mp_group, original_bias, output):
     if mp_group is not None:
-        torch.ops.deepspeed_comm.all_reduce(output)
+        torch.ops.deepspeed_comm.all_reduce(
+            output,
+            "sum",
+            "",
+            list(torch.arange(int(os.environ["WORLD_SIZE"]))),
+            int(os.environ["WORLD_SIZE"]),
+        )
+
     if original_bias is not None:
         output += original_bias
 
@@ -114,16 +124,10 @@ def _all_reduce_and_bias_add(mp_group, original_bias, output):
 
 def _pre_ipex_gemm(input, world_size, rank):
     assert "deepspeed" in installed_pkg, "_pre_ipex_gemm requires deepspeed installed"
-    try:
-        from deepspeed.module_inject.tp_shard import get_shard_size, get_shard_size_list
-    except ImportError:
-        from deepspeed.utils.tp_shard import get_shard_size, get_shard_size_list
-
-    input_shard_size = get_shard_size(input.shape[-1], world_size, "lm_head")
-    input_shard_offset = sum(
-        get_shard_size_list(input.shape[-1], world_size, "lm_head")[0:rank]
-    )
-    return input[:, :, input_shard_offset : input_shard_offset + input_shard_size]
+    from deepspeed.utils.tp_shard import get_shard_size, get_shard_size_list
+    input_shard_size = get_shard_size(input.shape[-1], world_size)
+    input_shard_offset = sum(get_shard_size_list(input.shape[-1], world_size)[0:rank])
+    return input[:, :, input_shard_offset:input_shard_offset + input_shard_size]
 
 
 def _ipex_module_load_from_state_dict_(self, state_dict, prefix):
@@ -268,9 +272,7 @@ class _IPEXLinear(_IPEXPrepackModule):
                         x, self.weight, self.bias, self.out_features
                     )
                 else:
-                    output = torch.ops.torch_ipex.tpp_linear(
-                        x, self.weight, self.out_features
-                    )
+                    output = torch.ops.torch_ipex.tpp_linear(x, self.weight, self.out_features)
         else:
             output = torch.ops.torch_ipex.ipex_MKLSGEMM(
                 x,
@@ -490,8 +492,8 @@ def weight_prepack_with_ipex(model, optimizer, params_attr, device_type="cpu"):
                     new_m.tpp_fallback = True
                 else:
                     new_m.tpp_fallback = False
-                    params_attr[m.weight] = params_attr.pop(weight_key)
-                    del weight_key
+                params_attr[m.weight] = params_attr.pop(weight_key)
+                del weight_key
 
             else:
                 param_wrapper.prepack(m, is_training)
diff --git a/intel_extension_for_pytorch/quantization/__init__.py b/intel_extension_for_pytorch/quantization/__init__.py
index 7145c392d..6b8829645 100644
--- a/intel_extension_for_pytorch/quantization/__init__.py
+++ b/intel_extension_for_pytorch/quantization/__init__.py
@@ -7,13 +7,5 @@ from ._qconfig import (
     get_smooth_quant_qconfig_mapping,
     get_weight_only_quant_qconfig_mapping,
     WoqLowpMode,
-    WoqActQuantMode,
-    QConfigWoq,
 )
 from ._autotune import autotune
-from ._quantize_utils import (
-    quantize_per_channel,
-    dequantize_per_channel,
-    quantize_per_block,
-    dequantize_per_block,
-)
diff --git a/intel_extension_for_pytorch/quantization/_qconfig.py b/intel_extension_for_pytorch/quantization/_qconfig.py
index a79f0295f..aaf63dc3b 100644
--- a/intel_extension_for_pytorch/quantization/_qconfig.py
+++ b/intel_extension_for_pytorch/quantization/_qconfig.py
@@ -8,10 +8,7 @@ from torch.ao.quantization import (
     QConfig,
     QConfigMapping,
 )
-from ._smooth_quant import (
-    SmoothQuantActivationObserver,
-    SmoothQuantWeightObserver,
-)
+from ._smooth_quant import SmoothQuantActivationObserver, SmoothQuantWeightObserver
 
 
 _default_weight_observer = PerChannelMinMaxObserver.with_args(
@@ -39,50 +36,29 @@ Default qconfig configuration for dynamic quantization.
 default_dynamic_qconfig_mapping = QConfigMapping().set_global(default_dynamic_qconfig)
 
 
-# Define QConfig for SmoothQuant by extending PyTorch's QConfig
-QConfigSmoothQuant = namedtuple(
-    "QConfigSmoothQuant", [*QConfig._fields, "share_weight_observers"]
-)
-
-
 def get_smooth_quant_qconfig_mapping(
     alpha=0.5,
     act_observer=None,
     act_ic_observer=None,
     wei_observer=None,
     wei_ic_observer=None,
-    share_weight_observers=True,
 ):
     """
     Configuration with SmoothQuant for static quantization of large language models (LLM)
     For SmoothQuant, see https://arxiv.org/pdf/2211.10438.pdf
-
-    Args:
-        alpha: Hyper-parameter for SmoothQuant.
-        act_observer: Observer for activation of ops other than nn.Linear.
-            HistogramObserver by default. For nn.Linear with SmoothQuant
-            enabled, q-param is calculated based on act_ic_observer's and
-            wei_ic_observer's min/max. It is not affected by this argument.
-            Example: ``torch.ao.quantization.MinMaxObserver``
-        act_ic_observer: Per-input-channel Observer for activation.
-            For nn.Linear with SmoothQuant enabled only.
-            PerChannelMinMaxObserver by default.
-            Example: ``torch.ao.quantization.PerChannelMinMaxObserver.with_args(ch_axis=1)``
-        wei_observer: Observer for weight of all weighted ops.
-            For nn.Linear with SmoothQuant enabled, it calculates q-params
-            after applying scaling factors. PerChannelMinMaxObserver by
-            default.
-            Example: ``torch.ao.quantization.PerChannelMinMaxObserver.with_args(\
-dtype=torch.qint8, qscheme=torch.per_channel_symmetric)``
-        wei_ic_observer: Per-input-channel Observer for weight.
-            For nn.Linear with SmoothQuant enabled only.
-            PerChannelMinMaxObserver by default.
-            Example: ``torch.ao.quantization.PerChannelMinMaxObserver.with_args(ch_axis=1)``
-
-    Returns:
-        torch.ao.quantization.QConfig
+    Arguments:
+        alpha:              Hyper-parameter for SmoothQuant.
+        act_observer:       Observer for activation of ops other than nn.Linear. HistogramObserver by default.
+                            For nn.Linear with SmoothQuant enabled, q-param is calculated based on act_ic_observer's
+                            and wei_ic_observer's min/max. It is not affected by this argument.
+        act_ic_observer:    Per-input-channel Observer for activation. For nn.Linear with SmoothQuant enabled only.
+                            PerChannelMinMaxObserver by default.
+        wei_observer:       Observer for weight of all weighted ops. For nn.Linear with SmoothQuant enabled, it
+                            calculates q-params after applying scaling factors. PerChannelMinMaxObserver by default.
+        wei_ic_observer:    Per-input-channel Observer for weight. For nn.Linear with SmoothQuant enabled only.
+                            PerChannelMinMaxObserver by default.
     """
-    qconfig = QConfigSmoothQuant(
+    qconfig = QConfig(
         activation=SmoothQuantActivationObserver.with_args(
             reduce_range=False,
             alpha=alpha,
@@ -96,7 +72,6 @@ dtype=torch.qint8, qscheme=torch.per_channel_symmetric)``
             wei_observer=wei_observer,
             wei_ic_observer=wei_ic_observer,
         ),
-        share_weight_observers=share_weight_observers,
     )
     return QConfigMapping().set_global(qconfig)
 
@@ -108,70 +83,14 @@ class WoqLowpMode(IntEnum):
     BF16 = 2
     INT8 = 3
 
-
-class WoqActQuantMode(IntEnum):
-    NONE = -1
-    PER_TENSOR = 0
-    PER_IC_BLOCK = 1  # IC = Input Channel
-    PER_BATCH = 2
-    PER_BATCH_IC_BLOCK = 3
-
-
-QConfigWoq = namedtuple(
-    "QConfigWoq",
-    [*QConfig._fields, "lowp_mode", "act_quant_mode", "weight_dtype", "group_size"],
-)
-
-
+QConfigWoq = namedtuple('QConfigWoq', [*QConfig._fields, 'lowp_mode'])
 def get_weight_only_quant_qconfig_mapping(
-    *,
-    weight_dtype: torch.dtype = torch.qint8,
-    lowp_mode: int = WoqLowpMode.NONE,
-    act_quant_mode: int = WoqActQuantMode.PER_IC_BLOCK,
-    group_size: int = -1
-):
-    """
-    Configuration for weight-only quantization (WOQ) for LLM.
-    Arguments:
-        weight_dtype:   Data type for weight, torch.qint8 (INT8) or torch.quint4x2 (INT4)
-        lowp_mode:      specify the lowest precision data type for computation. Data types
-                        that has even lower precision won't be used.
-                        Not necessarily related to activation or weight dtype.
-                        - NONE(0): Use the activation data type for computation.
-                        - FP16(1): Use float16 (a.k.a. half) as the lowest precision for computation.
-                        - BF16(2): Use bfloat16 as the lowest precision for computation.
-                        - INT8(3): Use INT8 as the lowest precision for computation.
-                                   Activation is quantized to int8 at runtime in this case.
-                        Note that lowp_mode=INT8(3) is only available when weight_dtype=torch.quint4x2.
-                        In other cases, it will fall back to lowp_mode=BF16(2).
-        act_quant_mode: Quantization granularity of activation. It only works for lowp_mode=INT8.
-                        It has no effect in other cases. The tensor is divided into groups, and
-                        each group is quantized with its own quantization parameters.
-                        Suppose the activation has shape batch_size by input_channel (IC).
-                        - PER_TENSOR(0): Use the same quantization parameters for the entire tensor.
-                        - PER_IC_BLOCK(1): Tensor is divided along IC with group size = IC_BLOCK.
-                        - PER_BATCH(2): Tensor is divided along batch_size with group size = 1.
-                        - PER_BATCH_IC_BLOCK(3): Tenosr is divided into blocks of 1 x IC_BLOCK.
-                        Note that IC_BLOCK is determined by group_size automatically.
-        group_size:     Control quantization granularity along input channel (IC) dimension of weight.
-                        Must be a positive power of 2 (i.e., 2^k, k > 0) or -1.
-                        If group_size = -1:
-                            If act_quant_mode = PER_TENSOR ro PER_BATCH:
-                                No grouping along IC for both activation and weight
-                            If act_quant_mode = PER_IC_BLOCK or PER_BATCH_IC_BLOCK:
-                                No grouping along IC for weight. For activation,
-                                IC_BLOCK is determined automatically by IC.
-                        If group_size > 0:
-                            act_quant_mode can be any. If act_quant_mode is PER_IC_BLOCK
-                            or PER_BATCH_IC_BLOCK, weight is grouped along IC by group_size.
-                            The IC_BLOCK for activation is determined by group_size automatically.
-                            Each group has its own quantization parameters.
-    """
-    assert group_size == -1 or (
-        group_size > 0 and (group_size & (group_size - 1)) == 0
-    ), "Group size must be -1 or a positive power of 2, but got {}".format(group_size)
+        *,
+        weight_dtype: torch.dtype = torch.qint8,
+        lowp_mode: int = WoqLowpMode.NONE):
     dtype_to_qscheme = {
         torch.qint8: torch.per_channel_affine,
+        torch.quint8: torch.per_channel_affine,
         # It is required to use per_channel_affine_float_qparams for quint4x2 by PyTorch
         torch.quint4x2: torch.per_channel_affine_float_qparams,
     }
@@ -182,9 +101,6 @@ def get_weight_only_quant_qconfig_mapping(
             dtype=weight_dtype, qscheme=weight_qscheme
         ),
         lowp_mode=lowp_mode,
-        act_quant_mode=act_quant_mode,
-        weight_dtype=weight_dtype,
-        group_size=group_size,
     )
     weight_only_quant_qconfig_mapping = QConfigMapping().set_global(
         _weight_only_quant_qconfig
diff --git a/intel_extension_for_pytorch/quantization/_quantization_state.py b/intel_extension_for_pytorch/quantization/_quantization_state.py
index 37a3d86f8..d165e52e5 100644
--- a/intel_extension_for_pytorch/quantization/_quantization_state.py
+++ b/intel_extension_for_pytorch/quantization/_quantization_state.py
@@ -3,6 +3,7 @@ import torch
 import torch.nn.functional as F
 import intel_extension_for_pytorch._C as core
 from intel_extension_for_pytorch.nn.modules import MergedEmbeddingBagWithCat
+from intel_extension_for_pytorch.nn.modules import MergedEmbWithCat
 
 from ._utils import (
     OpQuantizeabilityType,
@@ -309,7 +310,7 @@ class AutoQuantizationState(torch.nn.Module):
                         observer(op._flat_weights[i])
                     else:
                         pass
-                elif isinstance(op, MergedEmbeddingBagWithCat):
+                elif isinstance(op, (MergedEmbeddingBagWithCat, MergedEmbWithCat)):
                     observer(op.weights[i])
                 else:
                     observer(op.weight)
@@ -391,33 +392,7 @@ class AutoQuantizationState(torch.nn.Module):
         act_key = str(self.idx)
         if act_key in self.idx_to_smooth_quant_scaling_factor:
             act_scaling_factors = self.idx_to_smooth_quant_scaling_factor[act_key]
-            # if users modifies qconf.json and cancals quantization of the linear,
-            # then any_arg_quant_or_dequant_needed[0] is False. Don't insert mul in this case.
-            if act_scaling_factors is not None and any_arg_quant_or_dequant_needed[0]:
-                w_key = str(self.idx) + "_0"
-                act_scaling_factors = (
-                    act_scaling_factors[w_key]
-                    if len(act_scaling_factors) > 1
-                    else next(iter(act_scaling_factors.values()))
-                )
-                # update arg_quant_infos
-                if isinstance(arg_quant_infos[0][0], dict):
-                    scale = (
-                        arg_quant_infos[0][0][w_key]
-                        if len(arg_quant_infos[0][0]) > 1
-                        else next(iter(arg_quant_infos[0][0].values()))
-                    )
-                    zp = (
-                        arg_quant_infos[0][1][w_key]
-                        if len(arg_quant_infos[0][1]) > 1
-                        else next(iter(arg_quant_infos[0][1].values()))
-                    )
-                else:
-                    # For backward compatibility
-                    assert isinstance(arg_quant_infos[0][0], torch.Tensor)
-                    scale = arg_quant_infos[0][0]
-                    zp = arg_quant_infos[0][1]
-                arg_quant_infos = [(scale, zp, arg_quant_infos[0][2])]
+            if act_scaling_factors is not None:
                 args = list(args)
                 new_act = torch.mul(args[0], act_scaling_factors)
                 args[0] = new_act
@@ -529,7 +504,7 @@ class AutoQuantizationState(torch.nn.Module):
                 new_args.append(arg)
             else:
                 new_args.append(op.weight)
-        elif isinstance(op, MergedEmbeddingBagWithCat):
+        elif isinstance(op, (MergedEmbeddingBagWithCat, MergedEmbWithCat)):
             weights = op.weights
             for tensor_arg_idx in range(0, len(arg_quant_infos)):
                 quant_info = arg_quant_infos[tensor_arg_idx]
@@ -901,7 +876,7 @@ class AutoQuantizationState(torch.nn.Module):
             weight_tensor_infos = []
             weight_idx = 0
             if type(op) in quantized_modules_has_weights:
-                if isinstance(op, (torch.nn.LSTM, MergedEmbeddingBagWithCat)):
+                if isinstance(op, (torch.nn.LSTM, MergedEmbeddingBagWithCat, MergedEmbWithCat)):
                     if isinstance(op, torch.nn.LSTM):
                         weights = op._flat_weights
                     else:
@@ -1026,6 +1001,7 @@ class AutoQuantizationState(torch.nn.Module):
                 if seen_q_op_info.type in (
                     str(torch.nn.EmbeddingBag),
                     str(MergedEmbeddingBagWithCat),
+                    str(MergedEmbWithCat),
                 ):
                     obs = qconfig.activation()
                     self.weight_tensor_id_to_observer[
@@ -1076,29 +1052,19 @@ class AutoQuantizationState(torch.nn.Module):
                 str(seen_q_op_info.idx) + "_" + str(w_tensor_id)
             ]
             # Duplicate input:
-            # (1) In some cases, multiple linear layers share the same activation (like QKV).
-            #   - If qconfig specifies share_weight_observers=True (default), we regard these
-            #     weights as a single big tensor (i.e., concat along OC axis) during
-            #     calibration. So, these weights share the same per-IC observer.
-            #     But weights are not actually concated for computation.
-            #   - If qconfig specifies share_weight_observers=False, they use different observers.
+            # (1) In modules like MHA, multiple linear layers may share the same activation tensor
+            #   In other words, multiple weight tensors share one activation tensor
+            #   In this case, we regard these weights as a single big tensor (i.e., concat along OC axis).
+            #   When calculating scaling factor, consider per-IC min/max of the big tensor
+            #   So, these weights share the same per-IC observer
             # (2) It is also possible that linear shares activation with some non-weighted op.
             #   In that case, x_obs.weight_obs is not set. Also check it here.
-            w_id_str = str(seen_q_op_info.idx) + "_" + str(w_tensor_id)
             if not found_duplicate_input or x_obs.weight_obs is None:
-                x_obs.weight_obs = {w_id_str: w_obs.ic_obs}
+                x_obs.weight_obs = w_obs.ic_obs
             else:
-                # The input (activation) is shared by more than one linear layers
-                if getattr(qconfig, "share_weight_observers", True):
-                    # Weights of these layers share the same per-IC observer
-                    assert (
-                        isinstance(x_obs.weight_obs, dict)
-                        and len(x_obs.weight_obs) == 1
-                    )
-                    w_obs.ic_obs = next(iter(x_obs.weight_obs.values()))
-                else:
-                    # Weights of these layers use different observers
-                    x_obs.weight_obs.update({w_id_str: w_obs.ic_obs})
+                # The input (activation) has been used by other linear ops
+                # Weight should share the same per-IC observer with that linear
+                w_obs.ic_obs = x_obs.weight_obs
             # In all cases, weight observer holds a reference to activation's per-IC observer
             w_obs.act_obs = x_obs.ic_obs
             # For all linear ops, set smooth_quant_enabled to true
diff --git a/intel_extension_for_pytorch/quantization/_quantization_state_utils.py b/intel_extension_for_pytorch/quantization/_quantization_state_utils.py
index 033a59666..eb7e8a68d 100644
--- a/intel_extension_for_pytorch/quantization/_quantization_state_utils.py
+++ b/intel_extension_for_pytorch/quantization/_quantization_state_utils.py
@@ -5,6 +5,7 @@ import torch.nn.functional as F
 import torch.nn.quantized.dynamic as nnqd
 from intel_extension_for_pytorch.nn.functional import interaction
 from intel_extension_for_pytorch.nn.modules import MergedEmbeddingBagWithCat
+from intel_extension_for_pytorch.nn.modules import MergedEmbWithCat
 import intel_extension_for_pytorch._C as core
 
 
@@ -52,6 +53,7 @@ functions_supported_by_quantization_ipex = set(
         interaction,
         torch.ops.torch_ipex.interaction_forward,
         torch.ops.torch_ipex.merged_embeddingbag_cat_forward,
+        torch.ops.torch_ipex.mlperf_merged_emb_cat,
     ]
 )
 
@@ -73,6 +75,7 @@ module_types_supported_by_quantization = set(
         # torch.nn.GELU,     # TODO
         torch.nn.EmbeddingBag,
         MergedEmbeddingBagWithCat,
+        MergedEmbWithCat,
         torch.nn.Flatten,
         torch.nn.LSTM,
         # dynamic quantization module
@@ -258,6 +261,7 @@ def get_input_observed_arg_idxs(
     if op_type_is_module and op_type not in (
         str(torch.nn.EmbeddingBag),
         str(MergedEmbeddingBagWithCat),
+        str(MergedEmbWithCat),
     ):
         # TODO(future PR): handle RNNs
         return [0]
diff --git a/intel_extension_for_pytorch/quantization/_quantize_utils.py b/intel_extension_for_pytorch/quantization/_quantize_utils.py
index ecb920a26..71a2938f7 100644
--- a/intel_extension_for_pytorch/quantization/_quantize_utils.py
+++ b/intel_extension_for_pytorch/quantization/_quantize_utils.py
@@ -783,244 +783,3 @@ def auto_convert(
     swap_child_modules(module)
     module.__class__ = QuantizationDispatchModule
     return module
-
-
-def quantize_per_channel(t: torch.Tensor, is_int4, scales=None, zero_points=None):
-    r"""
-    Quantize a weight tensor of Linear modules per channel.
-    Assume the tensor shape is [output channel, input channel],
-    each output channel has its own quantization parameters.
-
-    Args:
-        input: The tensor to be quantized
-        is_int4: int4 or int8
-
-    Returns:
-        A tuple of
-        - The quantized tensor
-        - Scales
-        - Zero points
-    """
-    assert t.ndim == 2
-
-    def get_qparams(scales, zps):
-        if scales is not None and zps is not None:
-            return scales, zps
-        eps = torch.tensor([torch.finfo(torch.float32).eps])
-        zeros = torch.zeros(t.shape[0], dtype=t.dtype, device=t.device)
-        mins = torch.minimum(t.min(dim=1)[0], zeros)
-        maxs = torch.maximum(t.max(dim=1)[0], zeros)
-        scales = (maxs - mins) / 15 if is_int4 else (maxs - mins) / 255
-        scales = torch.max(scales, eps)
-        zps = -torch.round(mins / scales)
-        if not is_int4:
-            zps -= 128
-        return scales, zps
-
-    scales, zps = get_qparams(scales, zero_points)
-    qmin = 0 if is_int4 else -128
-    qmax = 15 if is_int4 else 127
-    inv_scales = 1 / scales.unsqueeze(1)
-    qt = torch.clamp(
-        torch.round(t * inv_scales) + zps.unsqueeze(1), min=qmin, max=qmax
-    )
-    qt = qt.to(torch.uint8) if is_int4 else qt.to(torch.int8)
-    if is_int4:
-        if qt.size(-1) % 2:
-            qt = torch.nn.functional.pad(qt, (0, 1), value=0)
-        qt = qt[:, 1::2].bitwise_left_shift(4).bitwise_or_(qt[:, ::2])
-    return qt.contiguous(), scales, zps
-
-
-def dequantize_per_channel(
-    qt: torch.Tensor,
-    scales: torch.Tensor,
-    zps: torch.Tensor,
-    is_int4,
-    weight_shape=None,
-):
-    r"""
-    Dequantize a weight tensor of Linear modules per channel.
-    Assume the tensor shape is [output channel, input channel],
-    each output channel has its own quantization parameters.
-
-    Args:
-        qt: The tensor to be dequantized
-        scales: Scales for dequantization
-        zps: Zero points for dequantization
-        is_int4: int4 or int8
-        weight_shape: True weight shape. INT4 tensor's input channel may
-            be padded to even, so we need this to return the correct weight.
-
-    Returns:
-        The dequantized tensor
-    """
-    assert qt.ndim == 2
-    scales = scales.squeeze()
-    zps = zps.squeeze()
-    if is_int4:
-        t = torch.empty(
-            qt.shape[0], qt.shape[1] * 2, dtype=torch.uint8, device=qt.device
-        )
-        t[:, ::2] = qt.bitwise_and(0xF)
-        t[:, 1::2] = qt.bitwise_right_shift(4)
-        t = (t.to(torch.float) - zps.unsqueeze(-1)) * scales.unsqueeze(-1)
-        if weight_shape is not None:
-            t = t[: weight_shape[0], : weight_shape[1]].contiguous()
-        return t
-    else:
-        return (qt.to(torch.float) - zps.unsqueeze(-1)) * scales.unsqueeze(-1)
-
-
-def quantize_per_block(
-    input: torch.Tensor, is_int4, group_size, scales=None, zero_points=None
-):
-    r"""
-    Quantize a weight tensor of Linear modules per block.
-    Assume the tensor shape is [output channel, input channel],
-    block shape is [1, group_size].
-
-    Args:
-        input: The tensor to be quantized
-        is_int4: int4 or int8
-        group_size: Size of group along input channel
-        scales: Scales for quantization. If None, find by min/max.
-        zero_points: zero points for quantization. If None, find by min/max.
-
-    Returns:
-        A tuple of
-        - The quantized tensor
-        - Scales in shape [N, #block_k]
-        - Zero points in shape [N, #block_k]
-    """
-    assert (
-        input.dim() == 2
-    ), f"{__name__}: Expect input has 2 dimensions but got {input.dim()}"
-    assert group_size > 0, f"{__name__}: Expect group_size > 0 but got {group_size}"
-    N = input.size(0)
-    K = input.size(1)
-    k_rem = K % group_size
-    has_rem = k_rem != 0
-
-    def get_qparams(scales, zps):
-        if scales is not None and zps is not None:
-            return scales, zps
-        eps = torch.tensor([torch.finfo(torch.float32).eps])
-        t_com = input[:, : K - k_rem].view(N, K // group_size, group_size)
-        mins = torch.minimum(t_com.min(dim=-1)[0], torch.tensor([0]))
-        maxs = torch.maximum(t_com.max(dim=-1)[0], torch.tensor([0]))
-        scales = (maxs - mins) / 15 if is_int4 else (maxs - mins) / 255
-        scales = torch.max(scales, eps)
-        zps = -torch.round(mins / scales)
-        if k_rem != 0:
-            t_rem = input[:, K - k_rem :].view(N, 1, k_rem)
-            mins_rem = torch.minimum(t_rem.min(dim=-1)[0], torch.tensor([0]))
-            maxs_rem = torch.maximum(t_rem.max(dim=-1)[0], torch.tensor([0]))
-            scales_rem = (
-                (maxs_rem - mins_rem) / 15 if is_int4 else (maxs_rem - mins_rem) / 255
-            )
-            zps_rem = -torch.round(mins_rem / scales_rem)
-            scales = torch.cat([scales, scales_rem], dim=-1)
-            zps = torch.cat([zps, zps_rem], dim=-1)
-        if not is_int4:
-            zps -= 128
-        return scales, zps
-
-    scales, zps = get_qparams(scales, zero_points)
-    qmin = 0 if is_int4 else -128
-    qmax = 15 if is_int4 else 127
-    Kc = (K + group_size - 1) // group_size
-    t_com = input[:, : K - k_rem].view(N, K // group_size, group_size)
-    scales_com = scales[:, : Kc - has_rem]
-    zps_com = zps[:, : Kc - has_rem]
-    inv_scales_com = 1 / scales_com.unsqueeze(-1)
-    qt = torch.clamp(
-        torch.round(t_com * inv_scales_com) + zps_com.unsqueeze(-1),
-        min=qmin,
-        max=qmax,
-    )
-    qt = qt.view(N, K // group_size * group_size)
-    if k_rem != 0:
-        t_rem = input[:, K - k_rem :].view(N, 1, k_rem)
-        scales_rem = scales[:, Kc - has_rem :]
-        zps_rem = zps[:, Kc - has_rem :]
-        inv_scales_rem = 1 / scales_rem.unsqueeze(-1)
-        qt_rem = torch.clamp(
-            torch.round(t_rem * inv_scales_rem) + zps_rem.unsqueeze(-1),
-            min=qmin,
-            max=qmax,
-        )
-        qt_rem = qt_rem.view(N, k_rem)
-        qt = torch.cat([qt, qt_rem], dim=1).contiguous()
-    qt = qt.to(torch.uint8) if is_int4 else qt.to(torch.int8)
-    qt = qt.view(N, K)
-    if is_int4:
-        if qt.size(-1) % 2:
-            qt = torch.nn.functional.pad(qt, (0, 1), value=0)
-        qt = qt[:, 1::2].bitwise_left_shift(4).bitwise_or_(qt[:, ::2])
-    return qt.contiguous(), scales, zps
-
-
-def dequantize_per_block(
-    qt: torch.Tensor,
-    scales: torch.Tensor,
-    zps: torch.Tensor,
-    is_int4,
-    group_size,
-    weight_shape=None,
-):
-    r"""
-    Dequantize a weight tensor of Linear modules per block.
-    Assume the tensor shape is [output channel, input channel],
-    block shape is [1, group_size].
-
-    Args:
-        qt: The tensor to be dequantized
-        scales: Scales in shape [N, #block_k]
-        zps: Zero points in shape [N, #block_k]
-        is_int4: int4 or int8
-        group_size: Size of group along input channel
-        block_oc: Block size of output channel, should be the same for weight packing
-
-    Returns:
-        The dequantized tensor
-    """
-    N = qt.size(0)
-    K = qt.size(1) * 2 if is_int4 else qt.size(1)
-    if scales.dim() > 2:
-        scales = scales.squeeze()
-        zps = zps.squeeze()
-    if is_int4:
-        t = torch.empty(
-            qt.shape[0], qt.shape[1] * 2, dtype=torch.uint8, device=qt.device
-        )
-        t[:, ::2] = qt.bitwise_and(0xF)
-        t[:, 1::2] = qt.bitwise_right_shift(4)
-        qt = t
-    k_rem = K % group_size
-    has_rem = k_rem != 0
-    Kc = (K + group_size - 1) // group_size
-    qt_com = qt[:, : K - k_rem].view(N, K // group_size, group_size)
-    scales_com = scales[:, : Kc - has_rem]
-    zps_com = zps[:, : Kc - has_rem]
-    t = (
-        ((qt_com.to(torch.float) - zps_com.unsqueeze(-1)) * scales_com.unsqueeze(-1))
-        .view(N, K - k_rem)
-        .contiguous()
-    )
-    if k_rem:
-        qt_rem = qt[:, K - k_rem :].view(N, 1, k_rem)
-        scales_rem = scales[:, Kc - has_rem :]
-        zps_rem = zps[:, Kc - has_rem :]
-        t_rem = (
-            (
-                (qt_rem.to(torch.float) - zps_rem.unsqueeze(-1))
-                * scales_rem.unsqueeze(-1)
-            )
-            .view(N, k_rem)
-            .contiguous()
-        )
-        t = torch.cat([t, t_rem], dim=1).contiguous()
-    if weight_shape is not None:
-        t = t[: weight_shape[0], : weight_shape[1]].contiguous()
-    return t
diff --git a/intel_extension_for_pytorch/quantization/_recipe.py b/intel_extension_for_pytorch/quantization/_recipe.py
index 20bce355f..bf43c0f61 100644
--- a/intel_extension_for_pytorch/quantization/_recipe.py
+++ b/intel_extension_for_pytorch/quantization/_recipe.py
@@ -3,6 +3,7 @@ import torch.nn as nn
 import torch.nn.functional as F
 from intel_extension_for_pytorch.nn.functional import interaction
 from intel_extension_for_pytorch.nn.modules import MergedEmbeddingBagWithCat
+from intel_extension_for_pytorch.nn.modules import MergedEmbWithCat
 
 from ._utils import ParentNode, set_node_output_quantized
 
@@ -71,6 +72,7 @@ s8_s8_symmetric_ops = [
     str(interaction),
     str(torch.ops.torch_ipex.interaction_forward),
     str(torch.ops.torch_ipex.merged_embeddingbag_cat_forward),
+    str(torch.ops.torch_ipex.mlperf_merged_emb_cat),
     str(torch.embedding_bag),
     str(F.embedding_bag),
     str(torch.nn.EmbeddingBag),
@@ -343,6 +345,7 @@ def _check_has_quantizable_node_before_node(node):
                     str(interaction),
                     str(torch.ops.torch_ipex.interaction_forward),
                     str(torch.ops.torch_ipex.merged_embeddingbag_cat_forward),
+                    str(torch.ops.torch_ipex.mlperf_merged_emb_cat),
                 ]:
                     for force_inf_dtype in node.input_tensor_force_inf_dtype:
                         if force_inf_dtype == torch.qint8:
@@ -533,7 +536,9 @@ def get_default_recipe(nodes):
         str(F.embedding_bag),
         str(torch.nn.EmbeddingBag),
         str(MergedEmbeddingBagWithCat),
+        str(MergedEmbWithCat),
         str(torch.ops.torch_ipex.merged_embeddingbag_cat_forward),
+        str(torch.ops.torch_ipex.mlperf_merged_emb_cat),
     ]
     for node in nodes:
         if isinstance(node, ParentNode):
diff --git a/intel_extension_for_pytorch/quantization/_smooth_quant.py b/intel_extension_for_pytorch/quantization/_smooth_quant.py
index ae16301e8..a56e5a4c2 100644
--- a/intel_extension_for_pytorch/quantization/_smooth_quant.py
+++ b/intel_extension_for_pytorch/quantization/_smooth_quant.py
@@ -24,6 +24,9 @@ class SmoothQuantActivationObserver(UniformQuantizationObserverBase):
       just act as a normal observer
     """
 
+    # As a 1d tensor, not diagonal
+    scaling_factors: torch.Tensor
+
     def __init__(
         self,
         act_observer=None,
@@ -60,7 +63,14 @@ class SmoothQuantActivationObserver(UniformQuantizationObserverBase):
                 eps=eps,
             )
         else:
-            self.ic_obs = act_ic_observer()
+            assert isinstance(act_ic_observer, UniformQuantizationObserverBase), (
+                f"act_ic_observer should be an instance of UniformQuantizationObserverBase "
+                f"or its subclass but got {type(act_ic_observer)}"
+            )
+            assert hasattr(
+                act_ic_observer, "ch_axis"
+            ), "act_ic_observer should be a per-channel observer and observe input channel axis"
+            self.ic_obs = act_ic_observer
         if act_observer is None:
             self.act_obs = HistogramObserver(
                 dtype=dtype,
@@ -72,7 +82,8 @@ class SmoothQuantActivationObserver(UniformQuantizationObserverBase):
                 eps=eps,
             )
         else:
-            self.act_obs = act_observer()
+            assert isinstance(act_observer, UniformQuantizationObserverBase)
+            self.act_obs = act_observer
         # if smooth_quant_enabled is false, this observer acts as
         # a normal per-tensor observer
         self.smooth_quant_enabled = smooth_quant_enabled
@@ -81,14 +92,10 @@ class SmoothQuantActivationObserver(UniformQuantizationObserverBase):
         # They are for checks, like `_check_observer_has_run`
         self.min_val = self.act_obs.min_val
         self.max_val = self.act_obs.max_val
-        # Dict of tensors. Keys are weight IDs. Factors are 1d tensors, not diagonal
-        self.scaling_factors = {}
 
     def forward(self, x_orig):
         if not self.smooth_quant_enabled:
             return self.act_obs.forward(x_orig)
-        # Run act_obs to indicate the observer has run
-        self.act_obs.forward(x_orig)
         # Call per-channel observer on IC to find scaling factor
         return self.ic_obs.forward(x_orig)
 
@@ -96,37 +103,31 @@ class SmoothQuantActivationObserver(UniformQuantizationObserverBase):
     def calculate_qparams(self):
         if not self.smooth_quant_enabled:
             return self.act_obs.calculate_qparams()
-        scales, zero_points = {}, {}
-        for k in self.weight_obs.keys():
-            # Get weight per IC min/max from weight observer
-            wei_min_per_ic = self.weight_obs[k].min_val
-            wei_max_per_ic = self.weight_obs[k].max_val
-            act_min_per_ic = self.ic_obs.min_val
-            act_max_per_ic = self.ic_obs.max_val
-            x_abs_max_per_ic = (
-                torch.max(torch.abs(act_min_per_ic), torch.abs(act_max_per_ic)) + 1e-6
-            )
-            w_abs_max_per_ic = (
-                torch.max(torch.abs(wei_min_per_ic), torch.abs(wei_max_per_ic)) + 1e-6
-            )
-            # Note: activation's scaling factors are reciprocals of weight's
-            scaling_factor = torch.pow(w_abs_max_per_ic, 1 - self.alpha) / torch.pow(
-                x_abs_max_per_ic, self.alpha
-            )
-            self.scaling_factors.update({k: scaling_factor})
-            # Apply scaling factors to each IC's min/max
-            act_min_per_ic_new = act_min_per_ic * scaling_factor.reshape(
-                act_min_per_ic.shape
-            )
-            act_max_per_ic_new = act_max_per_ic * scaling_factor.reshape(
-                act_max_per_ic.shape
-            )
-            min_val_per_tensor = torch.min(act_min_per_ic_new)
-            max_val_per_tensor = torch.max(act_max_per_ic_new)
-            scale, zp = self._calculate_qparams(min_val_per_tensor, max_val_per_tensor)
-            scales.update({k: scale})
-            zero_points.update({k: zp})
-        return scales, zero_points
+        # Get weight per IC min/max from weight observer
+        wei_min_per_ic = self.weight_obs.min_val
+        wei_max_per_ic = self.weight_obs.max_val
+        act_min_per_ic = self.ic_obs.min_val
+        act_max_per_ic = self.ic_obs.max_val
+        x_abs_max_per_ic = (
+            torch.max(torch.abs(act_min_per_ic), torch.abs(act_max_per_ic)) + 1e-6
+        )
+        w_abs_max_per_ic = (
+            torch.max(torch.abs(wei_min_per_ic), torch.abs(wei_max_per_ic)) + 1e-6
+        )
+        # Note: activation's scaling factors are reciprocals of weight's
+        self.scaling_factors = torch.pow(w_abs_max_per_ic, 1 - self.alpha) / torch.pow(
+            x_abs_max_per_ic, self.alpha
+        )
+        # Apply scaling factors to each IC's min/max
+        act_min_per_ic_new = act_min_per_ic * self.scaling_factors.reshape(
+            act_min_per_ic.shape
+        )
+        act_max_per_ic_new = act_max_per_ic * self.scaling_factors.reshape(
+            act_max_per_ic.shape
+        )
+        min_val_per_tensor = torch.min(act_min_per_ic_new)
+        max_val_per_tensor = torch.max(act_max_per_ic_new)
+        return self._calculate_qparams(min_val_per_tensor, max_val_per_tensor)
 
     def get_scaling_factors(self):
         if not self.smooth_quant_enabled:
@@ -197,7 +198,8 @@ class SmoothQuantWeightObserver(UniformQuantizationObserverBase):
                 eps=eps,
             )
         else:
-            self.oc_obs = wei_observer()
+            assert isinstance(wei_observer, UniformQuantizationObserverBase)
+            self.oc_obs = wei_observer
         if wei_ic_observer is None:
             self.ic_obs = PerChannelMinMaxObserver(
                 ch_axis=1,
@@ -210,7 +212,14 @@ class SmoothQuantWeightObserver(UniformQuantizationObserverBase):
                 eps=eps,
             )
         else:
-            self.ic_obs = wei_ic_observer()
+            assert isinstance(wei_ic_observer, UniformQuantizationObserverBase), (
+                f"wei_ic_observer should be an instance of UniformQuantizationObserverBase "
+                f"or its subclass but got {type(wei_ic_observer)}"
+            )
+            assert hasattr(
+                wei_ic_observer, "ch_axis"
+            ), "wei_ic_observer should be a per-channel observer and observe input channel axis"
+            self.ic_obs = wei_ic_observer
         # if smooth_quant_enabled is false, this observer acts as
         # a normal observer
         self.smooth_quant_enabled = smooth_quant_enabled
diff --git a/intel_extension_for_pytorch/quantization/_utils.py b/intel_extension_for_pytorch/quantization/_utils.py
index b330b7109..f65d80631 100644
--- a/intel_extension_for_pytorch/quantization/_utils.py
+++ b/intel_extension_for_pytorch/quantization/_utils.py
@@ -14,8 +14,8 @@ from intel_extension_for_pytorch.nn.functional import interaction
 
 from ._quantization_state_utils import QTensorInfo
 from ._smooth_quant import SmoothQuantActivationObserver, SmoothQuantWeightObserver
-from ._qconfig import QConfigSmoothQuant
 from intel_extension_for_pytorch.nn.modules import MergedEmbeddingBagWithCat
+from intel_extension_for_pytorch.nn.modules import MergedEmbWithCat
 
 add_and_mul_ops = set(
     [
@@ -32,6 +32,7 @@ quantized_modules_has_weights = set(
         torch.nn.Linear,
         torch.nn.EmbeddingBag,
         MergedEmbeddingBagWithCat,
+        MergedEmbWithCat,
         torch.nn.ConvTranspose2d,
         torch.nn.ConvTranspose3d,
         torch.nn.LSTM,
@@ -71,10 +72,12 @@ int8_int8_ops = set(
         str(interaction),
         str(torch.ops.torch_ipex.interaction_forward),
         str(torch.ops.torch_ipex.merged_embeddingbag_cat_forward),
+        str(torch.ops.torch_ipex.mlperf_merged_emb_cat),
         str(torch.embedding_bag),
         str(F.embedding_bag),
         str(torch.nn.EmbeddingBag),
         str(MergedEmbeddingBagWithCat),
+        str(MergedEmbWithCat),
         str(torch.nn.LSTM),
     ]
 )
@@ -472,16 +475,19 @@ def _check_after_nodes_all_quantized_give_node(node):
                     str(interaction),
                     str(torch.ops.torch_ipex.interaction_forward),
                     str(torch.ops.torch_ipex.merged_embeddingbag_cat_forward),
+                    str(torch.ops.torch_ipex.mlperf_merged_emb_cat),
                     str(torch.embedding_bag),
                     str(F.embedding_bag),
                     str(torch.nn.EmbeddingBag),
                     str(MergedEmbeddingBagWithCat),
+                    str(MergedEmbWithCat),
                 ]
                 if next.type in int8_int8_symmetric_ops:
                     if next.type in [
                         str(interaction),
                         str(torch.ops.torch_ipex.interaction_forward),
                         str(torch.ops.torch_ipex.merged_embeddingbag_cat_forward),
+                        str(torch.ops.torch_ipex.mlperf_merged_emb_cat),
                     ]:
                         # node.input_tensor_infos may be set, we can use force_inf_dtype to check whether this op is quantizabled.
                         for force_inf_dtype in next.input_tensor_force_inf_dtype:
@@ -529,6 +535,7 @@ def set_node_output_quantized(nodes):
             if node.type in (
                 str(torch.nn.EmbeddingBag),
                 str(MergedEmbeddingBagWithCat),
+                str(MergedEmbWithCat),
             ):
                 if (
                     node.weight_tensor_infos[0].inf_dtype == torch.qint8
@@ -549,6 +556,8 @@ def set_node_output_quantized(nodes):
                 str(interaction),
                 str(torch.ops.torch_ipex.interaction_forward),
                 str(torch.ops.torch_ipex.merged_embeddingbag_cat_forward),
+                str(torch.ops.torch_ipex.mlperf_merged_emb_cat),
+                str(MergedEmbWithCat),
             ]:
                 if (
                     node.input_tensor_force_inf_dtype[0] == torch.qint8
@@ -660,7 +669,7 @@ def _create_observer(setting):
         ]
         for key in smooth_quant_sub_obs_keys:
             if key in setting:
-                setting[key] = _create_observer(setting[key])
+                setting[key] = _create_observer(setting[key])()
         return observer.with_args(**setting)
     else:
         raise NameError("torch.quantization.observer %s not found" % setting["name"])
@@ -693,29 +702,12 @@ def save_quant_state(quant_state_map, configure_file):
                         cur_tensor_infos["inf_dtype"] = str(tensor_info.inf_dtype)
                         cur_tensor_infos["force_dtype"] = str(force_dtype)
                         if tensor_info.id in v.tensor_id_to_scale_zp:
-                            if isinstance(
-                                v.tensor_id_to_scale_zp[tensor_info.id][0], torch.Tensor
-                            ):
-                                cur_tensor_infos["scale"] = v.tensor_id_to_scale_zp[
-                                    tensor_info.id
-                                ][0].tolist()
-                                cur_tensor_infos[
-                                    "zero_point"
-                                ] = v.tensor_id_to_scale_zp[tensor_info.id][1].tolist()
-                            else:
-                                scales_dict = v.tensor_id_to_scale_zp[tensor_info.id][0]
-                                zp_dict = v.tensor_id_to_scale_zp[tensor_info.id][1]
-                                assert isinstance(scales_dict, dict) and isinstance(
-                                    zp_dict, dict
-                                )
-                                scales_to_save = {}
-                                zp_to_save = {}
-                                for key, val in scales_dict.items():
-                                    scales_to_save.update({key: val.tolist()})
-                                for key, val in zp_dict.items():
-                                    zp_to_save.update({key: val.tolist()})
-                                cur_tensor_infos["scale"] = scales_to_save
-                                cur_tensor_infos["zero_point"] = zp_to_save
+                            cur_tensor_infos["scale"] = v.tensor_id_to_scale_zp[
+                                tensor_info.id
+                            ][0].tolist()
+                            cur_tensor_infos["zero_point"] = v.tensor_id_to_scale_zp[
+                                tensor_info.id
+                            ][1].tolist()
                         if (
                             str(tensor_info.id)
                             in v.tensor_id_to_smooth_quant_scaling_factor
@@ -724,18 +716,11 @@ def save_quant_state(quant_state_map, configure_file):
                             ]
                             is not None
                         ):
-                            scaling_factor_dict = (
-                                v.tensor_id_to_smooth_quant_scaling_factor[
-                                    str(tensor_info.id)
-                                ]
-                            )
-                            assert isinstance(scaling_factor_dict, dict)
-                            scaling_factors_to_save = {}
-                            for key, val in scaling_factor_dict.items():
-                                scaling_factors_to_save.update({key: val.tolist()})
                             cur_tensor_infos[
                                 "smooth_quant_scaling_factor"
-                            ] = scaling_factors_to_save
+                            ] = v.tensor_id_to_smooth_quant_scaling_factor[
+                                str(tensor_info.id)
+                            ].tolist()
                             smooth_quant_enabled = True
                     input_tensor_infos.append(cur_tensor_infos)
                 info["input_tensor_infos"] = input_tensor_infos
@@ -774,49 +759,18 @@ def save_quant_state(quant_state_map, configure_file):
                         cur_tensor_infos["orig_dtype"] = str(tensor_info.orig_dtype)
                         cur_tensor_infos["inf_dtype"] = str(tensor_info.inf_dtype)
                         if tensor_info.id in v.tensor_id_to_scale_zp:
-                            if isinstance(
-                                v.tensor_id_to_scale_zp[tensor_info.id][0], torch.Tensor
-                            ):
-                                cur_tensor_infos["scale"] = v.tensor_id_to_scale_zp[
-                                    tensor_info.id
-                                ][0].tolist()
-                                cur_tensor_infos[
-                                    "zero_point"
-                                ] = v.tensor_id_to_scale_zp[tensor_info.id][1].tolist()
-                            else:
-                                scales_dict = v.tensor_id_to_scale_zp[tensor_info.id][0]
-                                zp_dict = v.tensor_id_to_scale_zp[tensor_info.id][1]
-                                assert isinstance(scales_dict, dict) and isinstance(
-                                    zp_dict, dict
-                                )
-                                scales_to_save = {}
-                                zp_to_save = {}
-                                for key, val in scales_dict.items():
-                                    scales_to_save.update({key: val.tolist()})
-                                for key, val in zp_dict.items():
-                                    zp_to_save.update({key: val.tolist()})
-                                cur_tensor_infos["scale"] = scales_to_save
-                                cur_tensor_infos["zero_point"] = zp_to_save
-                        if (
-                            str(tensor_info.id)
-                            in v.tensor_id_to_smooth_quant_scaling_factor
-                        ):
-                            scaling_factors = (
-                                v.tensor_id_to_smooth_quant_scaling_factor[
-                                    str(tensor_info.id)
-                                ]
-                            )
-                            scaling_factors_to_save = None
-                            if scaling_factors is not None:
-                                assert isinstance(
-                                    scaling_factors, dict
-                                ), f"Expect scaling factors is a dict but found {type(scaling_factors)}"
-                                scaling_factors_to_save = {}
-                                for key, val in scaling_factors.items():
-                                    scaling_factors_to_save.update({key: val.tolist()})
+                            cur_tensor_infos["scale"] = v.tensor_id_to_scale_zp[
+                                tensor_info.id
+                            ][0].tolist()
+                            cur_tensor_infos["zero_point"] = v.tensor_id_to_scale_zp[
+                                tensor_info.id
+                            ][1].tolist()
+                        if tensor_info.id in v.tensor_id_to_smooth_quant_scaling_factor:
                             cur_tensor_infos[
                                 "smooth_quant_scaling_factor"
-                            ] = scaling_factors_to_save
+                            ] = v.tensor_id_to_smooth_quant_scaling_factor[
+                                tensor_info.id
+                            ].tolist()
                     output_tensor_infos.append(cur_tensor_infos)
                 info["output_tensor_infos"] = output_tensor_infos
                 # qconfig
@@ -835,9 +789,6 @@ def save_quant_state(quant_state_map, configure_file):
                     info["activation_observer"][
                         "act_ic_observer"
                     ] = _get_observer_setting(op_info.qconfig.activation().ic_obs)
-                    info["share_weight_observers"] = getattr(
-                        op_info.qconfig, "share_weight_observers", True
-                    )
                 info["weight_observer"] = _get_observer_setting(
                     op_info.qconfig.weight()
                 )
@@ -935,47 +886,16 @@ def load_qconf_summary_to_model(model, qconf_summary):
                         dtype_dict[tensor_info["force_dtype"]]
                     )
                     if "scale" in tensor_info:
-                        if isinstance(tensor_info["scale"], list):
-                            scale = torch.FloatTensor(tensor_info["scale"])
-                            zp = torch.LongTensor(tensor_info["zero_point"])
-                        else:
-                            scale, zp = {}, {}
-                            scale_to_load = tensor_info["scale"]
-                            zp_to_load = tensor_info["zero_point"]
-                            assert isinstance(scale_to_load, dict) and isinstance(
-                                zp_to_load, dict
-                            ), (
-                                "Expect scales and zero points to load are dicts but "
-                                f"found types {type(scale_to_load)} and {type(zp_to_load)}"
-                            )
-                            for key, val in scale_to_load.items():
-                                s = torch.FloatTensor(val)
-                                scale.update({key: s})
-                            for key, val in zp_to_load.items():
-                                z = torch.LongTensor(val)
-                                zp.update({key: z})
+                        scale = torch.FloatTensor(tensor_info["scale"])
+                        zp = torch.LongTensor(tensor_info["zero_point"])
                         v.tensor_id_to_scale_zp[tensor_info["id"]] = (scale, zp)
                     if "smooth_quant_scaling_factor" in tensor_info:
-                        scaling_factors = {}
-                        scaling_factors_to_load = tensor_info[
-                            "smooth_quant_scaling_factor"
-                        ]
-                        if isinstance(scaling_factors_to_load, dict):
-                            for key, val in scaling_factors_to_load.items():
-                                scaling_factor = torch.FloatTensor(val)
-                                scaling_factors.update({key: scaling_factor})
-                        else:
-                            # for backward compatibility
-                            assert isinstance(scaling_factors_to_load, list), (
-                                f"Expect scaling factors to load to be a dict or list "
-                                f"but found type {type(scaling_factors_to_load)}"
-                            )
-                            scaling_factor = torch.FloatTensor(scaling_factors_to_load)
-                            dummy_weight_key = "0_0"
-                            scaling_factors.update({dummy_weight_key: scaling_factor})
+                        scaling_factor = torch.FloatTensor(
+                            tensor_info["smooth_quant_scaling_factor"]
+                        )
                         v.tensor_id_to_smooth_quant_scaling_factor[
                             str(tensor_info["id"])
-                        ] = scaling_factors
+                        ] = scaling_factor
                 else:
                     input_tensor_infos.append(None)
                     input_force_dtype_infos.append(None)
@@ -1019,43 +939,17 @@ def load_qconf_summary_to_model(model, qconf_summary):
                     )
                     insert_fake_quant_after_outputs.append(False)
                     if "scale" in tensor_info:
-                        if isinstance(tensor_info["scale"], list):
-                            scale = torch.FloatTensor(tensor_info["scale"])
-                            zp = torch.LongTensor(tensor_info["zero_point"])
-                        else:
-                            scale, zp = {}, {}
-                            scale_to_load = tensor_info["scale"]
-                            zp_to_load = tensor_info["zero_point"]
-                            assert isinstance(scale_to_load, dict) and isinstance(
-                                zp_to_load, dict
-                            ), (
-                                "Expect scales and zero points to load are dicts but "
-                                f"found types {type(scale_to_load)} and {type(zp_to_load)}"
-                            )
-                            for key, val in scale_to_load.items():
-                                s = torch.FloatTensor(val)
-                                scale.update({key: s})
-                            for key, val in zp_to_load.items():
-                                z = torch.LongTensor(val)
-                                zp.update({key: z})
+                        scale = torch.FloatTensor(tensor_info["scale"])
+                        zp = torch.LongTensor(tensor_info["zero_point"])
                         v.tensor_id_to_scale_zp[tensor_info["id"]] = (scale, zp)
                 else:
                     output_tensor_infos.append(None)
             activation_observer = q_op_info["activation_observer"]
             weight_observer = q_op_info["weight_observer"]
-            activation_obs = _create_observer(activation_observer)
-            if isinstance(activation_obs(), SmoothQuantActivationObserver):
-                share_weight_observers = q_op_info.get("share_weight_observers", True)
-                qconfig = QConfigSmoothQuant(
-                    activation=activation_obs,
-                    weight=_create_observer(weight_observer),
-                    share_weight_observers=share_weight_observers,
-                )
-            else:
-                qconfig = QConfig(
-                    activation=activation_obs,
-                    weight=_create_observer(weight_observer),
-                )
+            qconfig = QConfig(
+                activation=_create_observer(activation_observer),
+                weight=_create_observer(weight_observer),
+            )
             # overide the cur model's info
             v.idx_to_seen_q_op_infos[int(i)].input_tensor_infos = input_tensor_infos
             v.idx_to_seen_q_op_infos[
@@ -1263,6 +1157,10 @@ def module_call_to_function_call(module, args, weights):
         output = torch.ops.torch_ipex.merged_embeddingbag_cat_forward(
             weights, args[0], args[1], args[2]
         )
+    elif isinstance(module, MergedEmbWithCat):
+        output = torch.ops.torch_ipex.mlperf_merged_emb_cat(
+            weights, args[0], args[1], module._multi_hot
+        )
     elif isinstance(module, torch.nn.ConvTranspose2d) or isinstance(
         module, torch.nn.ConvTranspose3d
     ):
diff --git a/intel_extension_for_pytorch/transformers/generation/beam_search.py b/intel_extension_for_pytorch/transformers/generation/beam_search.py
index 7643999be..a4e55f82c 100644
--- a/intel_extension_for_pytorch/transformers/generation/beam_search.py
+++ b/intel_extension_for_pytorch/transformers/generation/beam_search.py
@@ -176,7 +176,6 @@ def _beam_search(
             or re.search("OPT", self.config.architectures[0], re.IGNORECASE)
             or re.search("falcon", self.config.architectures[0], re.IGNORECASE)
             or re.search("rw", self.config.architectures[0], re.IGNORECASE)
-            or re.search("codegen", self.config.architectures[0], re.IGNORECASE)
         ):
             first_token = False
             input_bs = input_ids.size()[0]
@@ -184,9 +183,7 @@ def _beam_search(
             if model_inputs["past_key_values"] is None:
                 first_token = True
             if first_token:
-                if re.search("GPTJ", self.config.architectures[0]) or re.search(
-                    "codegen", self.config.architectures[0], re.IGNORECASE
-                ):
+                if re.search("GPTJ", self.config.architectures[0]):
                     beam_idx_tmp = torch.zeros(
                         (2048, int(batch_size * num_beams)), dtype=torch.long
                     ).contiguous()
diff --git a/intel_extension_for_pytorch/transformers/generation/greedy_search.py b/intel_extension_for_pytorch/transformers/generation/greedy_search.py
index fa2fc778c..7e8f08d1e 100644
--- a/intel_extension_for_pytorch/transformers/generation/greedy_search.py
+++ b/intel_extension_for_pytorch/transformers/generation/greedy_search.py
@@ -157,16 +157,13 @@ def _greedy_search(
             or re.search("OPT", self.config.architectures[0], re.IGNORECASE)
             or re.search("falcon", self.config.architectures[0], re.IGNORECASE)
             or re.search("rw", self.config.architectures[0], re.IGNORECASE)
-            or re.search("codegen", self.config.architectures[0], re.IGNORECASE)
         ):
             first_token = False
             input_bs = input_ids.size()[0]
             if model_inputs["past_key_values"] is None:
                 first_token = True
             if first_token:
-                if re.search("GPTJ", self.config.architectures[0]) or re.search(
-                    "codegen", self.config.architectures[0], re.IGNORECASE
-                ):
+                if re.search("GPTJ", self.config.architectures[0]):
                     beam_idx_tmp = torch.zeros(
                         (2048, int(input_bs)), dtype=torch.long
                     ).contiguous()
diff --git a/intel_extension_for_pytorch/transformers/models/cpu/fusions/linear_fusion.py b/intel_extension_for_pytorch/transformers/models/cpu/fusions/linear_fusion.py
index 1634482fe..424225729 100644
--- a/intel_extension_for_pytorch/transformers/models/cpu/fusions/linear_fusion.py
+++ b/intel_extension_for_pytorch/transformers/models/cpu/fusions/linear_fusion.py
@@ -4,12 +4,7 @@ import math
 import warnings
 import copy
 from intel_extension_for_pytorch.nn.modules import IpexWoqLinear
-from intel_extension_for_pytorch.quantization import (
-    get_weight_only_quant_qconfig_mapping,
-    dequantize_per_channel,
-    dequantize_per_block,
-)
-
+from intel_extension_for_pytorch.quantization import get_weight_only_quant_qconfig_mapping
 
 class _IPEXlinearFusionCPU(nn.Module):
     def __init__(self, linear, tpp=False, woq=False):
@@ -19,7 +14,7 @@ class _IPEXlinearFusionCPU(nn.Module):
         self.dtype = linear.weight.dtype if self.tpp else None
 
     def extra_repr(self):
-        extra_repr_str = f"dtype = {self.dtype}, tpp = {self.tpp}, woq = {self.woq}"
+        extra_repr_str = f'dtype = {self.dtype}, tpp = {self.tpp}, woq = {self.woq}'
         return extra_repr_str
 
 
@@ -29,7 +24,7 @@ class _IPEXlinearSiluCPU(_IPEXlinearFusionCPU):
         self.linear = module
 
     def forward(self, x):
-        if self.tpp and not self.linear.tpp_fallback:
+        if self.tpp:
             x = x.to(self.dtype).contiguous()
             return torch.ops.torch_ipex.tpp_linear_silu(
                 x,
@@ -47,7 +42,7 @@ class _IPEXlinearReluCPU(_IPEXlinearFusionCPU):
         self.linear = module
 
     def forward(self, x):
-        if self.tpp and not self.linear.tpp_fallback:
+        if self.tpp:
             x = x.to(self.dtype).contiguous()
             return torch.ops.torch_ipex.tpp_linear_relu(
                 x,
@@ -65,7 +60,7 @@ class _IPEXlinearMulCPU(_IPEXlinearFusionCPU):
         self.linear = module
 
     def forward(self, x, y):
-        if self.tpp and not self.linear.tpp_fallback:
+        if self.tpp:
             x = x.to(self.dtype).contiguous()
             y = y.to(self.dtype).contiguous()
             return torch.ops.torch_ipex.tpp_linear_mul(
@@ -85,7 +80,7 @@ class _IPEXlinearAddCPU(_IPEXlinearFusionCPU):
         self.linear = module
 
     def forward(self, x, y):
-        if self.tpp and not self.linear.tpp_fallback:
+        if self.tpp:
             x = x.to(self.dtype).contiguous()
             y = y.to(self.dtype).contiguous()
             return torch.ops.torch_ipex.tpp_linear_add(
@@ -96,11 +91,8 @@ class _IPEXlinearAddCPU(_IPEXlinearFusionCPU):
                 1.0,
                 self.linear.out_features,
             )
-        if (
-            self.woq
-            and hasattr(self.linear, "_op_context")
-            and self.linear._op_context is not None
-        ):
+        if self.woq and hasattr(self.linear, "_op_context") and \
+                self.linear._op_context is not None:
             return torch.ops.torch_ipex.woq_linear_add(
                 x,
                 self.linear._op_context.get_data_handle(),
@@ -116,7 +108,7 @@ class _IPEXlinearAddAddCPU(_IPEXlinearFusionCPU):
         self.linear = module
 
     def forward(self, x, y, z):
-        if self.tpp and not self.linear.tpp_fallback:
+        if self.tpp:
             x = x.to(self.dtype).contiguous()
             y = y.to(self.dtype).contiguous()
             z = z.to(self.dtype).contiguous()
@@ -129,11 +121,8 @@ class _IPEXlinearAddAddCPU(_IPEXlinearFusionCPU):
                 1.0,
                 self.linear.out_features,
             )
-        if (
-            self.woq
-            and hasattr(self.linear, "_op_context")
-            and self.linear._op_context is not None
-        ):
+        if self.woq and hasattr(self.linear, "_op_context") and \
+                self.linear._op_context is not None:
             return torch.ops.torch_ipex.woq_linear_add_add(
                 x,
                 self.linear._op_context.get_data_handle(),
@@ -149,7 +138,7 @@ class _IPEXlinearNewGeluCPU(_IPEXlinearFusionCPU):
         self.linear = module
 
     def forward(self, x):
-        if self.tpp and not self.linear.tpp_fallback:
+        if self.tpp:
             x = x.to(self.dtype).contiguous()
             return torch.ops.torch_ipex.tpp_linear_gelu(
                 x,
@@ -157,6 +146,12 @@ class _IPEXlinearNewGeluCPU(_IPEXlinearFusionCPU):
                 self.linear.bias if self.linear.bias is not None else x.new_empty(0),
                 self.linear.out_features,
             )
+        if self.woq and hasattr(self.linear, "_op_context") and \
+                self.linear._op_context is not None:
+            return torch.ops.torch_ipex.woq_linear_gelu(
+                x,
+                self.linear._op_context.get_data_handle(),
+            )
 
         else:  # fallback path
             x = self.linear(x)
@@ -179,22 +174,13 @@ class _IPEXlinearGeluCPU(_IPEXlinearFusionCPU):
         self.gelu = nn.GELU()
 
     def forward(self, x):
-        if self.tpp and not self.linear.tpp_fallback:
+        if self.tpp:
             x = x.to(self.dtype).contiguous()
             return torch.ops.torch_ipex.tpp_linear_gelu(
                 x,
                 self.linear.weight,
                 self.linear.bias if self.linear.bias is not None else x.new_empty(0),
             )
-        if (
-            self.woq
-            and hasattr(self.linear, "_op_context")
-            and self.linear._op_context is not None
-        ):
-            return torch.ops.torch_ipex.woq_linear_gelu(
-                x,
-                self.linear._op_context.get_data_handle(),
-            )
         else:  # fallback path
             x = self.gelu(self.linear(x))
             return x
@@ -202,21 +188,26 @@ class _IPEXlinearGeluCPU(_IPEXlinearFusionCPU):
 
 class _IPEXConcatLinearCPU(_IPEXlinearFusionCPU):
     def __init__(self, module, tpp=False, woq=False):
-        assert hasattr(module, "linear_0")
+        assert hasattr(module, 'linear_0')
         super().__init__(module.linear_0, tpp=tpp, woq=woq)
-        assert hasattr(module, "num_concat")
+        assert hasattr(module, 'num_concat')
         self.num_concat = module.num_concat
         self.linear_list = []
         for i in range(self.num_concat):
-            attr_name = f"linear_{i}"
+            attr_name = f'linear_{i}'
             assert hasattr(module, attr_name)
             self.linear_list.append(getattr(module, attr_name))
         self.concat_linear = None
-        if woq and all(
-            isinstance(linear, IpexWoqLinear) for linear in self.linear_list
-        ):
+        if woq and all(isinstance(linear, IpexWoqLinear) for linear in self.linear_list):
             # Quantization is done before lowering to CPU.
-            # We assume weights are all in shape [N, K].
+            # We assume weights are all in shape [N, K] and per-channel quantized, axis = 0.
+            # And it must be one of the two cases below.
+            # Case 1:
+            #   - weight dtype = qint8, qscheme = torch.per_channel_affine,
+            #   - scales dtype = float, zero points dtype = int
+            # Case 2:
+            #   - weight dtype = quint4x2, qscheme = torch.per_channel_affine_float_qparams,
+            #   - scales dtype = float, zero points dtype = float
             # We need to unpack weights then concat them
             weights_list = []
             scales_list = []
@@ -224,86 +215,59 @@ class _IPEXConcatLinearCPU(_IPEXlinearFusionCPU):
             bias_list = []
             w_dtype = self.linear_list[0].dtype
             lowp_mode = self.linear_list[0]._lowp_mode
-            act_quant_mode = self.linear_list[0]._act_quant_mode
-            group_size = self.linear_list[0]._group_size
             qconfig_mapping = get_weight_only_quant_qconfig_mapping(
-                weight_dtype=w_dtype,
-                lowp_mode=lowp_mode,
-                act_quant_mode=act_quant_mode,
-                group_size=group_size,
+                weight_dtype=w_dtype, lowp_mode=lowp_mode
             )
             qconfig = qconfig_mapping.global_qconfig
             for i in range(self.num_concat):
                 linear = self.linear_list[i]
-                if not hasattr(linear, "_op_context"):
+                if not hasattr(linear, '_op_context'):
                     warnings.warn(
-                        "Concat linear fusion for CPU WOQ failed "
-                        "because linear is not converted to WOQ Linear. "
-                        "Falling back to separate linears."
+                        'Concat linear fusion for CPU WOQ failed '
+                        'because linear is not converted to WOQ Linear. '
+                        'Falling back to separate linears.'
                     )
                     weights_list = []
                     break
                 qw = linear._op_context.to_public(linear._op_context.get_weight())
-                scales = linear._op_context.get_scales()
-                zero_points = linear._op_context.get_zero_points()
-                is_int4 = w_dtype == torch.quint4x2
-                weight_shape = linear._op_context.get_weight_shape()
-                if group_size > 0:
-                    weights_list.append(
-                        dequantize_per_block(
-                            qw, scales, zero_points, is_int4, group_size, weight_shape
-                        )
-                    )
-                else:
-                    weights_list.append(
-                        dequantize_per_channel(
-                            qw, scales, zero_points, is_int4, weight_shape
-                        )
+                if qw.qscheme() not in \
+                        [torch.per_channel_affine, torch.per_channel_affine_float_qparams] \
+                        or qw.q_per_channel_axis() != 0:
+                    warnings.warn(
+                        'Concat linear fusion for CPU WOQ failed '
+                        'because quantization type of weight is not supported. '
+                        'Falling back to separate linears.'
                     )
-                # OC of Weight may be padded to a multiple of block_n. So are scales and zero points.
-                bias = linear._op_context.get_bias()
-                assert scales.shape == zero_points.shape
-                assert bias is None or bias.shape[0] == scales.shape[0]
-                if weight_shape[0] < scales.shape[0]:
-                    original_n = weight_shape[0]
-                    scales_list.append(scales.narrow(0, 0, original_n).contiguous())
-                    zeros_list.append(zero_points.narrow(0, 0, original_n).contiguous())
-                    bias_list.append(bias.narrow(0, 0, original_n).contiguous())
-                else:
-                    assert weight_shape[0] == scales.shape[0]
-                    scales_list.append(scales)
-                    zeros_list.append(zero_points)
-                    bias_list.append(bias)
+                    weights_list = []
+                    break
+                s = qw.q_per_channel_scales().float()
+                z = qw.q_per_channel_zero_points().float()
+                weights_list.append(qw.dequantize().float())
+                scales_list.append(s)
+                zeros_list.append(z)
+                bias_list.append(linear._op_context.get_bias())
                 w_dtype = linear.dtype
             if weights_list:
                 concat_weight = torch.concat(weights_list, 0)
-                concat_scales = torch.concat(scales_list, 0)
-                concat_zeros = torch.concat(zeros_list, 0)
+                concat_scales = torch.concat(scales_list, -1)
+                concat_zeros = torch.concat(zeros_list, -1)
                 use_bias = all(bias_list)
                 concat_bias = torch.concat(bias_list, 0) if use_bias else None
-                mod = nn.Linear(
-                    concat_weight.shape[1], concat_weight.shape[0], use_bias
-                )
+                mod = nn.Linear(concat_weight.shape[1], concat_weight.shape[0], use_bias)
                 mod.weight = nn.Parameter(concat_weight)
                 mod.bias = nn.Parameter(concat_bias) if use_bias else None
                 mod.qconfig = qconfig
                 mod._num_concats = len(weights_list)
                 if w_dtype == torch.quint4x2:
                     self.concat_linear = IpexWoqLinear.from_float_and_int4_weight(
-                        mod,
-                        concat_weight,
-                        concat_scales,
-                        concat_zeros,
-                        group_size=group_size,
+                        mod, concat_weight, concat_scales, concat_zeros
                     )
                 else:  # qint8
                     assert w_dtype == torch.qint8
-                    self.concat_linear = IpexWoqLinear.from_float(
-                        mod, concat_scales, concat_zeros
-                    )
+                    self.concat_linear = IpexWoqLinear.from_float(mod)
         else:
             for i in range(self.num_concat):
-                attr_name = f"linear_{i}"
+                attr_name = f'linear_{i}'
                 setattr(self, attr_name, copy.deepcopy(getattr(module, attr_name)))
 
     def forward(self, x):
@@ -313,13 +277,11 @@ class _IPEXConcatLinearCPU(_IPEXlinearFusionCPU):
             hidden_size = concat_output.shape[-1] // num_concats
             concat_output = concat_output.view(num_concats, -1, hidden_size)
             expected_shape = list(x.shape)[:-1] + [hidden_size]
-            return tuple(
-                [concat_output[i].view(expected_shape) for i in range(num_concats)]
-            )
+            return tuple([concat_output[i].view(expected_shape) for i in range(num_concats)])
         output_list = []
         for i in range(self.num_concat):
-            assert hasattr(self, f"linear_{i}")
-            linear = getattr(self, f"linear_{i}")
+            assert hasattr(self, f'linear_{i}')
+            linear = getattr(self, f'linear_{i}')
             y = linear(x)
             output_list.append(y)
         return tuple(output_list)
@@ -335,11 +297,7 @@ class _IPEXlinearSiluMulCPU(nn.Module):
         self.dtype = module_s.weight.dtype if self.tpp else None
 
     def forward(self, x):
-        if (
-            self.tpp
-            and not self.linear_s.tpp_fallback
-            and not self.linear_m.tpp_fallback
-        ):
+        if self.tpp:
             x = x.to(self.dtype).contiguous()
             x1 = torch.ops.torch_ipex.tpp_linear_silu(
                 x,
diff --git a/intel_extension_for_pytorch/transformers/models/cpu/fusions/mha_fusion.py b/intel_extension_for_pytorch/transformers/models/cpu/fusions/mha_fusion.py
index bb3b56061..0c58e3fc1 100644
--- a/intel_extension_for_pytorch/transformers/models/cpu/fusions/mha_fusion.py
+++ b/intel_extension_for_pytorch/transformers/models/cpu/fusions/mha_fusion.py
@@ -91,7 +91,7 @@ class _IPEXScaleDotProductCPU(nn.Module):
         )
 
         present = (
-            torch.empty(
+            torch.zeros(
                 1,
                 (layer_past[0].size(-2) + query.shape[1]),
                 (layer_past[0].size(-2) + query.shape[1]),
diff --git a/intel_extension_for_pytorch/transformers/models/cpu/modules/attentions.py b/intel_extension_for_pytorch/transformers/models/cpu/modules/attentions.py
index 3e098749f..cd267a5c4 100644
--- a/intel_extension_for_pytorch/transformers/models/cpu/modules/attentions.py
+++ b/intel_extension_for_pytorch/transformers/models/cpu/modules/attentions.py
@@ -26,13 +26,9 @@ class _IPEXAttentionCPU(nn.Module):
                 self.rope_base,
                 self.model_backbone,
             )
-
-        if re.search("codegen", self.model_backbone, re.IGNORECASE):
-            self._IPEXROPE.embed_positions.sin_cos = self.embed_positions
-        if re.search("GPTJ", self.model_backbone, re.IGNORECASE) or re.search(
-            "LLAMA", self.model_backbone, re.IGNORECASE
-        ):
-            if hasattr(module, "concat_qkv"):
+        if re.search("GPTJ", self.model_backbone, re.IGNORECASE) or \
+                re.search("LLAMA", self.model_backbone, re.IGNORECASE):
+            if hasattr(module, 'concat_qkv'):
                 self.concat_qkv = _IPEXConcatLinearCPU(
                     module.concat_qkv, tpp=tpp, woq=woq
                 )
diff --git a/intel_extension_for_pytorch/transformers/models/cpu/modules/decoder.py b/intel_extension_for_pytorch/transformers/models/cpu/modules/decoder.py
index 64cbfb32b..73e5e8e9d 100644
--- a/intel_extension_for_pytorch/transformers/models/cpu/modules/decoder.py
+++ b/intel_extension_for_pytorch/transformers/models/cpu/modules/decoder.py
@@ -6,7 +6,7 @@ from ...cpu.fusions.linear_fusion import (
     _IPEXlinearNewGeluCPU,
     _IPEXlinearReluCPU,
     _IPEXlinearGeluCPU,
-    _IPEXlinearSiluMulCPU,
+    _IPEXlinearSiluMulCPU
 )
 
 
@@ -67,14 +67,5 @@ class _IPEXDecoderLayerCPU(nn.Module):
                     self.linear_add = _IPEXlinearAddCPU(
                         module.linear_add.linear, tpp=tpp, woq=woq
                     )
-        elif re.search("codegen", self.model_backbone, re.IGNORECASE):
-            if not self.distributed:
-                self.linear_add_add = _IPEXlinearAddAddCPU(
-                    module.linear_add_add.linear, tpp=tpp, woq=woq
-                )
-            # woq_linear_gelu has accuracy issues on codegen, disable it
-            self.linear_gelu = _IPEXlinearNewGeluCPU(
-                module.linear_gelu.linear, tpp=tpp and not woq, woq=False
-            )
         else:
             AssertionError(False, "Do not support the optimization of your model yet")
diff --git a/intel_extension_for_pytorch/transformers/models/reference/fusions/linear_fusion.py b/intel_extension_for_pytorch/transformers/models/reference/fusions/linear_fusion.py
index cc198628a..7911cc8bd 100644
--- a/intel_extension_for_pytorch/transformers/models/reference/fusions/linear_fusion.py
+++ b/intel_extension_for_pytorch/transformers/models/reference/fusions/linear_fusion.py
@@ -83,20 +83,20 @@ class _IPEXConcatLinearRef(nn.Module):
         super().__init__()
         self.num_concat = len(linear_list)
         for i in range(self.num_concat):
-            attr_name = f"linear_{i}"
+            attr_name = f'linear_{i}'
             setattr(self, attr_name, copy.deepcopy(linear_list[i]))
 
     def forward(self, x):
         output_list = []
         for i in range(self.num_concat):
-            assert hasattr(self, f"linear_{i}")
-            linear = getattr(self, f"linear_{i}")
+            assert hasattr(self, f'linear_{i}')
+            linear = getattr(self, f'linear_{i}')
             y = linear(x)
             output_list.append(y)
         return tuple(output_list)
 
     def extra_repr(self):
-        return f"num_concat = {self.num_concat}"
+        return f'num_concat = {self.num_concat}'
 
 
 class _IPEXlinearSiluMulRef(nn.Module):
diff --git a/intel_extension_for_pytorch/transformers/models/reference/fusions/mha_fusion.py b/intel_extension_for_pytorch/transformers/models/reference/fusions/mha_fusion.py
index 65b78df1c..6be6dd206 100644
--- a/intel_extension_for_pytorch/transformers/models/reference/fusions/mha_fusion.py
+++ b/intel_extension_for_pytorch/transformers/models/reference/fusions/mha_fusion.py
@@ -161,17 +161,6 @@ class _IPEXRopeRef(nn.Module):
             _cos = _cos.type(x.dtype)[:, 0:seq_len]
             _sin = _sin.type(x.dtype)[:, 0:seq_len]
             x = (x * _cos) + (self.rotate_half(x) * _sin)
-        elif re.search("codegen", self.model_backbone, re.IGNORECASE):
-            sincos = _sin_cos[position_ids]
-            sin, cos = torch.split(sincos, sincos.shape[-1] // 2, dim=-1)
-            if rotary_ndims is not None:
-                x_rot = x[:, :, :, :rotary_ndims]
-                x_pass = x[:, :, :, rotary_ndims:]
-
-                x_rot = self.apply_rotary_pos_emb_gptj(x_rot, sin, cos)
-                x = torch.cat([x_rot, x_pass], dim=-1)
-            else:
-                x = self.apply_rotary_pos_emb_gptj(x, sin, cos)
         else:
             AssertionError(False, "Do not support the optimization of your model yet")
         return x
@@ -212,12 +201,6 @@ class _IPEXScaleDotProductRef(nn.Module):
                 if hasattr(module, "new_decoder_architecture")
                 else None
             )
-        elif re.search("codegen", self.model_backbone, re.IGNORECASE):
-            self.num_heads = module.num_attention_heads
-            self.head_dim = module.head_dim
-            self.scale_attn = module.scale_attn
-            self.attn_dropout = module.attn_dropout
-            self.causal_mask = module.causal_mask
 
         for k, v in module.__class__.__dict__.items():
             if k.startswith("__") or k.startswith("forward"):
@@ -264,14 +247,12 @@ class _IPEXScaleDotProductRef(nn.Module):
                 key.permute(0, 2, 1, 3)
                 if re.search("GPTJ", self.model_backbone, re.IGNORECASE)
                 or re.search("OPT", self.model_backbone, re.IGNORECASE)
-                or re.search("codegen", self.model_backbone, re.IGNORECASE)
                 else key
             )
             query = (
                 query.permute(0, 2, 1, 3)
                 if re.search("GPTJ", self.model_backbone, re.IGNORECASE)
                 or re.search("OPT", self.model_backbone, re.IGNORECASE)
-                or re.search("codegen", self.model_backbone, re.IGNORECASE)
                 else query
             )
             value = value.permute(0, 2, 1, 3)
@@ -282,9 +263,7 @@ class _IPEXScaleDotProductRef(nn.Module):
             value = torch.cat((past_value, value), dim=-2)
         present = (key, value)
 
-        if re.search("GPTJ", self.model_backbone, re.IGNORECASE) or re.search(
-            "codegen", self.model_backbone, re.IGNORECASE
-        ):
+        if re.search("GPTJ", self.model_backbone, re.IGNORECASE):
             attn_output, attn_weights = self._attn(
                 query, key, value, attention_mask, head_mask
             )
diff --git a/intel_extension_for_pytorch/transformers/models/reference/models.py b/intel_extension_for_pytorch/transformers/models/reference/models.py
index 0a4f9ec61..488993bb9 100644
--- a/intel_extension_for_pytorch/transformers/models/reference/models.py
+++ b/intel_extension_for_pytorch/transformers/models/reference/models.py
@@ -47,14 +47,6 @@ def GPTJForCausalLM_forward(
         torch.cuda.set_device(self.transformer.first_device)
         hidden_states = hidden_states.to(self.lm_head.weight.device)
 
-    if (
-        hasattr(self, "config")
-        and hasattr(self.config, "lm_head_generation")
-        and self.config.lm_head_generation
-        and hidden_states.size(1) != 1
-    ):
-        hidden_states = hidden_states[:, -1:, :]
-
     # make sure sampling in fp16 works correctly and
     # compute loss in fp32 to match with mesh-tf version
     # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179
@@ -127,14 +119,6 @@ def LlamaForCausalLM_forward(
     )
 
     hidden_states = outputs[0]
-    if (
-        hasattr(self, "config")
-        and hasattr(self.config, "lm_head_generation")
-        and self.config.lm_head_generation
-        and hidden_states.size(1) != 1
-    ):
-        hidden_states = hidden_states[:, -1:, :]
-
     logits = self.lm_head(hidden_states)
 
     loss = None
@@ -194,13 +178,6 @@ def GPTNeoXForCausalLM_forward(
     )
 
     hidden_states = outputs[0]
-    if (
-        hasattr(self, "config")
-        and hasattr(self.config, "lm_head_generation")
-        and self.config.lm_head_generation
-        and hidden_states.size(1) != 1
-    ):
-        hidden_states = hidden_states[:, -1:, :]
     lm_logits = self.embed_out(hidden_states)
 
     lm_loss = None
@@ -267,15 +244,8 @@ def OPTForCausalLM_forward(
         output_hidden_states=output_hidden_states,
         return_dict=return_dict,
     )
-    hidden_states = outputs[0]
-    if (
-        hasattr(self, "config")
-        and hasattr(self.config, "lm_head_generation")
-        and self.config.lm_head_generation
-        and hidden_states.size(1) != 1
-    ):
-        hidden_states = hidden_states[:, -1:, :]
-    logits = self.lm_head(hidden_states).contiguous()
+
+    logits = self.lm_head(outputs[0]).contiguous()
 
     loss = None
     if labels is not None:
@@ -301,79 +271,6 @@ def OPTForCausalLM_forward(
     )
 
 
-def CodeGenForCausalLM_forward(
-    self,
-    input_ids: Optional[torch.LongTensor] = None,
-    attention_mask: Optional[torch.FloatTensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
-    token_type_ids: Optional[torch.LongTensor] = None,
-    head_mask: Optional[torch.FloatTensor] = None,
-    inputs_embeds: Optional[torch.FloatTensor] = None,
-    labels: Optional[torch.LongTensor] = None,
-    use_cache: Optional[bool] = None,
-    output_attentions: Optional[bool] = None,
-    output_hidden_states: Optional[bool] = None,
-    return_dict: Optional[bool] = None,
-) -> Union[Tuple, CausalLMOutputWithPast]:
-    r"""
-    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
-        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
-        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
-    """
-    return_dict = (
-        return_dict if return_dict is not None else self.config.use_return_dict
-    )
-
-    transformer_outputs = self.transformer(
-        input_ids,
-        attention_mask=attention_mask,
-        past_key_values=past_key_values,
-        position_ids=position_ids,
-        token_type_ids=token_type_ids,
-        head_mask=head_mask,
-        inputs_embeds=inputs_embeds,
-        use_cache=use_cache,
-        output_attentions=output_attentions,
-        output_hidden_states=output_hidden_states,
-        return_dict=return_dict,
-    )
-    hidden_states = transformer_outputs[0]
-
-    # make sure sampling in fp16 works correctly and
-    # compute loss in fp32 to match with mesh-tf version
-    # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179
-    lm_logits = self.lm_head(hidden_states).to(torch.float32)
-
-    loss = None
-    if labels is not None:
-        # move labels to correct device to enable model parallelism
-        labels = labels.to(lm_logits.device)
-        # Shift so that tokens < n predict n
-        shift_logits = lm_logits[..., :-1, :].contiguous()
-        shift_labels = labels[..., 1:].contiguous()
-        # Flatten the tokens
-        loss_fct = CrossEntropyLoss()
-        loss = loss_fct(
-            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)
-        )
-
-        loss = loss.to(hidden_states.dtype)
-
-    if not return_dict:
-        output = (lm_logits,) + transformer_outputs[1:]
-        return ((loss,) + output) if loss is not None else output
-
-    return CausalLMOutputWithPast(
-        loss=loss,
-        logits=lm_logits,
-        past_key_values=transformer_outputs.past_key_values,
-        hidden_states=transformer_outputs.hidden_states,
-        attentions=transformer_outputs.attentions,
-    )
-
-
 def prepare_inputs_for_generation(
     self,
     input_ids: torch.LongTensor,
diff --git a/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py b/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py
index 30eb0e015..448133abb 100644
--- a/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py
+++ b/intel_extension_for_pytorch/transformers/models/reference/modules/attentions.py
@@ -11,7 +11,6 @@ from ..fusions.linear_fusion import (
     _IPEXConcatLinearRef,
 )
 
-
 def _GPTJAttention_forward(
     self,
     hidden_states: torch.FloatTensor,
@@ -25,7 +24,7 @@ def _GPTJAttention_forward(
     Tuple[torch.Tensor, Tuple[torch.Tensor]],
     Optional[Tuple[torch.Tensor, Tuple[torch.Tensor], Tuple[torch.Tensor, ...]]],
 ]:
-    if hasattr(self, "concat_qkv"):
+    if hasattr(self, 'concat_qkv'):
         query, key, value = self.concat_qkv(hidden_states)
     else:
         query = self.q_proj(hidden_states)
@@ -112,15 +111,19 @@ def _LlamaAttention_forward(
     use_cache: bool = False,
 ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
     bsz, q_len, _ = hidden_states.size()
-    if hasattr(self, "concat_qkv"):
+    if hasattr(self, 'concat_qkv'):
         query, key, value = self.concat_qkv(hidden_states)
     else:
         query = self.q_proj(hidden_states)
         key = self.k_proj(hidden_states)
         value = self.v_proj(hidden_states)
     query = query.view(bsz, q_len, self.num_heads, self.head_dim)
-    key = key.view(bsz, q_len, self.num_key_value_heads, self.head_dim)
-    value = value.view(bsz, q_len, self.num_key_value_heads, self.head_dim)
+    key = key.view(
+        bsz, q_len, self.num_key_value_heads, self.head_dim
+    )
+    value = value.view(
+        bsz, q_len, self.num_key_value_heads, self.head_dim
+    )
     kv_seq_len = (
         q_len + past_key_value[0].size(-2) if past_key_value is not None else q_len
     )
@@ -530,91 +533,6 @@ def _FalconAttention_forward(
         return output_tensor, present
 
 
-def _CodeGenAttention_forward(
-    self,
-    hidden_states: Optional[torch.FloatTensor],
-    layer_past: Optional[Tuple[torch.Tensor]] = None,
-    attention_mask: Optional[torch.FloatTensor] = None,
-    position_ids: Optional[torch.LongTensor] = None,
-    head_mask: Optional[torch.FloatTensor] = None,
-    use_cache: Optional[bool] = False,
-    output_attentions: Optional[bool] = False,
-) -> Union[
-    Tuple[torch.Tensor, Tuple[torch.Tensor]],
-    Optional[Tuple[torch.Tensor, Tuple[torch.Tensor], Tuple[torch.Tensor, ...]]],
-]:
-    qkv = self.qkv_proj(hidden_states)
-    # TODO(enijkamp): factor out number of logical TPU-v4 cores or make forward pass agnostic
-    mp_num = 4
-    qkv_split = qkv.reshape(qkv.shape[:-1] + (mp_num, -1))
-
-    local_dim = self.head_dim * self.num_attention_heads // mp_num
-    query, value, key = torch.split(qkv_split, local_dim, dim=-1)
-    query = self._split_heads(
-        query, self.num_attention_heads, self.head_dim, mp_num=mp_num
-    ).contiguous()
-    key = self._split_heads(
-        key, self.num_attention_heads, self.head_dim, mp_num=mp_num
-    ).contiguous()
-    value = self._split_heads(
-        value, self.num_attention_heads, self.head_dim, mp_num=mp_num
-    ).contiguous()
-
-    key = self._IPEXROPE(
-        key,
-        position_ids.contiguous(),
-        self.num_attention_heads,
-        self.head_dim,
-        1,  # neighbor elements
-        64,
-    )
-    query = self._IPEXROPE(
-        query,
-        position_ids.contiguous(),
-        self.num_attention_heads,
-        self.head_dim,
-        1,
-        64,
-    )
-
-    if use_cache:
-        (
-            attn_output,
-            attn_weights,
-            present,
-        ) = self._IPEXScaleDotProduct(
-            query,
-            key,
-            value,
-            self.scale_attn,
-            layer_past,
-            head_mask,
-            attention_mask,
-        )
-    else:
-        key = key.permute(0, 2, 1, 3)
-        query = query.permute(0, 2, 1, 3)
-        value = value.permute(0, 2, 1, 3)
-        present = None
-
-        # compute self-attention: V x Softmax(QK^T)
-        attn_output, attn_weights = self._attn(
-            query, key, value, attention_mask, head_mask
-        )
-
-    attn_output = self._merge_heads(
-        attn_output, self.num_attention_heads, self.head_dim
-    )
-    attn_output = self.out_proj(attn_output)
-    attn_output = self.resid_dropout(attn_output)
-
-    outputs = (attn_output, present)
-    if output_attentions:
-        outputs += (attn_weights,)
-
-    return outputs  # a, present, (attentions)
-
-
 class _IPEXAttentionRef(nn.Module):
     def __init__(self, module, config, sdp_module_ref, distributed=False):
         super().__init__()
@@ -663,11 +581,9 @@ class _IPEXAttentionRef(nn.Module):
                 self.pos_embd_dim = module.rotary_ndims
             else:
                 self.pos_embd_dim = self.head_dim
-            self.rope_base = 10000
-            if hasattr(config, "rotary_emb_base"):
-                self.rope_base = config.rotary_emb_base
-            elif hasattr(config, "rope_theta"):
-                self.rope_base = config.rope_theta
+            self.rope_base = (
+                config.rotary_emb_base if hasattr(config, "rotary_emb_base") else 10000
+            )
             self._IPEXROPE = _IPEXRopeRef(
                 self.max_position_embeddings,
                 self.pos_embd_dim,
@@ -675,32 +591,20 @@ class _IPEXAttentionRef(nn.Module):
                 self.model_backbone,
             )
 
-        if re.search("GPTJ", self.model_backbone, re.IGNORECASE) or re.search(
-            "LLAMA", self.model_backbone, re.IGNORECASE
-        ):
-            if (
-                hasattr(module, "q_proj")
-                and hasattr(module, "k_proj")
-                and hasattr(module, "v_proj")
-            ):
-
+        if re.search("GPTJ", self.model_backbone, re.IGNORECASE) or \
+                re.search("LLAMA", self.model_backbone, re.IGNORECASE):
+            if hasattr(module, 'q_proj') and hasattr(module, 'k_proj') and hasattr(module, 'v_proj'):
                 def get_weight_shape(mod):
-                    if hasattr(mod, "in_features") and hasattr(mod, "out_features"):
+                    if hasattr(mod, 'in_features') and hasattr(mod, 'out_features'):
                         return [mod.in_features, mod.out_features]
-                    elif hasattr(mod, "weight") and hasattr(mod.weight, "shape"):
+                    elif hasattr(mod, 'weight') and hasattr(mod.weight, 'shape'):
                         return list(mod.weight.shape)
                     return None
-
-                weight_shapes = [
-                    get_weight_shape(mod)
-                    for mod in [module.q_proj, module.k_proj, module.v_proj]
-                ]
-                if weight_shapes[0] is not None and all(
-                    weight_shapes[0] == shape for shape in weight_shapes[1:]
-                ):
-                    self.concat_qkv = _IPEXConcatLinearRef(
-                        [module.q_proj, module.k_proj, module.v_proj]
-                    )
+                weight_shapes = [get_weight_shape(mod)
+                                 for mod in [module.q_proj, module.k_proj, module.v_proj]]
+                if weight_shapes[0] is not None and \
+                        all(weight_shapes[0] == shape for shape in weight_shapes[1:]):
+                    self.concat_qkv = _IPEXConcatLinearRef([module.q_proj, module.k_proj, module.v_proj])
                     del module.q_proj, module.k_proj, module.v_proj
 
         self._IPEXScaleDotProduct = _IPEXScaleDotProductRef(module, config)
@@ -820,17 +724,6 @@ class _IPEXAttentionRef(nn.Module):
                 use_cache,
                 output_attentions,
             )
-        elif re.search("codegen", self.model_backbone, re.IGNORECASE):
-            return _CodeGenAttention_forward(
-                self,
-                hidden_states,
-                layer_past,
-                attention_mask,
-                position_ids,
-                head_mask,
-                use_cache,
-                output_attentions,
-            )
         else:
             AssertionError(False, "Do not support the optimization of your model yet")
 
diff --git a/intel_extension_for_pytorch/transformers/models/reference/modules/decoder.py b/intel_extension_for_pytorch/transformers/models/reference/modules/decoder.py
index a7f4951a0..486321e28 100644
--- a/intel_extension_for_pytorch/transformers/models/reference/modules/decoder.py
+++ b/intel_extension_for_pytorch/transformers/models/reference/modules/decoder.py
@@ -259,9 +259,7 @@ class _IPEXDecoderLayerRef(nn.Module):
         self.distributed = distributed
         self.model_backbone = config.architectures[0]
 
-        if re.search("GPTJ", self.model_backbone, re.IGNORECASE) or re.search(
-            "CodeGen", self.model_backbone, re.IGNORECASE
-        ):
+        if re.search("GPTJ", self.model_backbone, re.IGNORECASE):
             if not self.distributed:
                 self.linear_add_add = _IPEXlinearAddAddRef(module.mlp.fc_out)
                 del self.__dict__["_modules"]["mlp"].fc_out
@@ -318,9 +316,7 @@ class _IPEXDecoderLayerRef(nn.Module):
         past_key_value: Optional[Tuple[torch.Tensor]] = None,
         alibi: Optional[torch.Tensor] = None,
     ):
-        if re.search("GPTJ", self.model_backbone, re.IGNORECASE) or re.search(
-            "CodeGen", self.model_backbone, re.IGNORECASE
-        ):
+        if re.search("GPTJ", self.model_backbone, re.IGNORECASE):
             return GPTJBlock_forward(
                 self,
                 hidden_states,
diff --git a/intel_extension_for_pytorch/transformers/optimize.py b/intel_extension_for_pytorch/transformers/optimize.py
index 17650fc27..ee658424e 100644
--- a/intel_extension_for_pytorch/transformers/optimize.py
+++ b/intel_extension_for_pytorch/transformers/optimize.py
@@ -11,7 +11,7 @@ from intel_extension_for_pytorch.cpu._auto_kernel_selection import (
 import intel_extension_for_pytorch as ipex
 from ..utils.weight_only_quantization import (
     _is_woq_qconfig,
-    _convert_woq_with_low_precision_checkpoint,
+    _convert_woq_with_low_precision_checkpoint
 )
 
 
@@ -108,7 +108,6 @@ def model_convert_reference(_model):
         LlamaForCausalLM_forward,
         GPTNeoXForCausalLM_forward,
         OPTForCausalLM_forward,
-        CodeGenForCausalLM_forward,
         prepare_inputs_for_generation,
     )
 
@@ -173,16 +172,6 @@ def model_convert_reference(_model):
             "forward",
             OPTForCausalLM_forward,
         )
-    elif (
-        hasattr(_model, "__class__")
-        and _model.__class__
-        == transformers.models.codegen.modeling_codegen.CodeGenForCausalLM
-    ):
-        convert_function(
-            _model,
-            "forward",
-            CodeGenForCausalLM_forward,
-        )
 
     # checking if model has been wrapped by deepspeed (distributed or not)
     try:
@@ -200,7 +189,6 @@ def model_convert_reference(_model):
         transformers.models.llama.modeling_llama.LlamaAttention,
         transformers.models.gptj.modeling_gptj.GPTJAttention,
         transformers.models.opt.modeling_opt.OPTAttention,
-        transformers.models.codegen.modeling_codegen.CodeGenAttention,
     ]:
         convert_class(
             _model,
@@ -213,7 +201,6 @@ def model_convert_reference(_model):
     for supported_decoder_class in [
         transformers.models.llama.modeling_llama.LlamaDecoderLayer,
         transformers.models.gptj.modeling_gptj.GPTJBlock,
-        transformers.models.codegen.modeling_codegen.CodeGenBlock,
         transformers.models.opt.modeling_opt.OPTDecoderLayer,
     ]:
         convert_class(
@@ -349,25 +336,24 @@ def ipex_quantization_flow(
 ):
     from intel_extension_for_pytorch.quantization import prepare, convert
 
-    is_woq = _is_woq_qconfig(qconfig)
-    if not is_woq and sample_inputs is None:
+    if (
+        not _is_woq_qconfig(qconfig) and sample_inputs is None
+    ):
         sample_inputs = get_dummy_input(_model)
 
     prepared_model = prepare(
         _model.eval(), qconfig, example_inputs=sample_inputs, inplace=True
     )
-
     if static_qconfig_file is not None:
         prepared_model.load_qconf_summary(qconf_summary=static_qconfig_file)
         print("ipex.optimize_transformers is doing the static quantization")
     else:
         print("ipex.optimize_transformers is doing the weight only quantization")
-
     with torch.no_grad(), torch.cpu.amp.autocast(
         enabled=True if dtype is torch.bfloat16 else False
     ):
         convert_model = convert(prepared_model.eval(), inplace=True).eval()
-        if is_woq and dtype is torch.bfloat16:
+        if _is_woq_qconfig(qconfig) and dtype is torch.bfloat16:
             convert_model = convert_model.to(dtype)
     return convert_model
 
@@ -468,7 +454,7 @@ def optimize_transformers(
     r"""
     Apply optimizations at Python frontend to the given transformers model (nn.Module).
     This API focus on transformers models, especially for generation tasks inference.
-    Well supported model family: Llama, GPT-J, GPT-Neox, OPT, Falcon, CodeGen.
+    Well supported model family: Llama, GPT-J, GPT-Neox, OPT, Falcon.
 
     Args:
         model (torch.nn.Module): User model to apply optimizations.
@@ -501,7 +487,7 @@ def optimize_transformers(
             It means there is no need to further apply optimization like torchscirpt. Default value is ``True``.
 
     Returns:
-        Optimized model object for model.generate(), also workable with model.forward
+        optimized model object for model.generate(), also workable with model.forward
 
     .. warning::
         Please invoke ``optimize_transformers`` function AFTER invoking DeepSpeed in Tensor Parallel
@@ -563,11 +549,10 @@ def optimize_transformers(
             or re.search("OPT", model.config.architectures[0], re.IGNORECASE)
             or re.search("falcon", model.config.architectures[0], re.IGNORECASE)
             or re.search("rw", model.config.architectures[0], re.IGNORECASE)
-            or re.search("codegen", model.config.architectures[0], re.IGNORECASE)
         )
         if not well_supported_model:
             warnings.warn(
-                "optimize_transformers supports Llama, GPT-J, GPT-Neox, Falcon, OPT, and CodeGen, fallback to origin model"
+                "optimize_transformers supports Llama, GPT-J, GPT-Neox, Falcon, and OPT, fallback to origin model"
             )
             return model
 
@@ -584,19 +569,17 @@ def optimize_transformers(
                 is_woq = True
 
         # Load low precision checkpoint (generated by GPTQ, etc.) for WOQ before any conversion
-        if device == "cpu" and is_woq and low_precision_checkpoint is not None:
+        if device == 'cpu' and is_woq and low_precision_checkpoint is not None:
             state_dict, config = None, None
             if isinstance(low_precision_checkpoint, tuple):
-                assert (
-                    len(low_precision_checkpoint) == 2
-                    and isinstance(low_precision_checkpoint[0], dict)
-                    and isinstance(low_precision_checkpoint[1], dict)
-                ), "Invalid low_precision_checkpoint"
+                assert len(low_precision_checkpoint) == 2 and \
+                    isinstance(low_precision_checkpoint[0], dict) and \
+                    isinstance(low_precision_checkpoint[1], dict), \
+                    'Invalid low_precision_checkpoint'
                 state_dict, config = low_precision_checkpoint
             else:
-                assert isinstance(
-                    low_precision_checkpoint, dict
-                ), "Invalid low_precision_checkpoint argument"
+                assert isinstance(low_precision_checkpoint, dict), \
+                    'Invalid low_precision_checkpoint argument'
                 state_dict = low_precision_checkpoint
             lowp_checkpoint = state_dict if config is None else (state_dict, config)
             _model = _convert_woq_with_low_precision_checkpoint(
diff --git a/intel_extension_for_pytorch/utils/weight_only_quantization.py b/intel_extension_for_pytorch/utils/weight_only_quantization.py
index 818b165d9..296d943dc 100644
--- a/intel_extension_for_pytorch/utils/weight_only_quantization.py
+++ b/intel_extension_for_pytorch/utils/weight_only_quantization.py
@@ -6,25 +6,21 @@ from torch.ao.quantization import PlaceholderObserver, QConfigMapping
 # The config describes how to load low precision checkpoint for weight only quantization.
 # Weight shape is N by K if transposed is False otherwise K by N.
 # Bias is optional. If bias is not provided in the checkpoint, we read the original model.
-DEFAULT_LOWP_CHECKPOINT_CONFIG = {
-    "name": "default",
-    "weight_key": "packed_weight",
-    "scale_key": "scale",
-    "zero_point_key": "packed_zp",
-    "bias_key": "bias",
-}
+DEFAULT_LOWP_CHECKPOINT_CONFIG = \
+    {
+        "name": "default",
+        "weight_key": "packed_weight",
+        "scale_key": "scale",
+        "zero_point_key": "packed_zp",
+        "bias_key": "bias",
+    }
 
 
 def _is_woq_qconfig(qconfig_mapping):
-    qconfig = (
-        qconfig_mapping.global_qconfig
-        if isinstance(qconfig_mapping, QConfigMapping)
-        else qconfig_mapping
-    )
-    return (
-        isinstance(qconfig.activation(), PlaceholderObserver)
-        and not qconfig.activation().is_dynamic
-    )
+    qconfig = qconfig_mapping.global_qconfig \
+        if isinstance(qconfig_mapping, QConfigMapping) else qconfig_mapping
+    return isinstance(qconfig.activation(), PlaceholderObserver) and \
+        not qconfig.activation().is_dynamic
 
 
 def _default_lowp_checkpoint_config():
@@ -32,43 +28,34 @@ def _default_lowp_checkpoint_config():
 
 
 def _get_keys_from_config(checkpoint_config):
-    weight_key = checkpoint_config.get("weight_key", "weight")
-    scales_key = checkpoint_config.get("scale_key", "scale")
-    zeros_key = checkpoint_config.get("zero_point_key", "zero")
-    bias_key = checkpoint_config.get("bias_key", "bias")
+    weight_key = checkpoint_config.get('weight_key', 'weight')
+    scales_key = checkpoint_config.get('scale_key', 'scale')
+    zeros_key = checkpoint_config.get('zero_point_key', 'zero')
+    bias_key = checkpoint_config.get('bias_key', 'bias')
     return weight_key, scales_key, zeros_key, bias_key
 
 
 def _get_linear_parameters(attr_name, state_dict, checkpoint_config):
-    weight_key, scales_key, zeros_key, bias_key = _get_keys_from_config(
-        checkpoint_config
-    )
-    w_key = attr_name + "." + weight_key
-    s_key = attr_name + "." + scales_key
-    z_key = attr_name + "." + zeros_key
-    b_key = attr_name + "." + bias_key
+    weight_key, scales_key, zeros_key, bias_key = _get_keys_from_config(checkpoint_config)
+    w_key = attr_name + '.' + weight_key
+    s_key = attr_name + '.' + scales_key
+    z_key = attr_name + '.' + zeros_key
+    b_key = attr_name + '.' + bias_key
     # all are tensors
     qweight = state_dict.get(w_key, None)
     scales = state_dict.get(s_key, None)
     qzeros = state_dict.get(z_key, None)
     bias = state_dict.get(b_key, None)
-    group_size = -1
-    if qweight is not None and scales is not None:
-        assert scales.dim() == 2, "Unexpected scales tensor dimension"
-        if scales.size(-1) != 1:
-            # qweight is compressed along the last dim int4 * 8 -> int32
-            group_size = qweight.size(-1) * 8 // scales.size(-1)
-    return qweight, scales, qzeros, bias, group_size
+    return qweight, scales, qzeros, bias
 
 
 def _convert_woq_with_low_precision_checkpoint(
-    model,
-    qconfig_mapping,
-    low_precision_checkpoint,
-    checkpoint_config=None,
-    inplace=True,
-):
-    r"""
+        model,
+        qconfig_mapping,
+        low_precision_checkpoint,
+        checkpoint_config=None,
+        inplace=True):
+    r'''
     Method to convert fp32 model to WOQ model with checkpoint generated by GPTQ
     Args:
         model: original model
@@ -85,14 +72,12 @@ def _convert_woq_with_low_precision_checkpoint(
     Default format:
     - Weights and zero points in UINT4 and compressed as INT32, scales in FP16.
     - Keys are 'packed_weight', 'scale', 'packed_zp'
-    """
-
-    assert isinstance(
-        low_precision_checkpoint, dict
-    ), "low_precision_checkpoint should be a state_dict"
-    assert checkpoint_config is None or isinstance(
-        checkpoint_config, dict
-    ), "checkpoint_config should be a dict"
+    '''
+
+    assert isinstance(low_precision_checkpoint, dict), \
+        'low_precision_checkpoint should be a state_dict'
+    assert checkpoint_config is None or isinstance(checkpoint_config, dict), \
+        'checkpoint_config should be a dict'
     if checkpoint_config is None:
         checkpoint_config = _default_lowp_checkpoint_config()
 
@@ -101,27 +86,25 @@ def _convert_woq_with_low_precision_checkpoint(
     weight_key, scales_key, zeros_key, _ = _get_keys_from_config(checkpoint_config)
     keys_found = [False] * 3
     for k, _ in state_dict.items():
-        if k.endswith("." + weight_key):
+        if k.endswith('.' + weight_key):
             keys_found[0] = True
-        if k.endswith("." + scales_key):
+        if k.endswith('.' + scales_key):
             keys_found[1] = True
-        if k.endswith("." + zeros_key):
+        if k.endswith('.' + zeros_key):
             keys_found[2] = True
         if all(keys_found):
             break
-    assert all(keys_found), "Error: Format of checkpoint and config do not match"
+    assert all(keys_found), 'Error: Format of checkpoint and config do not match'
 
     def _convert(mod, attr_name):
         if isinstance(mod, torch.nn.Linear):
             mod.qconfig = qconfig_mapping.global_qconfig
-            qweight, scales, qzeros, bias, group_size = _get_linear_parameters(
+            qweight, scales, qzeros, bias = _get_linear_parameters(
                 attr_name, state_dict, checkpoint_config
             )
             if any(i is None for i in [qweight, scales, qzeros]):
                 return mod
-            mod_new = IpexWoqLinear.from_float_and_int4_weight(
-                mod, qweight, scales, qzeros, bias, group_size=group_size
-            )
+            mod_new = IpexWoqLinear.from_float_and_int4_weight(mod, qweight, scales, qzeros, bias)
             return mod_new
 
         mod_new = mod
diff --git a/scripts/compile_bundle.sh b/scripts/compile_bundle.sh
index 5fddd7a1c..87237e979 100644
--- a/scripts/compile_bundle.sh
+++ b/scripts/compile_bundle.sh
@@ -1,192 +1,83 @@
 #!/bin/bash
-set -eo pipefail
-
-VER_IPEX=v2.1.100+cpu
+set -x
+set -e
 
-# Mode: Select which components to install. PyTorch and Intel® Extension for PyTorch* are always installed.
-# High bit: 8 7 6 5 4 3 2 1 :Low bit
-#           | | | | | | | └- TorchAudio
-#           | | | | | | └--- TorchVision
-#           | | | | | └----- Rebuild LLVM
-#           | | | | └------- Undefined
-#           | | | └--------- Undefined
-#           | | └----------- Undefined
-#           | └------------- Undefined
-#           └--------------- Undefined
-MODE=0x03
-if [ $# -gt 0 ]; then
-    if [[ ! $1 =~ ^[0-9]+$ ]] && [[ ! $1 =~ ^0x[0-9a-fA-F]+$ ]]; then
-        echo "Warning: Unexpected argument. Using default value."
-    else
-        MODE=$1
-    fi
-fi
+VER_LLVM="llvmorg-16.0.6"
+VER_IPEX="v2.1.0+cpu"
 
 # Check existance of required Linux commands
-for CMD in conda git nproc; do
-    command -v ${CMD} > /dev/null || (echo "Error: Command \"${CMD}\" not found." ; exit 1)
+for CMD in conda git nproc make; do
+    command -v ${CMD} || (echo "Error: Command \"${CMD}\" not found." ; exit 4)
 done
 
+MAX_JOBS_VAR=$(nproc)
+if [ ! -z "${MAX_JOBS}" ]; then
+    MAX_JOBS_VAR=${MAX_JOBS}
+fi
+
 # Save current directory path
 BASEFOLDER=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
 cd ${BASEFOLDER}
-
 # Checkout individual components
+if [ ! -d llvm-project ]; then
+    git clone https://github.com/llvm/llvm-project.git
+fi
 if [ ! -d intel-extension-for-pytorch ]; then
     git clone https://github.com/intel/intel-extension-for-pytorch.git
 fi
-cd intel-extension-for-pytorch
-if [ ! -z "${VER_IPEX}" ]; then
-    rm -rf * > /dev/null
-    git checkout . > /dev/null
-    git checkout main > /dev/null
-    git pull > /dev/null
-    git checkout ${VER_IPEX}
-fi
-git submodule sync
-git submodule update --init --recursive
-
-python -m pip install pyyaml
-VER_TORCH=$(python tools/yaml_utils.py -f dependency_version.yml -d pytorch -k version)
-VER_TORCHVISION=$(python tools/yaml_utils.py -f dependency_version.yml -d torchvision -k version)
-VER_TORCHAUDIO=$(python tools/yaml_utils.py -f dependency_version.yml -d torchaudio -k version)
-VER_LLVM=llvmorg-$(python tools/yaml_utils.py -f dependency_version.yml -d llvm -k version)
-VER_GCC=$(python tools/yaml_utils.py -f dependency_version.yml -d gcc -k min-version)
-python -m pip uninstall -y pyyaml
-cd ..
 
-if [ ! -d llvm-project ]; then
-    git clone https://github.com/llvm/llvm-project.git
-fi
+# Checkout required branch/commit and update submodules
 cd llvm-project
-if [ ! -z "${VER_LLVM}" ]; then
-    rm -rf * > /dev/null
-    git checkout . > /dev/null
-    git checkout main > /dev/null
-    git pull > /dev/null
+if [ ! -z ${VER_LLVM} ]; then
     git checkout ${VER_LLVM}
 fi
 git submodule sync
 git submodule update --init --recursive
 cd ..
-
-function ver_compare() {
-    VER_MAJOR_CUR=$(echo $1 | cut -d "." -f 1)
-    VER_MINOR_CUR=$(echo $1 | cut -d "." -f 2)
-    VER_PATCH_CUR=$(echo $1 | cut -d "." -f 3)
-    VER_MAJOR_REQ=$(echo $2 | cut -d "." -f 1)
-    VER_MINOR_REQ=$(echo $2 | cut -d "." -f 2)
-    VER_PATCH_REQ=$(echo $2 | cut -d "." -f 3)
-    RET=0
-    if [[ ${VER_MAJOR_CUR} -lt ${VER_MAJOR_REQ} ]]; then
-        RET=1
-    else
-        if [[ ${VER_MAJOR_CUR} -eq ${VER_MAJOR_REQ} ]] &&
-           [[ ${VER_MINOR_CUR} -lt ${VER_MINOR_REQ} ]]; then
-            RET=2
-        else
-            if [[ ${VER_MAJOR_CUR} -eq ${VER_MAJOR_REQ} ]] &&
-               [[ ${VER_MINOR_CUR} -eq ${VER_MINOR_REQ} ]] &&
-               [[ ${VER_PATCH_CUR} -lt ${VER_PATCH_REQ} ]]; then
-                RET=3
-            fi
-        fi
-    fi
-    echo ${RET}
-}
-GCC_CONDA=0
-set +e
-command -v gcc > /dev/null
-EXIST_CC=$?
-command -v g++ > /dev/null
-EXIST_CXX=$?
-set -e
-if [ ${EXIST_CC} -gt 0 ] || [ ${EXIST_CXX} -gt 0 ]; then
-    echo -e '\a'
-    echo "Warning: GCC not found."
-    echo "         Installing gcc and g++ 12.3 from conda..."
-    echo ""
-    GCC_CONDA=1
-else
-    VER_COMP=$(ver_compare $(gcc -dumpfullversion) ${VER_GCC})
-    if [ ${VER_COMP} -ne 0 ]; then
-        echo -e '\a'
-        echo "Warning: GCC version equal to or newer than ${VER_GCC} is required."
-        echo "         Found GCC version $(gcc -dumpfullversion)."
-        echo "         Installing gcc and g++ 12.3 from conda..."
-        echo ""
-        GCC_CONDA=1
-    fi
-fi
-
-MAX_JOBS_VAR=$(nproc)
-if [ ! -z "${MAX_JOBS}" ]; then
-    MAX_JOBS_VAR=${MAX_JOBS}
+cd intel-extension-for-pytorch
+if [ ! -z ${VER_IPEX} ]; then
+    git checkout ${VER_IPEX}
 fi
+git submodule sync
+git submodule update --init --recursive
+cd ..
 
 # Install dependencies
+conda install -y gcc==12.3 gxx==12.3 cxx-compiler -c conda-forge
+conda update -y sysroot_linux-64
 python -m pip install cmake
-python -m pip uninstall -y torch torchvision torchaudio intel-extension-for-pytorch
-set +e
-echo ${VER_TORCH} | grep "dev" > /dev/null
-TORCH_DEV=$?
-set -e
-URL_NIGHTLY=""
-if [ ${TORCH_DEV} -eq 0 ]; then
-    URL_NIGHTLY="nightly/"
-fi
-python -m pip install torch==${VER_TORCH} --index-url https://download.pytorch.org/whl/${URL_NIGHTLY}cpu
-if [ $((${MODE} & 0x02)) -ne 0 ]; then
-    python -m pip install torchvision==${VER_TORCHVISION} --index-url https://download.pytorch.org/whl/${URL_NIGHTLY}cpu
-fi
-if [ $((${MODE} & 0x01)) -ne 0 ]; then
-    python -m pip install torchaudio==${VER_TORCHAUDIO} --index-url https://download.pytorch.org/whl/${URL_NIGHTLY}cpu
-fi
+python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
 ABI=$(python -c "import torch; print(int(torch._C._GLIBCXX_USE_CXX11_ABI))")
 
 # Compile individual component
-if [ ${GCC_CONDA} -eq 1 ]; then
-    conda install -y sysroot_linux-64
-    conda install -y gcc==12.3 gxx==12.3 cxx-compiler -c conda-forge
-    export CC=${CONDA_PREFIX}/bin/gcc
-    export CXX=${CONDA_PREFIX}/bin/g++
-    export LD_LIBRARY_PATH=${CONDA_PREFIX}/lib:${LD_LIBRARY_PATH}
-fi
-set +e
-command -v make > /dev/null
-if [ $? -gt 0 ]; then
-    conda install -y make -c conda-forge
-fi
-set -e
+export CC=${CONDA_PREFIX}/bin/gcc
+export CXX=${CONDA_PREFIX}/bin/g++
+export LD_LIBRARY_PATH=${CONDA_PREFIX}/lib:${LD_LIBRARY_PATH}
 
 #  LLVM
-LLVM_ROOT="$(pwd)/llvm-release"
-if [ $((${MODE} & 0x04)) -ne 0 ]; then
-    if [ -d ${LLVM_ROOT} ]; then
-        rm -rf ${LLVM_ROOT}
-    fi
-fi
 cd llvm-project
-if [ -d build ]; then
-    rm -rf build
+LLVM_ROOT="$(pwd)/release"
+if [ -d ${LLVM_ROOT} ]; then
+    rm -rf ${LLVM_ROOT}
 fi
-if [ ! -d ${LLVM_ROOT} ]; then
-    mkdir build
-    cd build
-    echo "***************************** cmake *****************************" > ../build.log
-    cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS="-D_GLIBCXX_USE_CXX11_ABI=${ABI}" -DLLVM_TARGETS_TO_BUILD=X86 -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_INCLUDE_TESTS=OFF -DLLVM_INCLUDE_EXAMPLES=OFF -DLLVM_INCLUDE_BENCHMARKS=OFF ../llvm 2>&1 | tee -a ../build.log
-    echo "***************************** build *****************************" >> ../build.log
-    cmake --build . -j ${MAX_JOBS_VAR} 2>&1 | tee -a ../build.log
-    echo "**************************** install ****************************" >> ../build.log
-    cmake -DCMAKE_INSTALL_PREFIX=${LLVM_ROOT} -P cmake_install.cmake 2>&1 | tee -a ../build.log
-    #xargs rm -rf < install_manifest.txt
-    cd ..
+if [ -d build ]; then
     rm -rf build
-    ln -s ${LLVM_ROOT}/bin/llvm-config ${LLVM_ROOT}/bin/llvm-config-13
 fi
+mkdir build
+cd build
+echo "***************************** cmake *****************************" > ../build.log
+cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS="-D_GLIBCXX_USE_CXX11_ABI=${ABI}" -DLLVM_TARGETS_TO_BUILD=X86 -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_INCLUDE_TESTS=OFF -DLLVM_INCLUDE_EXAMPLES=OFF -DLLVM_INCLUDE_BENCHMARKS=OFF ../llvm 2>&1 | tee -a ../build.log
+echo "***************************** build *****************************" >> ../build.log
+cmake --build . -j ${MAX_JOBS_VAR} 2>&1 | tee -a ../build.log
+echo "**************************** install ****************************" >> ../build.log
+cmake -DCMAKE_INSTALL_PREFIX=${LLVM_ROOT} -P cmake_install.cmake 2>&1 | tee -a ../build.log
+#xargs rm -rf < install_manifest.txt
 cd ..
+rm -rf build
+ln -s ${LLVM_ROOT}/bin/llvm-config ${LLVM_ROOT}/bin/llvm-config-13
 export PATH=${LLVM_ROOT}/bin:$PATH
 export LD_LIBRARY_PATH=${LLVM_ROOT}/lib:$LD_LIBRARY_PATH
+cd ..
 #  Intel® Extension for PyTorch*
 cd intel-extension-for-pytorch
 python -m pip install -r requirements.txt
@@ -199,34 +90,11 @@ python setup.py bdist_wheel 2>&1 | tee build.log
 export CXXFLAGS=${CXXFLAGS_BK}
 unset DNNL_GRAPH_BUILD_COMPILER_BACKEND
 unset LLVM_DIR
-python -m pip uninstall -y mkl-static mkl-include
-python -m pip install dist/*.whl
+python -m pip install --force-reinstall dist/*.whl
 cd ..
 
 # Sanity Test
-if [ ! -z ${CONDA_PREFIX} ]; then
-    LIBSTDCPP_SYS=$(find /usr -regextype sed -regex ".*libstdc++\.so\.[[:digit:]]*\.[[:digit:]]*\.[[:digit:]]*")
-    LIBSTDCPP_CONDA=$(find ${CONDA_PREFIX}/lib -regextype sed -regex ".*libstdc++\.so\.[[:digit:]]*\.[[:digit:]]*\.[[:digit:]]*")
-    LIBSTDCPP_VER_SYS=$(echo ${LIBSTDCPP_SYS} | sed "s/.*libstdc++.so.//")
-    LIBSTDCPP_VER_CONDA=$(echo ${LIBSTDCPP_CONDA} | sed "s/.*libstdc++.so.//")
-    VER_COMP=$(ver_compare ${LIBSTDCPP_VER_CONDA} ${LIBSTDCPP_VER_SYS})
-    LIBSTDCPP_ACTIVE=""
-    if [[ ${VER_COMP} -gt 0 ]]; then
-        LIBSTDCPP_ACTIVE=${LIBSTDCPP_SYS}
-    else
-        LIBSTDCPP_ACTIVE=${LIBSTDCPP_CONDA}
-    fi
-    export LD_PRELOAD=${LIBSTDCPP_ACTIVE}
-    echo "======================================================"
-    echo "Note: Set environment variable \"export LD_PRELOAD=${LIBSTDCPP_ACTIVE}\" to avoid the \"version \`GLIBCXX_N.N.NN' not found\" error."
-    echo "======================================================"
-fi
-CMD="import torch; print(f'torch_cxx11_abi:     {torch._C._GLIBCXX_USE_CXX11_ABI}'); print(f'torch_version:       {torch.__version__}');"
-if [ $((${MODE} & 0x02)) -ne 0 ]; then
-    CMD="${CMD} import torchvision; print(f'torchvision_version: {torchvision.__version__}');"
-fi
-if [ $((${MODE} & 0x01)) -ne 0 ]; then
-    CMD="${CMD} import torchaudio; print(f'torchaudio_version:  {torchaudio.__version__}');"
-fi
-CMD="${CMD} import intel_extension_for_pytorch as ipex; print(f'ipex_version:        {ipex.__version__}');"
-python -c "${CMD}"
+set +x
+export LD_PRELOAD=${CONDA_PREFIX}/lib/libstdc++.so
+echo "Note: Should you experience \"version \`GLIBCXX_N.N.NN' not found\" error, run command \"export LD_PRELOAD=${CONDA_PREFIX}/lib/libstdc++.so\" and try again."
+python -c "import torch; import torchvision; import torchaudio; import intel_extension_for_pytorch as ipex; print(f'torch_cxx11_abi:     {torch._C._GLIBCXX_USE_CXX11_ABI}'); print(f'torch_version:       {torch.__version__}'); print(f'torchvision_version: {torchvision.__version__}'); print(f'torchaudio_version:  {torchaudio.__version__}'); print(f'ipex_version:        {ipex.__version__}');"
diff --git a/setup.py b/setup.py
index aecad1149..ba66d8865 100644
--- a/setup.py
+++ b/setup.py
@@ -205,7 +205,6 @@ def _build_installation_dependency():
     install_requires = []
     install_requires.append("psutil")
     install_requires.append("numpy")
-    install_requires.append("packaging")
     return install_requires
 
 
@@ -566,48 +565,31 @@ def get_pybind11_abi_compiler_flags():
 
 
 def _gen_build_cfg_from_cmake(
-    cmake_exec, project_root_dir, cmake_args, build_dir, build_env, use_ninja=True
-):
+    cmake_exec, project_root_dir, cmake_args, build_dir, build_env, use_ninja = True):
     if IS_WINDOWS:
         if use_ninja:
-            check_call(
-                [cmake_exec, project_root_dir, "-GNinja"] + cmake_args,
-                cwd=build_dir,
-                env=build_env,
-            )
+            check_call([cmake_exec, project_root_dir, '-GNinja'] + cmake_args, cwd=build_dir, env=build_env)
         else:
             # using MSVC generator
-            check_call(
-                [
-                    cmake_exec,
-                    project_root_dir,
-                    "-G Visual Studio 17 2022",
-                    "-T Intel C++ Compiler 2023",
-                ]
-                + cmake_args,
-                cwd=build_dir,
-                env=build_env,
-            )
+            check_call([cmake_exec, project_root_dir, '-G Visual Studio 17 2022', '-T Intel C++ Compiler 2023'] + cmake_args, cwd=build_dir, env=build_env)
     else:
         # Linux
-        check_call(
-            [cmake_exec, project_root_dir] + cmake_args, cwd=build_dir, env=build_env
-        )
+        check_call([cmake_exec, project_root_dir] + cmake_args, cwd=build_dir, env=build_env)
 
 
-def _build_project(build_args, build_dir, build_env, use_ninja=True):
+def _build_project(build_args, build_dir, build_env, use_ninja = True):
     if IS_WINDOWS:
         if use_ninja:
-            check_call(["ninja"] + build_args, cwd=build_dir, env=build_env)
+            check_call(['ninja'] + build_args, cwd=build_dir, env=build_env)
         else:
             # Current Windows MSVC needs manual build
             pass
     else:
         # Linux
         if use_ninja:
-            check_call(["ninja"] + build_args, cwd=build_dir, env=build_env)
+            check_call(['ninja'] + build_args, cwd=build_dir, env=build_env)
         else:
-            check_call(["make"] + build_args, cwd=build_dir, env=build_env)
+            check_call(['make'] + build_args, cwd=build_dir, env=build_env)
 
 
 def define_build_options(args, **kwargs):
@@ -672,7 +654,7 @@ class IPEXCPPLibBuild(build_clib, object):
         use_ninja = False
         # Windows uses Ninja as default generator
         if IS_WINDOWS:
-            use_ninja = True
+            use_ninja = True        
         sequential_build = False
 
         cmake_common_args = []
@@ -690,9 +672,9 @@ class IPEXCPPLibBuild(build_clib, object):
                     use_ninja = True
                     cmake_common_args.append("-GNinja")
                     continue
-                if IS_WINDOWS and var == "USE_MSVC" and val.upper() in ON_ENV_VAL:
+                if IS_WINDOWS and var == 'USE_MSVC' and val.upper() in ON_ENV_VAL:
                     use_ninja = False
-                    continue
+                    continue                
                 if var == "BUILD_STATS" and val.upper() in ON_ENV_VAL:
                     sequential_build = True
                     # fall through
@@ -741,12 +723,7 @@ class IPEXCPPLibBuild(build_clib, object):
             cmake_args_gpu = []
             define_build_options(cmake_args_gpu, **build_option_gpu)
             _gen_build_cfg_from_cmake(
-                cmake_exec,
-                project_root_dir,
-                cmake_args_gpu,
-                ipex_xpu_build_dir,
-                my_env,
-                use_ninja,
+                cmake_exec, project_root_dir, cmake_args_gpu, ipex_xpu_build_dir, my_env, use_ninja
             )
 
         if build_with_cpu:
@@ -756,12 +733,7 @@ class IPEXCPPLibBuild(build_clib, object):
             cmake_args_cpu = []
             define_build_options(cmake_args_cpu, **build_option_cpu)
             _gen_build_cfg_from_cmake(
-                cmake_exec,
-                project_root_dir,
-                cmake_args_cpu,
-                ipex_cpu_build_dir,
-                my_env,
-                use_ninja,
+                cmake_exec, project_root_dir, cmake_args_cpu, ipex_cpu_build_dir, my_env, use_ninja
             )
 
             # Generate cmake for the CPP UT
@@ -780,7 +752,7 @@ class IPEXCPPLibBuild(build_clib, object):
                 cmake_args_cpp_test,
                 get_cpp_test_build_dir(),
                 my_env,
-                use_ninja,
+                use_ninja
             )
 
         if _get_build_target() in ["develop", "python"]:
@@ -799,7 +771,7 @@ class IPEXCPPLibBuild(build_clib, object):
                 cmake_args_python,
                 ipex_python_build_dir,
                 my_env,
-                use_ninja,
+                use_ninja
             )
 
         elif _get_build_target() == "cppsdk":
@@ -820,7 +792,7 @@ class IPEXCPPLibBuild(build_clib, object):
                 cmake_args_cppsdk,
                 ipex_cppsdk_build_dir,
                 my_env,
-                use_ninja,
+                use_ninja
             )
 
         if build_with_xpu:
@@ -964,72 +936,63 @@ def pyi_module():
     library_dirs = ["lib", os.path.join(pytorch_install_dir, "lib")]
 
     if not IS_WINDOWS:
-        library_dirs = ["lib", os.path.join(pytorch_install_dir, "lib")]
+        library_dirs = [
+            "lib",
+            os.path.join(pytorch_install_dir, "lib")
+            ]
 
         extra_compile_args = [
-            "-Wall",
-            "-Wextra",
-            "-Wno-strict-overflow",
-            "-Wno-unused-parameter",
-            "-Wno-missing-field-initializers",
-            "-Wno-write-strings",
-            "-Wno-unknown-pragmas",
+            '-Wall',
+            '-Wextra',
+            '-Wno-strict-overflow',
+            '-Wno-unused-parameter',
+            '-Wno-missing-field-initializers',
+            '-Wno-write-strings',
+            '-Wno-unknown-pragmas',
             # This is required for Python 2 declarations that are deprecated in 3.
-            "-Wno-deprecated-declarations",
+            '-Wno-deprecated-declarations',
             # Python 2.6 requires -fno-strict-aliasing, see
             # http://legacy.python.org/dev/peps/pep-3123/
             # We also depend on it in our code (even Python 3).
-            "-fno-strict-aliasing",
+            '-fno-strict-aliasing',
             # Clang has an unfixed bug leading to spurious missing
             # braces warnings, see
             # https://bugs.llvm.org/show_bug.cgi?id=21629
-            "-Wno-missing-braces",
-        ]
+            '-Wno-missing-braces']
 
         C_ext = CppExtension(
             "{}._C".format(PACKAGE_NAME),
             libraries=main_libraries,
             sources=main_sources,
-            language="c++",
+            language='c++',
             extra_compile_args=extra_compile_args,
             include_dirs=include_dirs,
             library_dirs=library_dirs,
-            extra_link_args=[make_relative_rpath("lib")],
-        )
+            extra_link_args=[make_relative_rpath('lib')])
     else:
-        library_dirs = ["bin", os.path.join(pytorch_install_dir, "lib")]
-        extra_link_args = ["/NODEFAULTLIB:LIBCMT.LIB"]
+        library_dirs = [
+            "bin",
+            os.path.join(pytorch_install_dir, "lib")
+            ]
+        extra_link_args = ['/NODEFAULTLIB:LIBCMT.LIB']
         # /MD links against DLL runtime
         # and matches the flags set for protobuf and ONNX
         # /EHsc is about standard C++ exception handling
         # /DNOMINMAX removes builtin min/max functions
         # /wdXXXX disables warning no. XXXX
-        extra_compile_args = [
-            "/MD",
-            "/EHsc",
-            "/DNOMINMAX",
-            "/wd4267",
-            "/wd4251",
-            "/wd4522",
-            "/wd4522",
-            "/wd4838",
-            "/wd4305",
-            "/wd4244",
-            "/wd4190",
-            "/wd4101",
-            "/wd4996",
-            "/wd4275",
-        ]
+        extra_compile_args = ['/MD', '/EHsc', '/DNOMINMAX',
+                              '/wd4267', '/wd4251', '/wd4522', '/wd4522', '/wd4838',
+                              '/wd4305', '/wd4244', '/wd4190', '/wd4101', '/wd4996',
+                              '/wd4275']
         C_ext = CppExtension(
             "{}._C".format(PACKAGE_NAME),
             libraries=main_libraries,
             sources=main_sources,
-            language="c++",
+            language='c++',
             extra_compile_args=extra_compile_args,
             include_dirs=include_dirs,
             library_dirs=library_dirs,
-            extra_link_args=extra_link_args,
-        )
+            extra_link_args=extra_link_args)
     return C_ext
 
 
@@ -1050,72 +1013,63 @@ def pyi_isa_help_module():
     ]
 
     if not IS_WINDOWS:
-        library_dirs = ["lib", os.path.join(pytorch_install_dir, "lib")]
+        library_dirs = [
+            "lib",
+            os.path.join(pytorch_install_dir, "lib")
+            ]
 
         extra_compile_args = [
-            "-Wall",
-            "-Wextra",
-            "-Wno-strict-overflow",
-            "-Wno-unused-parameter",
-            "-Wno-missing-field-initializers",
-            "-Wno-write-strings",
-            "-Wno-unknown-pragmas",
+            '-Wall',
+            '-Wextra',
+            '-Wno-strict-overflow',
+            '-Wno-unused-parameter',
+            '-Wno-missing-field-initializers',
+            '-Wno-write-strings',
+            '-Wno-unknown-pragmas',
             # This is required for Python 2 declarations that are deprecated in 3.
-            "-Wno-deprecated-declarations",
+            '-Wno-deprecated-declarations',
             # Python 2.6 requires -fno-strict-aliasing, see
             # http://legacy.python.org/dev/peps/pep-3123/
             # We also depend on it in our code (even Python 3).
-            "-fno-strict-aliasing",
+            '-fno-strict-aliasing',
             # Clang has an unfixed bug leading to spurious missing
             # braces warnings, see
             # https://bugs.llvm.org/show_bug.cgi?id=21629
-            "-Wno-missing-braces",
-        ]
+            '-Wno-missing-braces']
 
         C_ext = CppExtension(
             "{}._isa_help".format(PACKAGE_NAME),
             libraries=main_libraries,
             sources=main_sources,
-            language="c++",
+            language='c++',
             extra_compile_args=extra_compile_args,
             include_dirs=include_dirs,
             library_dirs=library_dirs,
-            extra_link_args=[make_relative_rpath("lib")],
-        )
+            extra_link_args=[make_relative_rpath('lib')])
     else:
-        library_dirs = ["bin", os.path.join(pytorch_install_dir, "lib")]
-        extra_link_args = ["/NODEFAULTLIB:LIBCMT.LIB"]
+        library_dirs = [
+            "bin",
+            os.path.join(pytorch_install_dir, "lib")
+            ]
+        extra_link_args = ['/NODEFAULTLIB:LIBCMT.LIB']
         # /MD links against DLL runtime
         # and matches the flags set for protobuf and ONNX
         # /EHsc is about standard C++ exception handling
         # /DNOMINMAX removes builtin min/max functions
         # /wdXXXX disables warning no. XXXX
-        extra_compile_args = [
-            "/MD",
-            "/EHsc",
-            "/DNOMINMAX",
-            "/wd4267",
-            "/wd4251",
-            "/wd4522",
-            "/wd4522",
-            "/wd4838",
-            "/wd4305",
-            "/wd4244",
-            "/wd4190",
-            "/wd4101",
-            "/wd4996",
-            "/wd4275",
-        ]
+        extra_compile_args = ['/MD', '/EHsc', '/DNOMINMAX',
+                              '/wd4267', '/wd4251', '/wd4522', '/wd4522', '/wd4838',
+                              '/wd4305', '/wd4244', '/wd4190', '/wd4101', '/wd4996',
+                              '/wd4275']
         C_ext = CppExtension(
             "{}._isa_help".format(PACKAGE_NAME),
             libraries=main_libraries,
             sources=main_sources,
-            language="c++",
+            language='c++',
             extra_compile_args=extra_compile_args,
             include_dirs=include_dirs,
             library_dirs=library_dirs,
-            extra_link_args=extra_link_args,
-        )
+            extra_link_args=extra_link_args)
     return C_ext
 
 
diff --git a/tests/cpu/test_ao_jit_llga_quantization_fuser.py b/tests/cpu/test_ao_jit_llga_quantization_fuser.py
index 68bc13ead..57d37e0d3 100644
--- a/tests/cpu/test_ao_jit_llga_quantization_fuser.py
+++ b/tests/cpu/test_ao_jit_llga_quantization_fuser.py
@@ -2189,9 +2189,7 @@ class TestFusionPattern(JitLlgaTestCase):
 
     def test_inplace_computation_accuracy(self):
         class LowRankCrossNet(nn.Module):
-            def __init__(
-                self, in_features: int, num_layers: int, low_rank: int
-            ) -> None:
+            def __init__(self, in_features: int, num_layers: int, low_rank: int) -> None:
                 super().__init__()
                 assert low_rank >= 1, "Low rank must be larger or equal to 1"
                 self._num_layers = num_layers
@@ -2202,9 +2200,7 @@ class TestFusionPattern(JitLlgaTestCase):
                     W_kernels.append(Wp)
                 V_kernels: nn.ParameterList = nn.ParameterList()
                 for i in range(self._num_layers):
-                    V_kernels.append(
-                        nn.Parameter(torch.randn(self._low_rank, in_features))
-                    )
+                    V_kernels.append(nn.Parameter(torch.randn(self._low_rank, in_features)))
                 bias: nn.ParameterList = nn.ParameterList(
                     [
                         nn.Parameter(nn.init.zeros_(torch.empty(in_features)))
@@ -2213,24 +2209,24 @@ class TestFusionPattern(JitLlgaTestCase):
                 )
                 self.MLPs = nn.ModuleDict()
                 for i in range(num_layers):
-                    self.MLPs[f"V{i}"] = nn.Linear(in_features, low_rank, bias=False)
-                    self.MLPs[f"W{i}"] = nn.Linear(low_rank, in_features, bias=True)
-                    self.MLPs[f"V{i}"].weight = V_kernels[i]
-                    self.MLPs[f"W{i}"].weight = W_kernels[i]
-                    self.MLPs[f"W{i}"].bias = bias[i]
+                    self.MLPs[f'V{i}'] = nn.Linear(in_features, low_rank, bias=False)
+                    self.MLPs[f'W{i}'] = nn.Linear(low_rank, in_features, bias=True)
+                    self.MLPs[f'V{i}'].weight = V_kernels[i]
+                    self.MLPs[f'W{i}'].weight = W_kernels[i]
+                    self.MLPs[f'W{i}'].bias = bias[i]
 
             def forward(self, input: torch.Tensor) -> torch.Tensor:
                 x_0 = input
-                x_l = x_0  # .clone()
+                x_l = x_0 #.clone()
                 for layer in range(self._num_layers):
-                    x_l_v = self.MLPs[f"V{layer}"](x_l)
-                    x_l_w = self.MLPs[f"W{layer}"](x_l_v)
+                    x_l_v = self.MLPs[f'V{layer}'](x_l)
+                    x_l_w = self.MLPs[f'W{layer}'](x_l_v)
                     x_l = x_0 * x_l_w + x_l  # (B, N)
                 return x_l, x_0
 
         class FakeQuant(nn.Module):
             def __init__(self):
-                super().__init__()
+                super().__init__() 
 
             def forward(self, x):
                 x = torch.quantize_per_tensor(x, 0.1, 0, torch.qint8)
@@ -2247,6 +2243,7 @@ class TestFusionPattern(JitLlgaTestCase):
                 out = self.cross_net(out)
                 return out
 
+
         m = TinyDLRM().eval()
         x = torch.rand(2048, 2)
         graph = self.checkQuantizeTrace(m, [x], atol=2e-1)
@@ -2254,7 +2251,6 @@ class TestFusionPattern(JitLlgaTestCase):
         self.assertGraphContainsExactly(graph, LLGA_FUSION_GROUP, 4)
         self.assertFused(graph, ["aten::linear", "aten::mul", "aten::add"])
 
-
 class TestShapeFallback(JitLlgaTestCase):
     @unittest.skipIf(True, "Size peephole optimization not enabled yet")
     def test_view_permute(self):
diff --git a/tests/cpu/test_code_free_optimization.py b/tests/cpu/test_code_free_optimization.py
index 39adf81d1..0a6a66778 100644
--- a/tests/cpu/test_code_free_optimization.py
+++ b/tests/cpu/test_code_free_optimization.py
@@ -3,7 +3,7 @@ from common_utils import TestCase
 import os
 import subprocess
 import itertools
-import torch
+
 import logging
 
 logging.getLogger().setLevel(logging.DEBUG)
@@ -13,11 +13,7 @@ class TestCodeFreeOptimization(TestCase):
     def test_conv_bn(self):
         loc = os.path.dirname(os.path.abspath(__file__))
         disable_ipex_graph_modes = [False, True]
-        dtypes = (
-            ["float32", "bfloat16"]
-            if torch.ops.mkldnn._is_mkldnn_bf16_supported()
-            else ["float32"]
-        )
+        dtypes = ["float32", "bfloat16"]
         for disable_ipex_graph_mode, dtype in itertools.product(
             disable_ipex_graph_modes, dtypes
         ):
@@ -55,11 +51,7 @@ class TestCodeFreeOptimization(TestCase):
     def test_conv_bn_with_module_created_in_forward(self):
         loc = os.path.dirname(os.path.abspath(__file__))
         disable_ipex_graph_modes = [False, True]
-        dtypes = (
-            ["float32", "bfloat16"]
-            if torch.ops.mkldnn._is_mkldnn_bf16_supported()
-            else ["float32"]
-        )
+        dtypes = ["float32", "bfloat16"]
         for disable_ipex_graph_mode, dtype in itertools.product(
             disable_ipex_graph_modes, dtypes
         ):
@@ -96,11 +88,7 @@ class TestCodeFreeOptimization(TestCase):
     def test_auto_ipex_module(self):
         loc = os.path.dirname(os.path.abspath(__file__))
         disable_ipex_graph_modes = [False, True]
-        dtypes = (
-            ["float32", "bfloat16"]
-            if torch.ops.mkldnn._is_mkldnn_bf16_supported()
-            else ["float32"]
-        )
+        dtypes = ["float32", "bfloat16"]
         for disable_ipex_graph_mode, dtype in itertools.product(
             disable_ipex_graph_modes, dtypes
         ):
diff --git a/tests/cpu/test_deepspeed.py b/tests/cpu/test_deepspeed.py
index f2c3c5f27..8966b47f7 100644
--- a/tests/cpu/test_deepspeed.py
+++ b/tests/cpu/test_deepspeed.py
@@ -26,7 +26,6 @@ from intel_extension_for_pytorch.cpu._auto_kernel_selection import (
 )
 
 from test_weight_prepack import module_found
-
 try:
     import transformers
     from transformers import AutoConfig
@@ -42,8 +41,8 @@ class MyAttention(nn.Module):
     def __init__(self):
         super().__init__()
         # For deepspeed support, please do not change the name of the attribute.
-        self.q_proj = nn.Linear(64, 128)
-        self.out_proj = nn.Linear(128, 128)
+        self.q_proj = nn.Linear(4, 4)
+        self.out_proj = nn.Linear(4, 2)
 
     def forward(self, x):
         x = self.q_proj(x)
@@ -92,7 +91,7 @@ class DeepSpeedTestM(nn.Module):
     def __init__(self, module_type):
         super().__init__()
         self.linear = module_type()
-        self.lm_head = nn.Linear(128, 100)
+        self.lm_head = nn.Linear(2, 2)
 
     def forward(self, x):
         x = self.linear(x)
@@ -197,7 +196,7 @@ class DeepspeedTester(JitTestCase):
                 check_lm_head = True
                 LmHeadLinearAllreduce = deepspeed_modules[2]
 
-            x = torch.randn(2, 3, 64)
+            x = torch.randn(2, 3, 4)
             m_linear = DeepSpeedTestM(MyLmHeadModel).eval()
             y = m_linear(x)
 
@@ -244,7 +243,7 @@ class DeepspeedTester(JitTestCase):
                 check_lm_head = True
                 LmHeadLinearAllreduce = deepspeed_modules[2]
 
-            x = torch.randn(2, 3, 64)
+            x = torch.randn(2, 3, 4)
             m_linear = DeepSpeedTestM(MyLmHeadModel).eval()
             y = m_linear(x)
 
@@ -308,7 +307,7 @@ class DeepspeedTester(JitTestCase):
             [DynamicQuantizedLinearLayer, DynamicQuantizedLinearAllreduce],
             [DynamicQuantizedLmHeadLinearAllreduce],
             ["quantized::linear_dynamic", "deepspeed_comm::all_reduce"],
-            atol=0.02,
+            atol=0.009,
             rtol=1.3e-6,
         )
 
@@ -368,6 +367,7 @@ class DeepspeedTester(JitTestCase):
             inplace=True,
             deployment_mode=True,
         )
+        print(model)
         if not hasattr(model, "trace_graph"):
             AssertionError(False)
         _IPEXAttentionCPU = (
@@ -411,7 +411,6 @@ class DeepspeedTester(JitTestCase):
         with torch.no_grad():
             model(*example_inputs)
 
-
 if __name__ == "__main__":
     deepspeed_modules = may_import_deepspeed_modules()
     if deepspeed_modules is not None:
diff --git a/tests/cpu/test_instance_norm.py b/tests/cpu/test_instance_norm.py
index de9323c73..1c7121ad6 100644
--- a/tests/cpu/test_instance_norm.py
+++ b/tests/cpu/test_instance_norm.py
@@ -1,7 +1,6 @@
 # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
 import torch
-
-# import unittest
+import unittest
 from common_utils import TestCase
 from torch.nn import InstanceNorm2d, InstanceNorm3d, BatchNorm2d, BatchNorm3d
 
@@ -54,7 +53,7 @@ class InstanceNormTester(TestCase):
             y2 = m(x2)
             self.assertTrue(y2.dtype == torch.float32)
             self.assertEqual(y2, y1)
-            self.assertTrue(y2.is_contiguous(memory_format=torch.contiguous_format))
+            self.assertTrue(y2.is_contiguous(memory_format=memory_format))
 
             y2.mean().backward()
             self.assertTrue(x2.grad.dtype == torch.float32)
@@ -110,7 +109,7 @@ class InstanceNormTester(TestCase):
                 )
                 y2 = m(x2)
                 self.assertTrue(y2.dtype == torch.bfloat16)
-                self.assertTrue(y2.is_contiguous(memory_format=torch.contiguous_format))
+                self.assertTrue(y2.is_contiguous(memory_format=memory_format))
                 self.assertEqual(y2, y1, prec=0.1)
 
                 y2.mean().backward()
@@ -119,5 +118,5 @@ class InstanceNormTester(TestCase):
                 self.assertEqual(x2.grad, x1.grad)
 
 
-# if __name__ == "__main__":
-#     test = unittest.main()
+if __name__ == "__main__":
+    test = unittest.main()
diff --git a/tests/cpu/test_ipex_optimize.py b/tests/cpu/test_ipex_optimize.py
index 485fa93a8..7b69d02e5 100644
--- a/tests/cpu/test_ipex_optimize.py
+++ b/tests/cpu/test_ipex_optimize.py
@@ -35,7 +35,6 @@ skipIfNoTransformers = unittest.skipIf(not HAS_TRANSFORMERS, "no transformers")
 
 curpath = os.path.abspath(os.path.dirname(__file__))
 
-
 class ConvBatchNorm(torch.nn.Module):
     def __init__(
         self,
@@ -225,9 +224,7 @@ class TestOptimizeCases(TestCase):
         from transformers.models import albert
         from intel_extension_for_pytorch.nn.utils import _parameter_wrapper
 
-        config = transformers.AutoConfig.from_pretrained(
-            f"{curpath}/hf_configs/albert-base-v1"
-        )
+        config = transformers.AutoConfig.from_pretrained(f"{curpath}/hf_configs/albert-base-v1")
         model = albert.modeling_albert.AlbertForMaskedLM(config)
         params_attr = {}
         _parameter_wrapper.get_shared_parameter_status(model, params_attr)
@@ -252,7 +249,7 @@ class TestOptimizeCases(TestCase):
             AdamW,
             Adamax,
             ASGD,
-            # RMSprop, # TODO: accuracy fails on SPR starting from oneDNN commit 0f354d
+            # RMSprop, # TODO: accuracy fails on EMR starting from oneDNN commit 0f354d
             Rprop,
             SGD,
         ]
diff --git a/tests/cpu/test_ipex_optimize_transformers.py b/tests/cpu/test_ipex_optimize_transformers.py
index 5a3685678..a8a2dd057 100644
--- a/tests/cpu/test_ipex_optimize_transformers.py
+++ b/tests/cpu/test_ipex_optimize_transformers.py
@@ -7,7 +7,6 @@ import os
 import copy
 import re
 import tempfile
-from intel_extension_for_pytorch.quantization import prepare, convert
 
 try:
     import transformers
@@ -99,9 +98,7 @@ class OptimizeTransformersTester(TestCase):
                         )
                     self.assertEqual(key_hf[0], key_ipex[0], prec=0.1)
 
-                    if re.search("GPTJ", model.config.architectures[0]) or re.search(
-                        "codegen", model.config.architectures[0]
-                    ):
+                    if re.search("GPTJ", model.config.architectures[0]):
                         assert (
                             ipex_m.transformer.h[0].attn.__class__
                             is ipex.transformers.models.cpu.modules.attentions._IPEXAttentionCPU
@@ -231,34 +228,12 @@ class OptimizeTransformersTester(TestCase):
             ipex.nn.utils._model_convert.replace_customized_linear_with_linear(m.eval())
         self.model_replacement_check(m, False, torchcompile=True)
 
-    def test_model_replacement_codegen(self):
-        config = AutoConfig.from_pretrained(
-            f"{curpath}/hf_configs/codegen", return_dict=False
-        )
-        m = transformers.models.codegen.modeling_codegen.CodeGenForCausalLM(
-            config
-        ).eval()
-        self.model_replacement_check(m, True)
-
-    def test_model_replacement_codegen_torchcompile(self):
-        config = AutoConfig.from_pretrained(
-            f"{curpath}/hf_configs/codegen", return_dict=False
-        )
-        m = transformers.models.codegen.modeling_codegen.CodeGenForCausalLM(
-            config
-        ).eval()
-        self.model_replacement_check(m, True, torchcompile=True)
-
     def _model_replacement_check_woq(self, model):
-        qconfig_mapping = ipex.quantization.get_weight_only_quant_qconfig_mapping()
-        orig_model = copy.deepcopy(model)
-        orig_woq_model = prepare(orig_model, qconfig_mapping, inplace=True)
-        orig_woq_model = convert(orig_woq_model, inplace=True)
-
+        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping()
         model = ipex.optimize_transformers(
             model,
             dtype=torch.float,
-            quantization_config=qconfig_mapping,
+            quantization_config=qconfig,
             deployment_mode=True,
             inplace=True,
         )
@@ -299,14 +274,7 @@ class OptimizeTransformersTester(TestCase):
         # Ensure model can run without errors
         with torch.no_grad():
             example_inputs = _get_gptj_example_inputs()
-            y = model(*example_inputs)
-            y_ref = orig_woq_model(
-                input_ids=example_inputs[0],
-                attention_mask=example_inputs[1],
-                position_ids=example_inputs[2],
-                use_cache=True,
-            )
-            self.assertEqual(y[0], y_ref[0], prec=1e-4)
+            model(*example_inputs)
 
     def test_weight_only_quant_flow_for_gptj(self):
         config = AutoConfig.from_pretrained(
diff --git a/tests/cpu/test_quantization_default_recipe.py b/tests/cpu/test_quantization_default_recipe.py
index 77013ff69..4731e6dd6 100644
--- a/tests/cpu/test_quantization_default_recipe.py
+++ b/tests/cpu/test_quantization_default_recipe.py
@@ -10,21 +10,12 @@ from torch.ao.quantization import (
     QConfigMapping,
 )
 import copy
-import unittest
-from common_utils import TestCase
 
 import intel_extension_for_pytorch as ipex
 from test_ao_jit_llga_utils import JitLlgaTestCase, LLGA_FUSION_GROUP
 from torch.testing._internal.common_utils import run_tests
 from torch.ao.nn.quantized.modules.utils import _quantize_weight
-from intel_extension_for_pytorch.quantization import (
-    prepare,
-    convert,
-    dequantize_per_channel,
-    dequantize_per_block,
-    quantize_per_channel,
-    quantize_per_block,
-)
+from intel_extension_for_pytorch.quantization import prepare, convert
 
 
 class TestDefaultRecipe(JitLlgaTestCase):
@@ -315,10 +306,9 @@ class TestDefaultRecipe(JitLlgaTestCase):
                 super().__init__()
                 self.dense = nn.Linear(4, 4)
                 self.relu = nn.ReLU()
-                self.dense2 = nn.Linear(4, 4)
 
             def forward(self, x):
-                return self.dense2(self.relu(self.dense(x)))
+                return self.relu(self.dense(x))
 
         m = Mod().eval()
         x = torch.rand(1, 4)
@@ -328,13 +318,12 @@ class TestDefaultRecipe(JitLlgaTestCase):
         )
         custom_config = {
             "alpha": 0.75,
-            "act_observer": torch.ao.quantization.MinMaxObserver,
-            "act_ic_observer": per_channel_observer.with_args(ch_axis=-1),
-            "wei_observer": per_channel_observer.with_args(
+            "act_observer": torch.ao.quantization.MinMaxObserver(),
+            "act_ic_observer": per_channel_observer(ch_axis=-1),
+            "wei_observer": per_channel_observer(
                 dtype=torch.qint8, qscheme=torch.per_channel_symmetric
             ),
-            "wei_ic_observer": per_channel_observer.with_args(ch_axis=1),
-            "share_weight_observers": False,
+            "wei_ic_observer": per_channel_observer(ch_axis=1),
         }
         for use_custom_config in [False, True]:
             kwargs = custom_config if use_custom_config else {}
@@ -356,17 +345,6 @@ class TestDefaultRecipe(JitLlgaTestCase):
                     ].weight_tensor_id_to_observer,
                 }
                 observer_info_dict = {}
-                observer_info_dict["share_weight_observers"] = (
-                    prepared_model._fqn_to_auto_quant_state_map[" "]
-                    .idx_to_seen_q_op_infos[0]
-                    .qconfig.share_weight_observers
-                )
-                sub_observer_ids = {
-                    "act_ic_obs": [],
-                    "act_obs": [],
-                    "wei_oc_obs": [],
-                    "wei_ic_obs": [],
-                }
                 for key, obs in observer_info.items():
                     observer_info_dict[key] = {
                         "smooth_quant_enabled": obs.smooth_quant_enabled,
@@ -374,17 +352,6 @@ class TestDefaultRecipe(JitLlgaTestCase):
                         "ic_obs": type(obs.ic_obs),
                         "act_obs": type(obs.act_obs),
                     }
-                    if isinstance(
-                        obs,
-                        ipex.quantization._smooth_quant.SmoothQuantActivationObserver,
-                    ):
-                        sub_observer_ids["act_ic_obs"].append(id(obs.ic_obs))
-                        sub_observer_ids["act_obs"].append(id(obs.act_obs))
-                    else:
-                        sub_observer_ids["wei_oc_obs"].append(id(obs.oc_obs))
-                        sub_observer_ids["wei_ic_obs"].append(id(obs.ic_obs))
-                for _, id_list in sub_observer_ids.items():
-                    assert all([id_list[0] != id for id in id_list[1:]])
 
             for data in calib_dataset:
                 prepared_model(data)
@@ -415,17 +382,6 @@ class TestDefaultRecipe(JitLlgaTestCase):
                         ].weight_tensor_id_to_observer,
                     }
                     observer_info_dict_2 = {}
-                    observer_info_dict_2["share_weight_observers"] = (
-                        prepared_model_2._fqn_to_auto_quant_state_map[" "]
-                        .idx_to_seen_q_op_infos[0]
-                        .qconfig.share_weight_observers
-                    )
-                    sub_observer_ids = {
-                        "act_ic_obs": [],
-                        "act_obs": [],
-                        "wei_oc_obs": [],
-                        "wei_ic_obs": [],
-                    }
                     for key, obs in observer_info_2.items():
                         observer_info_dict_2[key] = {
                             "smooth_quant_enabled": obs.smooth_quant_enabled,
@@ -433,17 +389,6 @@ class TestDefaultRecipe(JitLlgaTestCase):
                             "ic_obs": type(obs.ic_obs),
                             "act_obs": type(obs.act_obs),
                         }
-                        if isinstance(
-                            obs,
-                            ipex.quantization._smooth_quant.SmoothQuantActivationObserver,
-                        ):
-                            sub_observer_ids["act_ic_obs"].append(id(obs.ic_obs))
-                            sub_observer_ids["act_obs"].append(id(obs.act_obs))
-                        else:
-                            sub_observer_ids["wei_oc_obs"].append(id(obs.oc_obs))
-                            sub_observer_ids["wei_ic_obs"].append(id(obs.ic_obs))
-                    for _, id_list in sub_observer_ids.items():
-                        assert all([id_list[0] != id for id in id_list[1:]])
 
                 q_model_2 = ipex.quantization.convert(prepared_model_2)
 
@@ -454,109 +399,12 @@ class TestDefaultRecipe(JitLlgaTestCase):
 
                 assert torch.allclose(out_ref, out_2)
 
-                # Scales and zero points should be updated after rerunning calibration
-                scale_zp_0 = prepared_model_2._fqn_to_auto_quant_state_map[
-                    " "
-                ].tensor_id_to_scale_zp
-                scale_zp_0 = copy.deepcopy(scale_zp_0)
-                for data in calib_dataset:
-                    prepared_model_2(data + 1)
-                prepared_model_2.save_qconf_summary(qconf_summary=qconf_filename)
-                scale_zp_1 = prepared_model_2._fqn_to_auto_quant_state_map[
-                    " "
-                ].tensor_id_to_scale_zp
-                assert scale_zp_0 != scale_zp_1
-
             # Check observers
             if use_custom_config:
                 assert (
                     observer_info_dict == observer_info_dict_2
                 ), "Error: SmoothQuant observer info lost after saving/loading qconf JSON"
 
-    def test_smooth_quant_cancel_by_qconf_summary(self):
-        class Mod(nn.Module):
-            def __init__(self):
-                super().__init__()
-                self.dense = nn.Linear(4, 4)
-                self.relu = nn.ReLU()
-
-            def forward(self, x):
-                return self.relu(self.dense(x))
-
-        m = Mod().eval()
-        x = torch.rand(1, 4)
-        calib_dataset = [torch.rand(1, 4) for _ in range(5)]
-        qconfig_mapping = ipex.quantization.get_smooth_quant_qconfig_mapping()
-        prepared_model = ipex.quantization.prepare(
-            m, qconfig_mapping, example_inputs=x, inplace=False
-        )
-        for data in calib_dataset:
-            prepared_model(data)
-
-        with tempfile.NamedTemporaryFile() as fp:
-            qconf_filename = fp.name
-            prepared_model.save_qconf_summary(qconf_summary=qconf_filename)
-            import json
-
-            with open(qconf_filename, "r") as qconf_file:
-                parsed = json.load(qconf_file)
-                parsed[" "]["q_op_infos"]["0"]["input_tensor_infos"][0][
-                    "force_dtype"
-                ] = "torch.float32"
-
-            with open(qconf_filename, "w") as qconf_file:
-                json.dump(parsed, qconf_file, indent=4)
-
-            prepared_model_2 = ipex.quantization.prepare(
-                m, qconfig_mapping, example_inputs=x, inplace=False
-            )
-            prepared_model_2.load_qconf_summary(qconf_summary=qconf_filename)
-            converted_model = ipex.quantization.convert(prepared_model_2)
-            with torch.no_grad():
-                jit_model = torch.jit.trace(converted_model, x)
-                jit_model = torch.jit.freeze(jit_model)
-                for _ in range(2):
-                    jit_model(x)
-                graph = jit_model.graph_for(x)
-                for n in graph.nodes():
-                    assert n.kind() != "aten::mul"
-
-    def test_smooth_quant_share_weight_observers(self):
-        class Mod(nn.Module):
-            def __init__(self):
-                super().__init__()
-                self.q_proj = nn.Linear(4, 4)
-                self.k_proj = nn.Linear(4, 4)
-                self.v_proj = nn.Linear(4, 4)
-                self.relu = nn.ReLU()
-
-            def forward(self, x):
-                q = self.q_proj(x)
-                k = self.k_proj(x)
-                v = self.v_proj(x)
-                return self.relu(torch.concat([q, k, v], axis=1))
-
-        m = Mod().eval()
-        x = torch.rand(1, 4)
-        calib_dataset = [torch.rand(1, 4) for _ in range(5)]
-        for share_weight_observers in [True, False]:
-            qconfig_mapping = ipex.quantization.get_smooth_quant_qconfig_mapping(
-                share_weight_observers=share_weight_observers
-            )
-            prepared_model = ipex.quantization.prepare(
-                m, qconfig_mapping, example_inputs=x, inplace=True
-            )
-            for data in calib_dataset:
-                prepared_model(data)
-            q_model = ipex.quantization.convert(prepared_model)
-            with torch.no_grad():
-                q_model = torch.jit.trace(q_model, x)
-                q_model = torch.jit.freeze(q_model)
-                graph = q_model.graph_for(x)
-                num_mul = [n.kind() for n in graph.nodes()].count("aten::mul")
-                assert num_mul == 1 if share_weight_observers else 3
-                q_model(x)
-
     def test_none_example_input_for_quantization(self):
         class M(nn.Module):
             def __init__(self):
@@ -583,8 +431,6 @@ class TestDefaultRecipe(JitLlgaTestCase):
         with self.assertRaises(AssertionError):
             prepared_model = ipex.quantization.prepare(m, qconfig_mapping)
 
-
-class WeightOnlyQuantizationTester(TestCase):
     def test_weight_only_quantization(self):
         class M(nn.Module):
             def __init__(self, input_channel, output_channel, has_bias):
@@ -599,11 +445,12 @@ class WeightOnlyQuantizationTester(TestCase):
             m = model.eval()
             data = torch.rand(1, feature[0], feature[1])
             weight = model.linear.weight
-            is_int4 = False
-            weight_int8, w_scales, w_zero_points = quantize_per_channel(weight, is_int4)
-            weight_fp32 = dequantize_per_channel(
-                weight_int8, w_scales, w_zero_points.int(), is_int4, weight.shape
+            weight_observer = (
+                ipex.quantization.get_weight_only_quant_qconfig_mapping().global_qconfig.weight()
             )
+            weight_observer(weight)
+            weight_int8 = _quantize_weight(weight, weight_observer)
+            weight_fp32 = weight_int8.dequantize()
             if has_bias:
                 bias = model.linear.bias
                 output1 = torch.matmul(data, weight_fp32.T) + bias
@@ -658,74 +505,65 @@ class WeightOnlyQuantizationTester(TestCase):
             m = model.eval()
             data = torch.rand(feature[0], feature[1])
 
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-                weight_dtype=w_dtype
-            )
+            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(weight_dtype=w_dtype)
             prepared_model = prepare(m, qconfig, example_inputs=data, inplace=False)
 
-            is_int4 = w_dtype == torch.quint4x2
             with torch.no_grad():
                 weight = m.linear.weight
-                weight_int8, w_scales, w_zero_points = quantize_per_channel(
-                    weight, is_int4
-                )
-                weight_fp32 = dequantize_per_channel(
-                    weight_int8, w_scales, w_zero_points.int(), is_int4, weight.shape
-                )
+                weight_observer = qconfig.global_qconfig.weight()
+                weight_observer(weight)
+                weight_int8 = _quantize_weight(weight, weight_observer)
+                weight_fp32 = weight_int8.dequantize()
                 weight_bf16 = weight_fp32.bfloat16()
                 weight_fp16 = weight_fp32.half()
                 data_bf16 = data.bfloat16()
                 data_fp16 = data_bf16.half()
                 bias_fp32 = m.linear.bias
-                # if M >= 32, compute in bf16
-                # if M < 32, compute in fp32 or fp16. Depends on fp16 support.
-                if feature[0] >= 32:
-                    output1 = torch.matmul(
-                        data_bf16.float(), weight_bf16.float().T
-                    ).bfloat16()
-                    if has_bias:
-                        output1 = output1 + bias_fp32.bfloat16()
+                use_tpp = tpp_is_used(feature[2], feature[1])
+                if use_tpp:
+                    # if M >= 32, compute in bf16
+                    # if M < 32, compute in fp32 or fp16. Depends on fp16 support.
+                    if feature[0] >= 32:
+                        output1 = torch.matmul(data_bf16.float(), weight_bf16.float().T).bfloat16()
+                        if has_bias:
+                            output1 = output1 + bias_fp32.bfloat16()
+                    else:
+                        output1_fp32 = torch.matmul(data_bf16.float(), weight_bf16.float().T)
+                        if has_bias:
+                            output1_fp32 = output1_fp32 + bias_fp32
+                        output1_fp16 = torch.matmul(data_fp16.float(), weight_fp16.float().T).half()
+                        if has_bias:
+                            output1_fp16 = output1_fp16 + bias_fp32.half()
                 else:
-                    output1_fp32 = torch.matmul(
-                        data_bf16.float(), weight_bf16.float().T
-                    )
-                    if has_bias:
-                        output1_fp32 = output1_fp32 + bias_fp32
-                    output1_fp16 = torch.matmul(
-                        data_fp16.float(), weight_fp16.float().T
-                    ).half()
+                    if feature[0] <= 4:
+                        output1 = torch.matmul(data_bf16.float(), weight_fp32.T)
+                    else:
+                        output1 = torch.matmul(data_bf16.float(), weight_bf16.float().T)
                     if has_bias:
-                        output1_fp16 = output1_fp16 + bias_fp32.half()
-                with torch.autocast(
-                    device_type="cpu", enabled=True, dtype=torch.bfloat16
-                ):
+                        output1 = output1 + bias_fp32
+                    output1 = output1.bfloat16()
+                with torch.autocast(device_type='cpu', enabled=True, dtype=torch.bfloat16):
                     woq_model = convert(prepared_model)
-                    woq_linear_class = (
-                        ipex.nn.modules.weight_only_quantization.IpexWoqLinear
-                    )
-                    assert isinstance(woq_model.linear, woq_linear_class)
+                    woq_linear_class = ipex.nn.modules.weight_only_quantization.IpexWoqLinear
+                    assert isinstance(woq_model.linear, woq_linear_class)   
 
                     woq_model = torch.jit.trace(woq_model, data)
                     woq_model = torch.jit.freeze(woq_model)
                     output2 = woq_model(data)
                     output2 = output2.bfloat16()
-                if feature[0] < 32:
+                if use_tpp and feature[0] < 32:
                     try:
-                        torch.testing.assert_close(
-                            output1_fp32.bfloat16(), output2, atol=0.01, rtol=0.1
-                        )
+                        torch.testing.assert_close(output1_fp32.bfloat16(), output2, atol=0.01, rtol=0.1)
                     except Exception as e:
-                        torch.testing.assert_close(
-                            output1_fp16.bfloat16(), output2, atol=0.01, rtol=0.1
-                        )
+                        torch.testing.assert_close(output1_fp16.bfloat16(), output2, atol=0.01, rtol=0.1)
                 else:
                     torch.testing.assert_close(output1, output2)
 
         shape_list = [
             [3, 31, 31],
-            [4, 64, 64],
-            [9, 128, 128],
-            [196, 63, 255],
+            # [4, 4096, 4096], # not supported by TPP yet (block_n = 16 issue)
+            [9, 4095, 4095],
+            [196, 4095, 16383],
         ]
         use_bias_list = [True, False]
         w_dtype_list = [torch.qint8, torch.quint4x2]
@@ -749,15 +587,12 @@ class WeightOnlyQuantizationTester(TestCase):
         use_bias_list = [True, False]
         w_dtype_list = [torch.qint8, torch.quint4x2]
         model_dtype_list = [torch.bfloat16, torch.half]
-        cases = itertools.product(
-            shape_list, use_bias_list, w_dtype_list, model_dtype_list
-        )
+        cases = itertools.product(shape_list, use_bias_list, w_dtype_list, model_dtype_list)
         for shape, use_bias, w_dtype, model_dtype in cases:
             m = M(shape[1], shape[2], use_bias).to(model_dtype).eval()
             data = torch.rand(shape[0], shape[1])
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-                weight_dtype=w_dtype
-            )
+            qconfig = \
+                ipex.quantization.get_weight_only_quant_qconfig_mapping(weight_dtype=w_dtype)
             prepared_model = prepare(m, qconfig, example_inputs=data, inplace=False)
             with torch.no_grad():
                 woq_model = convert(prepared_model)
@@ -778,9 +613,7 @@ class WeightOnlyQuantizationTester(TestCase):
             m = model.eval()
             example_inputs = torch.rand(feature[0], feature[1])
 
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-                weight_dtype=w_dtype
-            )
+            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(weight_dtype=w_dtype)
             prepared_model = prepare(
                 m, qconfig, example_inputs=example_inputs, inplace=False
             )
@@ -828,30 +661,23 @@ class WeightOnlyQuantizationTester(TestCase):
             m = model.eval()
             data = torch.rand(feature[0], feature[1])
             weight = model.linear.weight
-            weight_observer = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-                weight_dtype=torch.quint4x2
-            ).global_qconfig.weight()
-            weight_observer(weight)
-            is_int4 = True
-            weight_int4, w_scales, w_zero_points = quantize_per_channel(weight, is_int4)
-            weight_fp32 = dequantize_per_channel(
-                weight_int4, w_scales, w_zero_points, is_int4, weight.shape
+            weight_observer = (
+                ipex.quantization.get_weight_only_quant_qconfig_mapping(weight_dtype=torch.quint4x2).global_qconfig.weight()
             )
+            weight_observer(weight)
+            weight_int4 = _quantize_weight(weight, weight_observer)
+            weight_fp32 = weight_int4.dequantize()
             if has_bias:
                 bias = model.linear.bias
                 output1 = torch.matmul(data, weight_fp32.T) + bias
             else:
                 output1 = torch.matmul(data, weight_fp32.T)
 
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-                weight_dtype=torch.quint4x2
-            )
+            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(weight_dtype=torch.quint4x2)
             prepared_model = prepare(m, qconfig, example_inputs=data, inplace=False)
             with torch.no_grad():
                 woq_model = convert(prepared_model)
-                woq_linear_class = (
-                    ipex.nn.modules.weight_only_quantization.IpexWoqLinear
-                )
+                woq_linear_class = ipex.nn.modules.weight_only_quantization.IpexWoqLinear
                 assert isinstance(woq_model.linear, woq_linear_class)
 
                 output2 = woq_model(data)
@@ -883,26 +709,16 @@ class WeightOnlyQuantizationTester(TestCase):
         bf16_list = [False, True]
         cases = itertools.product(bias_list, bf16_list)
         for bias, bf16 in cases:
-            with torch.cpu.amp.autocast(
-                enabled=bf16, dtype=torch.bfloat16 if bf16 else None
-            ):
+            with torch.cpu.amp.autocast(enabled=bf16, dtype=torch.bfloat16 if bf16 else None):
                 model = Mod(bias).eval()
                 data = torch.rand(4, 64)
-                qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-                    lowp_mode=2
-                )
-                prepared_model = prepare(
-                    model, qconfig, example_inputs=data, inplace=False
-                )
+                qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(lowp_mode=2)
+                prepared_model = prepare(model, qconfig, example_inputs=data, inplace=False)
                 with torch.no_grad():
                     woq_model = convert(prepared_model)
                     output1 = woq_model(data)
-                    output2 = torch.ops.torch_ipex.woq_linear_gelu(
-                        data, woq_model.linear._op_context.get_data_handle()
-                    )
-                    torch.testing.assert_close(
-                        output1, output2.to(output1.dtype), atol=1e-2, rtol=1e-4
-                    )
+                    output2 = torch.ops.torch_ipex.woq_linear_gelu(data, woq_model.linear._op_context.get_data_handle())
+                    torch.testing.assert_close(output1, output2.to(output1.dtype), atol=1e-2, rtol=1e-4)
 
     def test_weight_only_quantization_add_fused_op(self):
         class Mod(nn.Module):
@@ -921,34 +737,21 @@ class WeightOnlyQuantizationTester(TestCase):
         others_len_list = [1, 2]
         cases = itertools.product(bias_list, bf16_list, others_len_list)
         for bias, bf16, others_len in cases:
-            with torch.cpu.amp.autocast(
-                enabled=bf16, dtype=torch.bfloat16 if bf16 else None
-            ):
+            with torch.cpu.amp.autocast(enabled=bf16, dtype=torch.bfloat16 if bf16 else None):
                 model = Mod(bias).eval()
                 data = torch.rand(4, 64)
                 others = [torch.rand(4, 64)] * others_len
-                fused_op = (
-                    torch.ops.torch_ipex.woq_linear_add
-                    if others_len == 1
+                fused_op = torch.ops.torch_ipex.woq_linear_add if others_len == 1 \
                     else torch.ops.torch_ipex.woq_linear_add_add
-                )
-                qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-                    lowp_mode=2
-                )
-                prepared_model = prepare(
-                    model, qconfig, example_inputs=data, inplace=False
-                )
+                qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(lowp_mode=2)
+                prepared_model = prepare(model, qconfig, example_inputs=data, inplace=False)
                 with torch.no_grad():
                     woq_model = convert(prepared_model)
                     output1 = woq_model(data, others)
-                    output2 = fused_op(
-                        data, woq_model.linear._op_context.get_data_handle(), others
-                    )
-                    torch.testing.assert_close(
-                        output1, output2.to(output1.dtype), atol=1.5e-2, rtol=1e-3
-                    )
-
-    def test_weight_only_quantization_lowp_mode_functionality(self):
+                    output2 = fused_op(data, woq_model.linear._op_context.get_data_handle(), others)
+                    torch.testing.assert_close(output1, output2.to(output1.dtype), atol=1.5e-2, rtol=1e-3)
+
+    def test_weight_only_quantization_lowp_compute(self):
         from intel_extension_for_pytorch.quantization import WoqLowpMode
 
         class M(nn.Module):
@@ -961,70 +764,17 @@ class WeightOnlyQuantizationTester(TestCase):
 
         data = torch.rand(4, 64)
         m = M()
-        for mode in [
-            WoqLowpMode.NONE,
-            WoqLowpMode.FP16,
-            WoqLowpMode.BF16,
-            WoqLowpMode.INT8,
-        ]:
-            kwargs = {"lowp_mode": mode}
+        for mode in [WoqLowpMode.FP16, WoqLowpMode.BF16, WoqLowpMode.INT8]:
+            kwargs = {'lowp_mode': mode}
             if mode == WoqLowpMode.INT8:
-                kwargs["weight_dtype"] = torch.quint4x2
+                kwargs['weight_dtype'] = torch.quint4x2
             qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(**kwargs)
             prepared_model = prepare(m, qconfig, example_inputs=data, inplace=False)
             with torch.no_grad():
                 woq_model = convert(prepared_model)
                 woq_model(data)
-                assert (
-                    hasattr(woq_model.linear, "_lowp_mode")
-                    and woq_model.linear._lowp_mode == mode
-                ), "Weight-only quantization: low precision gemm flag is not correctly set"
-
-    def test_weight_only_quantization_int8_lowp_mode_correctness(self):
-        from intel_extension_for_pytorch.quantization import WoqLowpMode
-
-        class M(nn.Module):
-            def __init__(self):
-                super(M, self).__init__()
-                self.linear = torch.nn.Linear(64, 128)
-
-            def forward(self, x):
-                return self.linear(x)
-
-        # When lowp_mode=BF16, only case of batch size >= 32 uses BF16.
-        data = torch.rand(32, 64)
-        m = M()
-
-        lowp_mode_list = [WoqLowpMode.NONE, WoqLowpMode.FP16, WoqLowpMode.BF16]
-        act_dtype_list = [torch.bfloat16, torch.half]
-        compute_dtype_list = [None, torch.half, torch.bfloat16]
-        cases = itertools.product(lowp_mode_list, act_dtype_list)
-        # lowp_mode does not affect weight observer for int8
-        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping()
-        weight = copy.deepcopy(m.linear.weight)
-        weight_observer = qconfig.global_qconfig.weight()
-        weight_observer(weight)
-        weight_int8 = _quantize_weight(weight, weight_observer)
-        weight_fp32 = weight_int8.dequantize()
-        bias_fp32 = copy.deepcopy(m.linear.bias)
-        for lowp_mode, act_dtype in cases:
-            if lowp_mode == WoqLowpMode.NONE:
-                compute_dtype_list[0] = act_dtype
-            compute_dtype = compute_dtype_list[int(lowp_mode)]
-            qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-                lowp_mode=lowp_mode,
-                weight_dtype=torch.qint8,
-            )
-            prepared_model = prepare(m, qconfig, example_inputs=data, inplace=False)
-            with torch.no_grad():
-                woq_model = convert(prepared_model)
-                y = woq_model(data.to(act_dtype))
-                weight_for_compute = weight_fp32.to(compute_dtype).float()
-                act_for_compute = data.to(act_dtype).to(compute_dtype).float()
-                bias_for_compute = bias_fp32.to(compute_dtype).float()
-                y_ref = act_for_compute @ weight_for_compute.T + bias_for_compute
-                y_ref = y_ref.to(act_dtype)
-                torch.testing.assert_close(y, y_ref, atol=0.005, rtol=0.01)
+                assert hasattr(woq_model.linear, '_lowp_mode') and woq_model.linear._lowp_mode == mode, \
+                    'Weight-only quantization: low precision gemm flag is not correctly set'
 
     def test_weight_only_quantization_num_concats(self):
         class Mod(nn.Module):
@@ -1053,267 +803,20 @@ class WeightOnlyQuantizationTester(TestCase):
 
         m = Mod().eval()
         m2 = Mod2().eval()
-        m2.qkv.weight = nn.Parameter(
-            torch.cat([m.q.weight, m.k.weight, m.v.weight], dim=0)
-        )
+        m2.qkv.weight = nn.Parameter(torch.cat([m.q.weight, m.k.weight, m.v.weight], dim=0))
         data = torch.rand(4, 64)
         qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(lowp_mode=2)
         prepared = prepare(m, qconfig, example_inputs=data, inplace=True)
         prepared2 = prepare(m2, qconfig, example_inputs=data, inplace=True)
         for bf16 in [False, True]:
-            with torch.no_grad(), torch.cpu.amp.autocast(
-                enabled=bf16, dtype=torch.bfloat16 if bf16 else None
-            ):
+            with torch.no_grad(), \
+                    torch.cpu.amp.autocast(enabled=bf16, dtype=torch.bfloat16 if bf16 else None):
                 qm = convert(prepared)
                 qm2 = convert(prepared2)
                 output1 = qm(data)
                 output2 = qm2(data)
                 torch.testing.assert_close(output1, output2, atol=1e-2, rtol=1e-4)
 
-    def _fakequant_by_group(self, t, quant_a_mode, groupsize):
-        assert quant_a_mode >= 0 and quant_a_mode <= 3
-        if quant_a_mode == 0:
-            obs = torch.ao.quantization.MinMaxObserver(torch.quint8)
-            obs(t)
-            scale, zero_point = obs.calculate_qparams()
-            return (
-                torch.quantize_per_tensor(
-                    t.to(torch.float), scale, zero_point, torch.quint8
-                )
-                .dequantize()
-                .to(t.dtype)
-            )
-        orig_shape = t.shape
-        if t.shape[-1] % groupsize:
-            pad_len = t.shape[-1] // groupsize * groupsize + groupsize - t.shape[-1]
-            t = torch.nn.functional.pad(t, (0, pad_len), value=0)
-        grouped = t.view(-1, t.shape[-1] // groupsize, groupsize)
-        if quant_a_mode == 1:
-            grouped_min = grouped.min(dim=-1)[0].min(dim=0)[0]
-            grouped_max = grouped.max(dim=-1)[0].max(dim=0)[0]
-        elif quant_a_mode == 2:
-            grouped_min = grouped.min(dim=-1)[0].min(dim=1)[0]
-            grouped_max = grouped.max(dim=-1)[0].max(dim=1)[0]
-        else:
-            grouped_min = grouped.min(dim=-1)[0]
-            grouped_max = grouped.max(dim=-1)[0]
-        zeros = torch.zeros_like(grouped_min)
-        min = torch.minimum(grouped_min, zeros)
-        max = torch.maximum(grouped_max, zeros)
-        eps = torch.tensor([torch.finfo(torch.float32).eps])
-        scales = (max - min) / 255
-        scales = torch.max(scales, eps)
-        zps = -torch.round(min / scales)
-        if quant_a_mode == 1:
-            qt = torch.clamp(
-                torch.round(grouped / scales.unsqueeze(1)) + zps.unsqueeze(1),
-                min=0,
-                max=255,
-            )
-            out = (
-                ((qt - zps.unsqueeze(1)) * scales.unsqueeze(1))
-                .to(t.dtype)
-                .view(t.shape)
-            )
-            if orig_shape != out.shape:
-                out = out[: orig_shape[0], : orig_shape[1]].contiguous()
-            return out
-        elif quant_a_mode == 2:
-            qt = torch.clamp(
-                torch.round(grouped / scales.unsqueeze(1).unsqueeze(2))
-                + zps.unsqueeze(1).unsqueeze(2),
-                min=0,
-                max=255,
-            )
-            out = (
-                (
-                    (qt - zps.unsqueeze(1).unsqueeze(2))
-                    * scales.unsqueeze(1).unsqueeze(2)
-                )
-                .to(t.dtype)
-                .view(t.shape)
-            )
-            if orig_shape != out.shape:
-                out = out[: orig_shape[0], : orig_shape[1]].contiguous()
-            return out
-        else:
-            qt = torch.clamp(
-                torch.round(grouped / scales.unsqueeze(-1)) + zps.unsqueeze(-1),
-                min=0,
-                max=255,
-            )
-            out = (
-                ((qt - zps.unsqueeze(-1)) * scales.unsqueeze(-1))
-                .to(t.dtype)
-                .view(t.shape)
-            )
-            if orig_shape != out.shape:
-                out = out[: orig_shape[0], : orig_shape[1]].contiguous()
-            return out
-
-    def test_weight_only_quantization_act_quant_mode(self):
-        M, N, K = 4, 64, 128
-        groupsize = 64
-
-        class Mod(nn.Module):
-            def __init__(self, has_bias):
-                super(Mod, self).__init__()
-                self.linear = torch.nn.Linear(K, N, has_bias)
-
-            def forward(self, x):
-                return self.linear(x)
-
-        def test(has_bias, act_quant_mode):
-            dtype = torch.bfloat16
-            model = Mod(has_bias)
-            m = model.eval()
-            m2 = copy.deepcopy(m)
-            data = torch.rand(M, K) * 0.5
-            qconfig_mapping = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-                weight_dtype=torch.quint4x2,
-                lowp_mode=ipex.quantization.WoqLowpMode.INT8,
-                act_quant_mode=act_quant_mode,
-            )
-            fake_quant_x = self._fakequant_by_group(data, act_quant_mode, groupsize)
-            prepared_model = prepare(m2, qconfig_mapping, inplace=True)
-            with torch.no_grad(), torch.autocast(
-                device_type="cpu", enabled=True, dtype=dtype
-            ):
-                woq_model = convert(prepared_model)
-                # Behavior of WOQ Linear to simulate:
-                # Quantize weight to int4 by float qparams at quantization time
-                # Quantize activation to int8 at runtime
-                # Convert weight and its zero points to INT8 for computation
-                qw = woq_model.linear._op_context.to_public(
-                    woq_model.linear._op_context.get_weight()
-                )
-                w_scales = woq_model.linear._op_context.get_scales()
-                w_zero_points = woq_model.linear._op_context.get_zero_points()
-                w = copy.deepcopy(m.linear.weight.data)
-                is_int4 = True
-                qw, _, _ = quantize_per_channel(w, is_int4, w_scales, w_zero_points)
-                fake_quant_w = dequantize_per_channel(
-                    qw, w_scales, w_zero_points.int(), is_int4, w.shape
-                )
-                m.linear.weight.data = fake_quant_w
-                y_ref = m(fake_quant_x).to(dtype)
-                y = woq_model(data)
-                try:
-                    torch.testing.assert_close(y, y_ref, atol=1e-2 * 5, rtol=1e-1 * 2)
-                except Exception:
-                    # The fallback kernel does not support act quant mode
-                    # It computes in fp32 by dequantizing weight.
-                    fake_quant_w = qw.dequantize()
-                    y_ref = data @ fake_quant_w.T + (m.linear.bias if has_bias else 0)
-                    y_ref = y_ref.to(dtype)
-                    torch.testing.assert_close(y, y_ref, atol=1e-2, rtol=1e-1)
-
-        has_bias_list = [False, True]
-        quant_mode_list = [0, 1, 2, 3]
-        cases = itertools.product(has_bias_list, quant_mode_list)
-        for has_bias, quant_mode in cases:
-            test(has_bias, quant_mode)
-
-    def test_weight_only_quantization_group_size(self):
-        # M, N, K = 4, 64, 128
-
-        class Mod(nn.Module):
-            def __init__(self, ic, oc, has_bias):
-                super(Mod, self).__init__()
-                self.linear = torch.nn.Linear(ic, oc, has_bias)
-
-            def forward(self, x):
-                return self.linear(x)
-
-        def test(shape, has_bias, act_quant_mode, group_size):
-            M, N, K = shape
-            dtype = torch.bfloat16
-            model = Mod(K, N, has_bias)
-            m = model.eval()
-            m2 = copy.deepcopy(m)
-            data = torch.rand(M, K) * 0.5
-            if group_size == -1 and act_quant_mode != 0:
-                # these cases are covered by another test case for act_quant_mode
-                return
-            qconfig_mapping = ipex.quantization.get_weight_only_quant_qconfig_mapping(
-                weight_dtype=torch.quint4x2,
-                lowp_mode=ipex.quantization.WoqLowpMode.INT8,
-                act_quant_mode=act_quant_mode,
-                group_size=group_size,
-            )
-            fake_quant_x = self._fakequant_by_group(data, act_quant_mode, group_size)
-            prepared_model = prepare(m2, qconfig_mapping, inplace=True)
-            with torch.no_grad(), torch.autocast(
-                device_type="cpu", enabled=True, dtype=dtype
-            ):
-                woq_model = convert(prepared_model)
-                # Behavior of WOQ Linear to simulate:
-                # Quantize weight to int4 by float qparams at quantization time
-                # Quantize activation to int8 at runtime
-                # Convert weight and its zero points to INT8 for computation
-                w = copy.deepcopy(m.linear.weight.data)
-                is_int4 = True
-                if group_size == -1:
-                    qw, w_scales, w_zero_points = quantize_per_channel(
-                        w, is_int4, None, None
-                    )
-                    fake_quant_w = dequantize_per_channel(
-                        qw, w_scales, w_zero_points.int(), is_int4, w.shape
-                    )
-                else:
-                    qw, w_scales, w_zero_points = quantize_per_block(
-                        w, is_int4, group_size, None, None
-                    )
-                    fake_quant_w = dequantize_per_block(
-                        qw,
-                        w_scales,
-                        w_zero_points,
-                        is_int4,
-                        group_size,
-                        weight_shape=w.shape,
-                    )
-                m.linear.weight.data = fake_quant_w
-                y_ref = m(fake_quant_x).to(dtype)
-                y = woq_model(data)
-                try:
-                    torch.testing.assert_close(y, y_ref, atol=1e-2 * 5, rtol=1e-1 * 2)
-                except Exception:
-                    # The fallback kernel does not support act quant mode
-                    # It computes in fp32 by dequantizing weight.
-                    # fake_quant_w = qw.dequantize()
-                    y_ref = data @ fake_quant_w.T + (m.linear.bias if has_bias else 0)
-                    y_ref = y_ref.to(dtype)
-                    torch.testing.assert_close(y, y_ref, atol=1e-2, rtol=1e-1)
-
-        MNK_list = [(4, 64, 128), (4, 32, 127), (9, 31, 256)]
-        has_bias_list = [False, True]
-        quant_mode_list = [0, 1, 2, 3]
-        group_size_list = [-1, 32, 64, 128]
-        cases = itertools.product(
-            MNK_list, has_bias_list, quant_mode_list, group_size_list
-        )
-        for shape, has_bias, act_quant_mode, group_size in cases:
-            test(shape, has_bias, act_quant_mode, group_size)
-
-
-class QuantizedOpsTester(TestCase):
-    def test_matmul_i8i8i32(self):
-        x = torch.randn(4, 8)
-        w = torch.randn(4, 8)
-        x_min, x_max = x.aminmax()
-        x_scale = torch.max(x_max, x_min.neg()) / 127
-        qx = torch.round(x / x_scale).to(torch.int8)
-        w_min, w_max = w.aminmax(dim=1)
-        w_scale = torch.max(w_max, w_min.neg()) / 127
-        qw = torch.round(w / w_scale.unsqueeze(-1)).to(torch.int8)
-        for use_bf16 in [False, True]:
-            dtype = torch.bfloat16 if use_bf16 else torch.float32
-            with torch.cpu.amp.autocast(enabled=use_bf16, dtype=dtype):
-                qy = torch.ops.torch_ipex.matmul_i8i8i32(qx, qw)
-                qy_ref = torch.nn.functional.linear(qx.to(dtype), qw.to(dtype))
-                self.assertEqual(qy.to(dtype), qy_ref)
-
 
 if __name__ == "__main__":
-    test = unittest.main()
     run_tests()
diff --git a/tests/cpu/test_tpp_linear.py b/tests/cpu/test_tpp_linear.py
index 7905914a0..d5ca27501 100644
--- a/tests/cpu/test_tpp_linear.py
+++ b/tests/cpu/test_tpp_linear.py
@@ -82,36 +82,7 @@ class Linear_add_add(torch.nn.Module):
         return self.mlp(x) + x + x
 
 
-class Linear_tpp_fallback_dnnl(torch.nn.Module):
-    def __init__(self):
-        super(Linear_tpp_fallback_dnnl, self).__init__()
-        self.mlp = torch.nn.Linear(4097, 4097)
-
-    def forward(self, x):
-        return self.mlp(x)
-
-
 class TestTPPlinear(TestCase):
-    def test_tpp_linear_fallback(self):
-        x1 = torch.rand(1, 1, 4097)
-        x2 = copy.deepcopy(x1)
-        for dtype in [torch.float, torch.bfloat16]:
-            model = Linear_tpp_fallback_dnnl().eval()
-
-            with torch.no_grad(), torch.cpu.amp.autocast(
-                enabled=True if dtype is torch.bfloat16 else False
-            ):
-                ref_out = model(x1)
-
-            _enable_tpp()
-            model = ipex.optimize(model, dtype=dtype)
-            with torch.no_grad(), torch.cpu.amp.autocast(
-                enabled=True if dtype is torch.bfloat16 else False
-            ):
-                out = model(x2)
-            self.assertEqual(out, ref_out)
-            _disable_tpp()
-
     def test_tpp_linear(self):
         x1 = torch.rand(1, 1, 4096)
         x2 = copy.deepcopy(x1)
@@ -260,7 +231,7 @@ class TestTPPlinear(TestCase):
 
             _enable_tpp()
             model = ipex.optimize(model, dtype=dtype)
-
+            
             def fn(x):
                 return torch.ops.torch_ipex.tpp_linear_silu(
                     x, model.mlp.weight, x.new_empty(0), model.mlp.out_features
@@ -315,7 +286,7 @@ class TestTPPlinear(TestCase):
 
             _enable_tpp()
             model = ipex.optimize(model, dtype=dtype)
-
+            
             def fn(x):
                 return torch.ops.torch_ipex.tpp_linear_relu(
                     x, model.mlp.weight, x.new_empty(0), model.mlp.out_features
@@ -370,7 +341,7 @@ class TestTPPlinear(TestCase):
 
             _enable_tpp()
             model = ipex.optimize(model, dtype=dtype)
-
+            
             def fn(x):
                 return torch.ops.torch_ipex.tpp_linear_mul(
                     x, x, model.mlp.weight, x.new_empty(0), model.mlp.out_features
@@ -425,7 +396,7 @@ class TestTPPlinear(TestCase):
 
             _enable_tpp()
             model = ipex.optimize(model, dtype=dtype)
-
+            
             def fn(x):
                 return torch.ops.torch_ipex.tpp_linear_add(
                     x, x, model.mlp.weight, x.new_empty(0), 1.0, model.mlp.out_features
@@ -461,7 +432,7 @@ class TestTPPlinear(TestCase):
                 )
                 self.assertEqual(out, ref_out)
                 _disable_tpp()
-
+                
     def test_tpp_linear_add2_torchcompile(self):
         x = torch.rand(2, 2, 4096)
 
@@ -480,16 +451,10 @@ class TestTPPlinear(TestCase):
 
             _enable_tpp()
             model = ipex.optimize(model, dtype=dtype)
-
+            
             def fn(x):
                 return torch.ops.torch_ipex.tpp_linear_add_add(
-                    x,
-                    x,
-                    x,
-                    model.mlp.weight,
-                    model.mlp.bias,
-                    1.0,
-                    model.mlp.out_features,
+                    x, x, x, model.mlp.weight, model.mlp.bias, 1.0, model.mlp.out_features
                 )
 
             with torch.no_grad():
diff --git a/version.txt b/version.txt
index 9a7c2c152..0a4e7bdb7 100644
--- a/version.txt
+++ b/version.txt
@@ -1,3 +1,3 @@
 VERSION_MAJOR 2
 VERSION_MINOR 1
-VERSION_PATCH 100
+VERSION_PATCH 0
